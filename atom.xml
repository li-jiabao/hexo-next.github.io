<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-03-04T15:17:56.894Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/2021-03-03-hhh/"/>
    <id>http://example.com/wiki/2021-03-03-hhh/</id>
    <published>2021-03-04T15:17:56.894Z</published>
    <updated>2021-03-04T15:17:56.894Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Wikitten-configuration</title>
    <link href="http://example.com/wiki/2021-03-04-2021-03-04-Wikitten-themes-configuration.md/"/>
    <id>http://example.com/wiki/2021-03-04-2021-03-04-Wikitten-themes-configuration.md/</id>
    <published>2021-03-04T15:17:56.051Z</published>
    <updated>2021-03-04T15:17:56.051Z</updated>
    
    <content type="html"><![CDATA[<h3 id="wikitten主题"><a href="#wikitten主题" class="headerlink" title="wikitten主题"></a>wikitten主题</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p>具体看官网</p><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><ol><li>wiget配置：在主题的config文件中，找到相关的配置项，解除注释或者按文档中提示配置</li><li>Menu配置：在主题config中，找到menu配置处，可以按照自己的需要增加菜单项名字和链接</li><li>img配置：图形文件等可以放在wiki站点目录下的public文件夹下面的<code>/css/img</code>或者<code>images</code>下面</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;wikitten主题&quot;&gt;&lt;a href=&quot;#wikitten主题&quot; class=&quot;headerlink&quot; title=&quot;wikitten主题&quot;&gt;&lt;/a&gt;wikitten主题&lt;/h3&gt;&lt;h4 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Hexo" scheme="http://example.com/categories/Hexo/"/>
    
    <category term="Theme" scheme="http://example.com/categories/Hexo/Theme/"/>
    
    
    <category term="Hexo" scheme="http://example.com/tags/Hexo/"/>
    
    <category term="Theme" scheme="http://example.com/tags/Theme/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/2020-12-06-Modify-Attributes/"/>
    <id>http://example.com/wiki/2020-12-06-Modify-Attributes/</id>
    <published>2021-03-04T15:17:54.708Z</published>
    <updated>2021-03-04T15:17:54.708Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Add-Replace-Bindings-with-Attributes"><a href="#Add-Replace-Bindings-with-Attributes" class="headerlink" title="Add, Replace Bindings with Attributes"></a>Add, Replace Bindings with Attributes</h1><p>The naming examples discussed how you can use<br><a href="bind.html"><tt>bind()</tt>, <tt>rebind()</tt></a>. The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html"><tt>DirContext</tt></a> interface contains overloaded versions of these methods that accept attributes. You can use these <tt>DirContext</tt> methods to associate attributes with the object at the time that the binding or subcontext is added to the namespace. For example, you might create a <tt>Person</tt> object and bind it to the namespace and at the same time associate attributes about that <tt>Person</tt> object.</p><h2 id="Adding-a-Binding-That-Has-Attributes"><a href="#Adding-a-Binding-That-Has-Attributes" class="headerlink" title="Adding a Binding That Has Attributes"></a><a name="BIND" id="BIND">Adding a Binding That Has Attributes</a></h2><p><a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html#bind-javax.naming.Name-java.lang.Object-javax.naming.directory.Attributes-"><tt>DirContext.bind()</tt></a> is used to add a binding that has attributes to a context. It accepts as arguments the name of the object, the object to be bound, and a set of attributes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#x2F;&#x2F; Create the object to be bound</span><br><span class="line">Fruit fruit &#x3D; new Fruit(&quot;orange&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Create attributes to be associated with the object</span><br><span class="line">Attributes attrs &#x3D; new BasicAttributes(true); &#x2F;&#x2F; case-ignore</span><br><span class="line">Attribute objclass &#x3D; new BasicAttribute(&quot;objectclass&quot;);</span><br><span class="line">objclass.add(&quot;top&quot;);</span><br><span class="line">objclass.add(&quot;organizationalUnit&quot;);</span><br><span class="line">attrs.put(objclass);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Perform bind</span><br><span class="line">ctx.bind(&quot;ou&#x3D;favorite, ou&#x3D;Fruits&quot;, fruit, attrs);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="examples/Bind.java"><code>This example</code></a> creates an object of class </p><p><a href="examples/Fruit.java"><code>&lt;tt&gt;Fruit&lt;/tt&gt;</code></a><br> and binds it to the name <tt>“ou=favorite”</tt> into the context named <tt>“ou=Fruits”</tt>, relative to <tt>ctx</tt>. This binding has the <tt>“objectclass”</tt> attribute. If you subsequently looked up the name <tt>“ou=favorite, ou=Fruits”</tt> in <tt>ctx</tt>, then you would get the <tt>fruit</tt> object. If you then got the attributes of <tt>“ou=favorite, ou=Fruits”</tt>, you would get those attributes with which the object was created. Following is this example’s output.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># java Bind</span><br><span class="line">orange</span><br><span class="line">attribute: objectclass</span><br><span class="line">value: top</span><br><span class="line">value: organizationalUnit</span><br><span class="line">value: javaObject</span><br><span class="line">value: javaNamingReference</span><br><span class="line">attribute: javaclassname</span><br><span class="line">value: Fruit</span><br><span class="line">attribute: javafactory</span><br><span class="line">value: FruitFactory</span><br><span class="line">attribute: javareferenceaddress</span><br><span class="line">value: #0#fruit#orange</span><br><span class="line">attribute: ou</span><br><span class="line">value: favorite</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The extra attributes and attribute values shown are used to store information about the object (<tt>fruit</tt>). These extra attributes are discussed in more detail in the<br> trail.</p><p>If you were to run this example twice, then the second attempt would fail with a<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/NameAlreadyBoundException.html"><tt>NameAlreadyBoundException</tt></a>. This is because the name <tt>“ou=favorite”</tt> is already bound in the <tt>“ou=Fruits”</tt> context. For the second attempt to succeed, you would have to use <tt>rebind()</tt>.</p><h2 id="Replacing-a-Binding-That-Has-Attributes"><a href="#Replacing-a-Binding-That-Has-Attributes" class="headerlink" title="Replacing a Binding That Has Attributes"></a><a name="REBIND" id="REBIND">Replacing a Binding That Has Attributes</a></h2><p><a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html#rebind-javax.naming.Name-java.lang.Object-javax.naming.directory.Attributes-"><tt>DirContext.rebind()</tt></a> is used to add or replace a binding and its attributes. It accepts the same arguments as <tt>bind()</tt>. However, <tt>rebind()</tt>‘s semantics require that if the name is already bound, then it will be unbound and the newly given object and attributes will be bound.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#x2F;&#x2F; Create the object to be bound</span><br><span class="line">Fruit fruit &#x3D; new Fruit(&quot;lemon&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Create attributes to be associated with the object</span><br><span class="line">Attributes attrs &#x3D; new BasicAttributes(true); &#x2F;&#x2F; case-ignore</span><br><span class="line">Attribute objclass &#x3D; new BasicAttribute(&quot;objectclass&quot;);</span><br><span class="line">objclass.add(&quot;top&quot;);</span><br><span class="line">objclass.add(&quot;organizationalUnit&quot;);</span><br><span class="line">attrs.put(objclass);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Perform bind</span><br><span class="line">ctx.rebind(&quot;ou&#x3D;favorite, ou&#x3D;Fruits&quot;, fruit, attrs);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>When you run<br><a href="examples/Rebind.java"><code>this example</code></a><br>, it replaces the binding that the<br><a href="examples/Bind.java"><code>&lt;tt&gt;bind()&lt;/tt&gt;</code></a><br> example created.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># java Rebind</span><br><span class="line">lemon</span><br><span class="line">attribute: objectclass</span><br><span class="line">value: top</span><br><span class="line">value: organizationalUnit</span><br><span class="line">value: javaObject</span><br><span class="line">value: javaNamingReference</span><br><span class="line">attribute: javaclassname</span><br><span class="line">value: Fruit</span><br><span class="line">attribute: javafactory</span><br><span class="line">value: FruitFactory</span><br><span class="line">attribute: javareferenceaddress</span><br><span class="line">value: #0#fruit#lemon</span><br><span class="line">attribute: ou</span><br><span class="line">value: favorite</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Add-Replace-Bindings-with-Attributes&quot;&gt;&lt;a href=&quot;#Add-Replace-Bindings-with-Attributes&quot; class=&quot;headerlink&quot; title=&quot;Add, Replace Binding</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/2020-12-06-Retrieving-and-Modifying-Values-from-Result-Sets/"/>
    <id>http://example.com/wiki/2020-12-06-Retrieving-and-Modifying-Values-from-Result-Sets/</id>
    <published>2021-03-04T15:17:53.994Z</published>
    <updated>2021-03-04T15:17:54.091Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Retrieving-and-Modifying-Values-from-Result-Sets"><a href="#Retrieving-and-Modifying-Values-from-Result-Sets" class="headerlink" title="Retrieving and Modifying Values from Result Sets"></a>Retrieving and Modifying Values from Result Sets</h1><p>The following method, <code>[CoffeesTable.viewTable](gettingstarted.html)</code> outputs the contents of the <code>COFFEES</code> tables, and demonstrates the use of <code>ResultSet</code> objects and cursors:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public static void viewTable(Connection con) throws SQLException &#123;</span><br><span class="line">  String query &#x3D; &quot;select COF_NAME, SUP_ID, PRICE, SALES, TOTAL from COFFEES&quot;;</span><br><span class="line">  try (Statement stmt &#x3D; con.createStatement()) &#123;</span><br><span class="line">    ResultSet rs &#x3D; stmt.executeQuery(query);</span><br><span class="line">    while (rs.next()) &#123;</span><br><span class="line">      String coffeeName &#x3D; rs.getString(&quot;COF_NAME&quot;);</span><br><span class="line">      int supplierID &#x3D; rs.getInt(&quot;SUP_ID&quot;);</span><br><span class="line">      float price &#x3D; rs.getFloat(&quot;PRICE&quot;);</span><br><span class="line">      int sales &#x3D; rs.getInt(&quot;SALES&quot;);</span><br><span class="line">      int total &#x3D; rs.getInt(&quot;TOTAL&quot;);</span><br><span class="line">      System.out.println(coffeeName + &quot;, &quot; + supplierID + &quot;, &quot; + price +</span><br><span class="line">                         &quot;, &quot; + sales + &quot;, &quot; + total);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (SQLException e) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printSQLException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>A <code>ResultSet</code> object is a table of data representing a database result set, which is usually generated by executing a statement that queries the database. For example, the <code>[CoffeeTables.viewTable](gettingstarted.html)</code> method creates a <code>ResultSet</code>, <code>rs</code>, when it executes the query through the <code>Statement</code> object, <code>stmt</code>. Note that a <code>ResultSet</code> object can be created through any object that implements the <code>Statement</code> interface, including <code>PreparedStatement</code>, <code>CallableStatement</code>, and <code>RowSet</code>.</p><p>You access the data in a <code>ResultSet</code> object through a cursor. Note that this cursor is not a database cursor. This cursor is a pointer that points to one row of data in the <code>ResultSet</code>. Initially, the cursor is positioned before the first row. The method <code>ResultSet.next</code> moves the cursor to the next row. This method returns <code>false</code> if the cursor is positioned after the last row. This method repeatedly calls the <code>ResultSet.next</code> method with a <code>while</code> loop to iterate through all the data in the <code>ResultSet</code>.</p><p>This page covers the following topics:</p><ul><li><a href="#rs_interface">ResultSet Interface</a></li><li><a href="#retrieve_rs">Retrieving Column Values from Rows</a></li><li><a href="#cursors">Cursors</a></li><li><a href="#rs_update">Updating Rows in ResultSet Objects</a></li><li><a href="#batch_updates">Using Statement Objects for Batch Updates</a></li><li><a href="#rs_insert">Inserting Rows in ResultSet Objects</a></li></ul><h2 id="ResultSet-Interface"><a href="#ResultSet-Interface" class="headerlink" title="ResultSet Interface"></a><a name="rs_interface" id="rs_interface">ResultSet Interface</a></h2><p>The <code>ResultSet</code> interface provides methods for retrieving and manipulating the results of executed queries, and <code>ResultSet</code> objects can have different functionality and characteristics. These characteristics are type, concurrency, and cursor <strong>holdability</strong>.</p><h3 id="ResultSet-Types"><a href="#ResultSet-Types" class="headerlink" title="ResultSet Types"></a>ResultSet Types</h3><p>The type of a <code>ResultSet</code> object determines the level of its functionality in two areas: the ways in which the cursor can be manipulated, and how concurrent changes made to the underlying data source are reflected by the <code>ResultSet</code> object.</p><p>The sensitivity of a <code>ResultSet</code> object is determined by one of three different <code>ResultSet</code> types:</p><ul><li><code>TYPE_FORWARD_ONLY</code>: The result set cannot be scrolled; its cursor moves forward only, from before the first row to after the last row. The rows contained in the result set depend on how the underlying database generates the results. That is, it contains the rows that satisfy the query at either the time the query is executed or as the rows are retrieved.</li><li><code>TYPE_SCROLL_INSENSITIVE</code>: The result can be scrolled; its cursor can move both forward and backward relative to the current position, and it can move to an absolute position. The result set is insensitive to changes made to the underlying data source while it is open. It contains the rows that satisfy the query at either the time the query is executed or as the rows are retrieved.</li><li><code>TYPE_SCROLL_SENSITIVE</code>: The result can be scrolled; its cursor can move both forward and backward relative to the current position, and it can move to an absolute position. The result set reflects changes made to the underlying data source while the result set remains open.</li></ul><p>The default <code>ResultSet</code> type is <code>TYPE_FORWARD_ONLY</code>.</p><p><strong>Note</strong>: Not all databases and JDBC drivers support all <code>ResultSet</code> types. The method <code>DatabaseMetaData.supportsResultSetType</code> returns <code>true</code> if the specified <code>ResultSet</code> type is supported and <code>false</code> otherwise.</p><h3 id="ResultSet-Concurrency"><a href="#ResultSet-Concurrency" class="headerlink" title="ResultSet Concurrency"></a>ResultSet Concurrency</h3><p>The concurrency of a <code>ResultSet</code> object determines what level of update functionality is supported.</p><p>There are two concurrency levels:</p><ul><li><code>CONCUR_READ_ONLY</code>: The <code>ResultSet</code> object cannot be updated using the <code>ResultSet</code> interface.</li><li><code>CONCUR_UPDATABLE</code>: The <code>ResultSet</code> object can be updated using the <code>ResultSet</code> interface.</li></ul><p>The default <code>ResultSet</code> concurrency is <code>CONCUR_READ_ONLY</code>.</p><p><strong>Note</strong>: Not all JDBC drivers and databases support concurrency. The method <code>DatabaseMetaData.supportsResultSetConcurrency</code> returns <code>true</code> if the specified concurrency level is supported by the driver and <code>false</code> otherwise.</p><p>The method <code>[CoffeesTable.modifyPrices](gettingstarted.html)</code> demonstrates how to use a <code>ResultSet</code> object whose concurrency level is <code>CONCUR_UPDATABLE</code>.</p><h3 id="Cursor-Holdability"><a href="#Cursor-Holdability" class="headerlink" title="Cursor Holdability"></a>Cursor Holdability</h3><p>Calling the method <code>Connection.commit</code> can close the <code>ResultSet</code> objects that have been created during the current transaction. In some cases, however, this may not be the desired behavior. The <code>ResultSet</code> property <strong>holdability</strong> gives the application control over whether <code>ResultSet</code> objects (cursors) are closed when commit is called.</p><p>The following <code>ResultSet</code> constants may be supplied to the <code>Connection</code> methods <code>createStatement</code>, <code>prepareStatement</code>, and <code>prepareCall</code>:</p><ul><li><code>HOLD_CURSORS_OVER_COMMIT</code>: <code>ResultSet</code> cursors are not closed; they are <strong>holdable</strong>: they are held open when the method <code>commit</code> is called. Holdable cursors might be ideal if your application uses mostly read-only <code>ResultSet</code> objects.</li><li><code>CLOSE_CURSORS_AT_COMMIT</code>: <code>ResultSet</code> objects (cursors) are closed when the <code>commit</code> method is called. Closing cursors when this method is called can result in better performance for some applications.</li></ul><p>The default cursor holdability varies depending on your DBMS.</p><p><strong>Note</strong>: Not all JDBC drivers and databases support holdable and non-holdable cursors. The following method, <code>JDBCTutorialUtilities.cursorHoldabilitySupport</code>, outputs the default cursor holdability of <code>ResultSet</code> objects and whether <code>HOLD_CURSORS_OVER_COMMIT</code> and <code>CLOSE_CURSORS_AT_COMMIT</code> are supported:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public static void cursorHoldabilitySupport(Connection conn)</span><br><span class="line">    throws SQLException &#123;</span><br><span class="line"></span><br><span class="line">    DatabaseMetaData dbMetaData &#x3D; conn.getMetaData();</span><br><span class="line">    System.out.println(&quot;ResultSet.HOLD_CURSORS_OVER_COMMIT &#x3D; &quot; +</span><br><span class="line">        ResultSet.HOLD_CURSORS_OVER_COMMIT);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;ResultSet.CLOSE_CURSORS_AT_COMMIT &#x3D; &quot; +</span><br><span class="line">        ResultSet.CLOSE_CURSORS_AT_COMMIT);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;Default cursor holdability: &quot; +</span><br><span class="line">        **dbMetaData.getResultSetHoldability()**);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;Supports HOLD_CURSORS_OVER_COMMIT? &quot; +</span><br><span class="line">        &lt;strong&gt;dbMetaData.supportsResultSetHoldability(</span><br><span class="line">            ResultSet.HOLD_CURSORS_OVER_COMMIT)&lt;&#x2F;strong&gt;);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;Supports CLOSE_CURSORS_AT_COMMIT? &quot; +</span><br><span class="line">        &lt;strong&gt;dbMetaData.supportsResultSetHoldability(</span><br><span class="line">            ResultSet.CLOSE_CURSORS_AT_COMMIT)&lt;&#x2F;strong&gt;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Retrieving-Column-Values-from-Rows"><a href="#Retrieving-Column-Values-from-Rows" class="headerlink" title="Retrieving Column Values from Rows"></a><a name="retrieve_rs" id="retrieve_rs">Retrieving Column Values from Rows</a></h2><p>The <code>ResultSet</code> interface declares getter methods (for example, <code>getBoolean</code> and <code>getLong</code>) for retrieving column values from the current row. You can retrieve values using either the index number of the column or the alias or name of the column. The column index is usually more efficient. Columns are numbered from 1. For maximum portability, result set columns within each row should be read in left-to-right order, and each column should be read only once.</p><p>For example, the following method, <code>[CoffeesTable.alternateViewTable](gettingstarted.html)</code>, retrieves column values by number:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public static void alternateViewTable(Connection con) throws SQLException &#123;</span><br><span class="line">  String query &#x3D; &quot;select COF_NAME, SUP_ID, PRICE, SALES, TOTAL from COFFEES&quot;;</span><br><span class="line">  try (Statement stmt &#x3D; con.createStatement()) &#123;</span><br><span class="line">    ResultSet rs &#x3D; stmt.executeQuery(query);</span><br><span class="line">    while (rs.next()) &#123;</span><br><span class="line">      String coffeeName &#x3D; rs.getString(1);</span><br><span class="line">      int supplierID &#x3D; rs.getInt(2);</span><br><span class="line">      float price &#x3D; rs.getFloat(3);</span><br><span class="line">      int sales &#x3D; rs.getInt(4);</span><br><span class="line">      int total &#x3D; rs.getInt(5);</span><br><span class="line">      System.out.println(coffeeName + &quot;, &quot; + supplierID + &quot;, &quot; + price +</span><br><span class="line">                         &quot;, &quot; + sales + &quot;, &quot; + total);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (SQLException e) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printSQLException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Strings used as input to getter methods are case-insensitive. When a getter method is called with a string and more than one column has the same alias or name as the string, the value of the first matching column is returned. The option to use a string as opposed to an integer is designed to be used when column aliases and names are used in the SQL query that generated the result set. For columns that are <strong>not</strong> explicitly named in the query (for example, <code>select * from COFFEES</code>) it is best to use column numbers. If column names are used, the developer should guarantee that they uniquely refer to the intended columns by using column aliases. A column alias effectively renames the column of a result set. To specify a column alias, use the SQL <code>AS</code> clause in the <code>SELECT</code> statement.</p><p>The getter method of the appropriate type retrieves the value in each column. For example, in the method <code>[CoffeeTables.viewTable](gettingstarted.html)</code>, the first column in each row of the <code>ResultSet</code> <code>rs</code> is <code>COF_NAME</code>, which stores a value of SQL type <code>VARCHAR</code>. The method for retrieving a value of SQL type <code>VARCHAR</code> is <code>getString</code>. The second column in each row stores a value of SQL type <code>INTEGER</code>, and the method for retrieving values of that type is <code>getInt</code>.</p><p>Note that although the method <code>getString</code> is recommended for retrieving the SQL types <code>CHAR</code> and <code>VARCHAR</code>, it is possible to retrieve any of the basic SQL types with it. Getting all values with <code>getString</code> can be very useful, but it also has its limitations. For instance, if it is used to retrieve a numeric type, <code>getString</code> converts the numeric value to a Java <code>String</code> object, and the value has to be converted back to a numeric type before it can be operated on as a number. In cases where the value is treated as a string anyway, there is no drawback. Furthermore, if you want an application to retrieve values of any standard SQL type other than SQL3 types, use the <code>getString</code> method.</p><h2 id="Cursors"><a href="#Cursors" class="headerlink" title="Cursors"></a><a name="cursors" id="cursors">Cursors</a></h2><p>As mentioned previously, you access the data in a <code>ResultSet</code> object through a cursor, which points to one row in the <code>ResultSet</code> object. However, when a <code>ResultSet</code> object is first created, the cursor is positioned before the first row. The method <code>[CoffeeTables.viewTable](gettingstarted.html)</code> moves the cursor by calling the <code>ResultSet.next</code> method. There are other methods available to move the cursor:</p><ul><li><code>next</code>: Moves the cursor forward one row. Returns <code>true</code> if the cursor is now positioned on a row and <code>false</code> if the cursor is positioned after the last row.</li><li><code>previous</code>: Moves the cursor backward one row. Returns <code>true</code> if the cursor is now positioned on a row and <code>false</code> if the cursor is positioned before the first row.</li><li><code>first</code>: Moves the cursor to the first row in the <code>ResultSet</code> object. Returns <code>true</code> if the cursor is now positioned on the first row and <code>false</code> if the <code>ResultSet</code> object does not contain any rows.</li><li><code>last:</code>: Moves the cursor to the last row in the <code>ResultSet</code> object. Returns <code>true</code> if the cursor is now positioned on the last row and <code>false</code> if the <code>ResultSet</code> object does not contain any rows.</li><li><code>beforeFirst</code>: Positions the cursor at the start of the <code>ResultSet</code> object, before the first row. If the <code>ResultSet</code> object does not contain any rows, this method has no effect.</li><li><code>afterLast</code>: Positions the cursor at the end of the <code>ResultSet</code> object, after the last row. If the <code>ResultSet</code> object does not contain any rows, this method has no effect.</li><li><code>relative(int rows)</code>: Moves the cursor relative to its current position.</li><li><code>absolute(int row)</code>: Positions the cursor on the row specified by the parameter <code>row</code>.</li></ul><p>Note that the default sensitivity of a <code>ResultSet</code> is <code>TYPE_FORWARD_ONLY</code>, which means that it cannot be scrolled; you cannot call any of these methods that move the cursor, except <code>next</code>, if your <code>ResultSet</code> cannot be scrolled. The method <code>[CoffeesTable.modifyPrices](gettingstarted.html)</code>, described in the following section, demonstrates how you can move the cursor of a <code>ResultSet</code>.</p><h2 id="Updating-Rows-in-ResultSet-Objects"><a href="#Updating-Rows-in-ResultSet-Objects" class="headerlink" title="Updating Rows in ResultSet Objects"></a><a name="rs_update" id="rs_update">Updating Rows in ResultSet Objects</a></h2><p>You cannot update a default <code>ResultSet</code> object, and you can only move its cursor forward. However, you can create <code>ResultSet</code> objects that can be scrolled (the cursor can move backwards or move to an absolute position) and updated.</p><p>The following method, <code>[CoffeesTable.modifyPrices](gettingstarted.html)</code>, multiplies the <code>PRICE</code> column of each row by the argument <code>percentage</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void modifyPrices(float percentage) throws SQLException &#123;</span><br><span class="line">  try (Statement stmt &#x3D;</span><br><span class="line">    con.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE, ResultSet.CONCUR_UPDATABLE)) &#123;</span><br><span class="line">    ResultSet uprs &#x3D; stmt.executeQuery(&quot;SELECT * FROM COFFEES&quot;);</span><br><span class="line">    while (uprs.next()) &#123;</span><br><span class="line">      float f &#x3D; uprs.getFloat(&quot;PRICE&quot;);</span><br><span class="line">      uprs.updateFloat(&quot;PRICE&quot;, f * percentage);</span><br><span class="line">      uprs.updateRow();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (SQLException e) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printSQLException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The field <code>ResultSet.TYPE_SCROLL_SENSITIVE</code> creates a <code>ResultSet</code> object whose cursor can move both forward and backward relative to the current position and to an absolute position. The field <code>ResultSet.CONCUR_UPDATABLE</code> creates a <code>ResultSet</code> object that can be updated. See the <code>ResultSet</code> Javadoc for other fields you can specify to modify the behavior of <code>ResultSet</code> objects.</p><p>The method <code>ResultSet.updateFloat</code> updates the specified column (in this example, <code>PRICE</code> with the specified <code>float</code> value in the row where the cursor is positioned. <code>ResultSet</code> contains various updater methods that enable you to update column values of various data types. However, none of these updater methods modifies the database; you must call the method <code>ResultSet.updateRow</code> to update the database.</p><h2 id="Using-Statement-Objects-for-Batch-Updates"><a href="#Using-Statement-Objects-for-Batch-Updates" class="headerlink" title="Using Statement Objects for Batch Updates"></a><a name="batch_updates" id="batch_updates">Using Statement Objects for Batch Updates</a></h2><p><code>Statement</code>, <code>PreparedStatement</code> and <code>CallableStatement</code> objects have a list of commands that is associated with them. This list may contain statements for updating, inserting, or deleting a row; and it may also contain DDL statements such as <code>CREATE TABLE</code> and <code>DROP TABLE</code>. It cannot, however, contain a statement that would produce a <code>ResultSet</code> object, such as a <code>SELECT</code> statement. In other words, the list can contain only statements that produce an update count.</p><p>The list, which is associated with a <code>Statement</code> object at its creation, is initially empty. You can add SQL commands to this list with the method <code>addBatch</code> and empty it with the method <code>clearBatch</code>. When you have finished adding statements to the list, call the method <code>executeBatch</code> to send them all to the database to be executed as a unit, or batch.</p><p>For example, the following method <code>[CoffeesTable.batchUpdate](gettingstarted.html)</code> adds four rows to the <code>COFFEES</code> table with a batch update:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void batchUpdate() throws SQLException &#123;</span><br><span class="line">  con.setAutoCommit(false);</span><br><span class="line">  try (Statement stmt &#x3D; con.createStatement()) &#123;</span><br><span class="line"></span><br><span class="line">    stmt.addBatch(&quot;INSERT INTO COFFEES &quot; +</span><br><span class="line">                  &quot;VALUES(&#39;Amaretto&#39;, 49, 9.99, 0, 0)&quot;);</span><br><span class="line">    stmt.addBatch(&quot;INSERT INTO COFFEES &quot; +</span><br><span class="line">                  &quot;VALUES(&#39;Hazelnut&#39;, 49, 9.99, 0, 0)&quot;);</span><br><span class="line">    stmt.addBatch(&quot;INSERT INTO COFFEES &quot; +</span><br><span class="line">                  &quot;VALUES(&#39;Amaretto_decaf&#39;, 49, 10.99, 0, 0)&quot;);</span><br><span class="line">    stmt.addBatch(&quot;INSERT INTO COFFEES &quot; +</span><br><span class="line">                  &quot;VALUES(&#39;Hazelnut_decaf&#39;, 49, 10.99, 0, 0)&quot;);</span><br><span class="line"></span><br><span class="line">    int[] updateCounts &#x3D; stmt.executeBatch();</span><br><span class="line">    con.commit();</span><br><span class="line">  &#125; catch (BatchUpdateException b) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printBatchUpdateException(b);</span><br><span class="line">  &#125; catch (SQLException ex) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printSQLException(ex);</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    con.setAutoCommit(true);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The following line disables auto-commit mode for the <code>Connection</code> object con so that the transaction will not be automatically committed or rolled back when the method <code>executeBatch</code> is called.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">con.setAutoCommit(false);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To allow for correct error handling, you should always disable auto-commit mode before beginning a batch update.</p><p>The method <code>Statement.addBatch</code> adds a command to the list of commands associated with the <code>Statement</code> object <code>stmt</code>. In this example, these commands are all <code>INSERT INTO</code> statements, each one adding a row consisting of five column values. The values for the columns <code>COF_NAME</code> and <code>PRICE</code> are the name of the coffee and its price, respectively. The second value in each row is 49 because that is the identification number for the supplier, Superior Coffee. The last two values, the entries for the columns <code>SALES</code> and <code>TOTAL</code>, all start out being zero because there have been no sales yet. (<code>SALES</code> is the number of pounds of this row’s coffee sold in the current week; <code>TOTAL</code> is the total of all the cumulative sales of this coffee.)</p><p>The following line sends the four SQL commands that were added to its list of commands to the database to be executed as a batch:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int[] updateCounts &#x3D; stmt.executeBatch();</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Note that <code>stmt</code> uses the method <code>executeBatch</code> to send the batch of insertions, not the method <code>executeUpdate</code>, which sends only one command and returns a single update count. The DBMS executes the commands in the order in which they were added to the list of commands, so it will first add the row of values for Amaretto, then add the row for Hazelnut, then Amaretto decaf, and finally Hazelnut decaf. If all four commands execute successfully, the DBMS will return an update count for each command in the order in which it was executed. The update counts that indicate how many rows were affected by each command are stored in the array <code>updateCounts</code>.</p><p>If all four of the commands in the batch are executed successfully, <code>updateCounts</code> will contain four values, all of which are 1 because an insertion affects one row. The list of commands associated with <code>stmt</code> will now be empty because the four commands added previously were sent to the database when <code>stmt</code> called the method <code>executeBatch</code>. You can at any time explicitly empty this list of commands with the method <code>clearBatch</code>.</p><p>The <code>Connection.commit</code> method makes the batch of updates to the <code>COFFEES</code> table permanent. This method needs to be called explicitly because the auto-commit mode for this connection was disabled previously.</p><p>The following line enables auto-commit mode for the current <code>Connection</code> object.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">con.setAutoCommit(true);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now each statement in the example will automatically be committed after it is executed, and it no longer needs to invoke the method <code>commit</code>.</p><h3 id="Performing-Parameterized-Batch-Update"><a href="#Performing-Parameterized-Batch-Update" class="headerlink" title="Performing Parameterized Batch Update"></a>Performing Parameterized Batch Update</h3><p>It is also possible to have a parameterized batch update, as shown in the following code fragment, where <code>con</code> is a <code>Connection</code> object:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">con.setAutoCommit(false);</span><br><span class="line">PreparedStatement pstmt &#x3D; con.prepareStatement(</span><br><span class="line">                              &quot;INSERT INTO COFFEES VALUES( &quot; +</span><br><span class="line">                              &quot;?, ?, ?, ?, ?)&quot;);</span><br><span class="line">pstmt.setString(1, &quot;Amaretto&quot;);</span><br><span class="line">pstmt.setInt(2, 49);</span><br><span class="line">pstmt.setFloat(3, 9.99);</span><br><span class="line">pstmt.setInt(4, 0);</span><br><span class="line">pstmt.setInt(5, 0);</span><br><span class="line">pstmt.addBatch();</span><br><span class="line"></span><br><span class="line">pstmt.setString(1, &quot;Hazelnut&quot;);</span><br><span class="line">pstmt.setInt(2, 49);</span><br><span class="line">pstmt.setFloat(3, 9.99);</span><br><span class="line">pstmt.setInt(4, 0);</span><br><span class="line">pstmt.setInt(5, 0);</span><br><span class="line">pstmt.addBatch();</span><br><span class="line"></span><br><span class="line">&lt;strong&gt;&#x2F;&#x2F; ... and so on for each new</span><br><span class="line">&#x2F;&#x2F; type of coffee&lt;&#x2F;strong&gt;</span><br><span class="line"></span><br><span class="line">int[] updateCounts &#x3D; pstmt.executeBatch();</span><br><span class="line">con.commit();</span><br><span class="line">con.setAutoCommit(true);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Handling-Batch-Update-Exceptions"><a href="#Handling-Batch-Update-Exceptions" class="headerlink" title="Handling Batch Update Exceptions"></a>Handling Batch Update Exceptions</h3><p>You will get a <code>BatchUpdateException</code> when you call the method <code>executeBatch</code> if (1) one of the SQL statements you added to the batch produces a result set (usually a query) or (2) one of the SQL statements in the batch does not execute successfully for some other reason.</p><p>You should not add a query (a <code>SELECT</code> statement) to a batch of SQL commands because the method <code>executeBatch</code>, which returns an array of update counts, expects an update count from each SQL statement that executes successfully. This means that only commands that return an update count (commands such as <code>INSERT INTO</code>, <code>UPDATE</code>, <code>DELETE</code>) or that return 0 (such as <code>CREATE TABLE</code>, <code>DROP TABLE</code>, <code>ALTER TABLE</code>) can be successfully executed as a batch with the <code>executeBatch</code> method.</p><p>A <code>BatchUpdateException</code> contains an array of update counts that is similar to the array returned by the method <code>executeBatch</code>. In both cases, the update counts are in the same order as the commands that produced them. This tells you how many commands in the batch executed successfully and which ones they are. For example, if five commands executed successfully, the array will contain five numbers: the first one being the update count for the first command, the second one being the update count for the second command, and so on.</p><p><code>BatchUpdateException</code> is derived from <code>SQLException</code>. This means that you can use all of the methods available to an <code>SQLException</code> object with it. The following method, <code>[JDBCTutorialUtilities.printBatchUpdateException](gettingstarted.html)</code> prints all of the <code>SQLException</code> information plus the update counts contained in a <code>BatchUpdateException</code> object. Because <code>BatchUpdateException.getUpdateCounts</code> returns an array of <code>int</code>, the code uses a <code>for</code> loop to print each of the update counts:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public static void printBatchUpdateException(BatchUpdateException b) &#123;</span><br><span class="line">  System.err.println(&quot;----BatchUpdateException----&quot;);</span><br><span class="line">  System.err.println(&quot;SQLState:  &quot; + b.getSQLState());</span><br><span class="line">  System.err.println(&quot;Message:  &quot; + b.getMessage());</span><br><span class="line">  System.err.println(&quot;Vendor:  &quot; + b.getErrorCode());</span><br><span class="line">  System.err.print(&quot;Update counts:  &quot;);</span><br><span class="line">  int[] updateCounts &#x3D; b.getUpdateCounts();</span><br><span class="line">  for (int i &#x3D; 0; i &lt; updateCounts.length; i++) &#123;</span><br><span class="line">    System.err.print(updateCounts[i] + &quot;   &quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Inserting-Rows-in-ResultSet-Objects"><a href="#Inserting-Rows-in-ResultSet-Objects" class="headerlink" title="Inserting Rows in ResultSet Objects"></a><a name="rs_insert" id="rs_insert">Inserting Rows in ResultSet Objects</a></h2><p><strong>Note</strong>: Not all JDBC drivers support inserting new rows with the <code>ResultSet</code> interface. If you attempt to insert a new row and your JDBC driver database does not support this feature, a <code>SQLFeatureNotSupportedException</code> exception is thrown.</p><p>The following method, <code>[CoffeesTable.insertRow](gettingstarted.html)</code>, inserts a row into the <code>COFFEES</code> through a <code>ResultSet</code> object:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void insertRow(String coffeeName, int supplierID, float price,</span><br><span class="line">                      int sales, int total) throws SQLException &#123;</span><br><span class="line">  </span><br><span class="line">  try (Statement stmt &#x3D;</span><br><span class="line">        con.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE, ResultSet.CONCUR_UPDATABLE))</span><br><span class="line">  &#123;      </span><br><span class="line">    ResultSet uprs &#x3D; stmt.executeQuery(&quot;SELECT * FROM COFFEES&quot;);</span><br><span class="line">    uprs.moveToInsertRow();</span><br><span class="line">    uprs.updateString(&quot;COF_NAME&quot;, coffeeName);</span><br><span class="line">    uprs.updateInt(&quot;SUP_ID&quot;, supplierID);</span><br><span class="line">    uprs.updateFloat(&quot;PRICE&quot;, price);</span><br><span class="line">    uprs.updateInt(&quot;SALES&quot;, sales);</span><br><span class="line">    uprs.updateInt(&quot;TOTAL&quot;, total);</span><br><span class="line"></span><br><span class="line">    uprs.insertRow();</span><br><span class="line">    uprs.beforeFirst();</span><br><span class="line"></span><br><span class="line">  &#125; catch (SQLException e) &#123;</span><br><span class="line">    JDBCTutorialUtilities.printSQLException(e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This example calls the <code>Connection.createStatement</code> method with two arguments, <code>ResultSet.TYPE_SCROLL_SENSITIVE</code> and <code>ResultSet.CONCUR_UPDATABLE</code>. The first value enables the cursor of the <code>ResultSet</code> object to be moved both forward and backward. The second value, <code>ResultSet.CONCUR_UPDATABLE</code>, is required if you want to insert rows into a <code>ResultSet</code> object; it specifies that it can be updatable.</p><p>The same stipulations for using strings in getter methods also apply to updater methods.</p><p>The method <code>ResultSet.moveToInsertRow</code> moves the cursor to the insert row. The insert row is a special row associated with an updatable result set. It is essentially a buffer where a new row can be constructed by calling the updater methods prior to inserting the row into the result set. For example, this method calls the method <code>ResultSet.updateString</code> to update the insert row’s <code>COF_NAME</code> column to <code>Kona</code>.</p><p>The method <code>ResultSet.insertRow</code> inserts the contents of the insert row into the <code>ResultSet</code> object and into the database.</p><p><strong>Note</strong>: After inserting a row with the <code>ResultSet.insertRow</code>, you should move the cursor to a row other than the insert row. For example, this example moves it to before the first row in the result set with the method <code>ResultSet.beforeFirst</code>. Unexpected results can occur if another part of your application uses the same result set and the cursor is still pointing to the insert row.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Retrieving-and-Modifying-Values-from-Result-Sets&quot;&gt;&lt;a href=&quot;#Retrieving-and-Modifying-Values-from-Result-Sets&quot; class=&quot;headerlink&quot; tit</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/2020-12-06-Modifying-the-Look-and-Feel/"/>
    <id>http://example.com/wiki/2020-12-06-Modifying-the-Look-and-Feel/</id>
    <published>2021-03-04T15:17:52.371Z</published>
    <updated>2021-03-04T15:17:52.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lesson-Modifying-the-Look-and-Feel"><a href="#Lesson-Modifying-the-Look-and-Feel" class="headerlink" title="Lesson: Modifying the Look and Feel"></a>Lesson: Modifying the Look and Feel</h1><p>This lesson tells you how to change the look and feel of your Swing application. The “look” of an application refers to its appearance.  The “feel” refers to how the widgets behave. You can choose to use the default Swing look and feel (e.g. the Ocean theme for the Metal look and feel), or the look and feel of the native platform (e.g. Windows, GTK+), or you can customize your own look and feel.</p><h2 id="How-to-Set-the-Look-and-Feel"><a href="#How-to-Set-the-Look-and-Feel" class="headerlink" title="How to Set the Look and Feel"></a><a href="plaf.html">How to Set the Look and Feel</a></h2><p>Basic information on setting the look and feel to one of the available look and feels. Information on setting the look and feel programmatically, and at the command line, is provided.</p><h2 id="The-Synth-Look-and-Feel"><a href="#The-Synth-Look-and-Feel" class="headerlink" title="The Synth Look and Feel"></a><a href="synth.html">The Synth Look and Feel</a></h2><p>Information on how to customize your look and feel using the Synth package is provided.</p><h2 id="The-Nimbus-Look-and-Feel"><a href="#The-Nimbus-Look-and-Feel" class="headerlink" title="The Nimbus Look and Feel"></a><a href="nimbus.html">The Nimbus Look and Feel</a></h2><p>Information on how to customize your look and feel using the Nimbus package, introduced in the JDK 7 release, is provided.</p><p>If you are interested in using JavaFX to create your GUI, see<br><a href="https://docs.oracle.com/javase/8/javafx/user-interface-tutorial/css_tutorial.htm">Skinning JavaFX Applications with CSS</a> and<br><a href="https://docs.oracle.com/javase/8/javafx/user-interface-tutorial/css-styles.htm">Using JavaFX Charts</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Lesson-Modifying-the-Look-and-Feel&quot;&gt;&lt;a href=&quot;#Lesson-Modifying-the-Look-and-Feel&quot; class=&quot;headerlink&quot; title=&quot;Lesson: Modifying the Lo</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Using Sequencer Methods</title>
    <link href="http://example.com/wiki/2021-03-04-Using%20Sequencer%20Methods/"/>
    <id>http://example.com/wiki/2021-03-04-Using%20Sequencer%20Methods/</id>
    <published>2021-03-04T14:32:16.571Z</published>
    <updated>2021-03-04T15:17:55.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Using-Sequencer-Methods"><a href="#Using-Sequencer-Methods" class="headerlink" title="Using Sequencer Methods"></a>Using Sequencer Methods</h1><p>The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Sequencer.html"><code>Sequencer</code></a> interface provides methods in several categories:</p><ul><li><a name="124593" id="124593"></a>Methods to load sequence data from a MIDI file or a <code>Sequence</code> object, and to save the currently loaded sequence data to a MIDI file.</li><li><a name="124594" id="124594"></a>Methods analogous to the transport functions of a tape recorder, for stopping and starting playback and recording, enabling and disabling recording on specific tracks, and shuttling the current playback or recording position in a <code>Sequence</code>.</li><li><a name="124595" id="124595"></a>Advanced methods for querying and setting the synchronization and timing parameters of the object. A <code>Sequencer</code> may play at different tempos, with some <code>Tracks</code> muted, and in various synchronization states with other objects.</li><li><a name="124596" id="124596"></a>Advanced methods for registering “listener” objects that are notified when the <code>Sequencer</code> processes certain kinds of MIDI events.</li></ul><p><a name="124597" id="124597"></a> Regardless of which <code>Sequencer</code> methods you’ll invoke, the first step is to obtain a <code>Sequencer</code> device from the system and reserve it for your program’s use.</p><p><a name="124598" id="124598"></a></p><h2 id="Obtaining-a-Sequencer"><a href="#Obtaining-a-Sequencer" class="headerlink" title="Obtaining a Sequencer"></a>Obtaining a Sequencer</h2><p><a name="124599" id="124599"></a> An application program doesn’t instantiate a <code>Sequencer</code>; after all, <code>Sequencer</code> is just an interface. Instead, like all devices in the Java Sound API’s MIDI package, a <code>Sequencer</code> is accessed through the static <code>MidiSystem</code> object. As previously mentioned in<br><a href="accessing-MIDI.html">Accessing MIDI System Resources</a>, the following <code>MidiSystem</code> method can be used to obtain the default <code>Sequencer</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static Sequencer getSequencer()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124604" id="124604"></a> The following code fragment obtains the default <code>Sequencer</code>, acquires any system resources it needs, and makes it operational:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Sequencer sequencer;</span><br><span class="line">&#x2F;&#x2F; Get default sequencer.</span><br><span class="line">sequencer &#x3D; MidiSystem.getSequencer(); </span><br><span class="line">if (sequencer &#x3D;&#x3D; null) &#123;</span><br><span class="line">    &#x2F;&#x2F; Error -- sequencer device is not supported.</span><br><span class="line">    &#x2F;&#x2F; Inform user and return...</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    &#x2F;&#x2F; Acquire resources and make operational.</span><br><span class="line">    sequencer.open();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124615" id="124615"></a> The invocation of <code>open</code> reserves the sequencer device for your program’s use. It doesn’t make much sense to imagine sharing a sequencer, because it can play only one sequence at a time. When you’re done using the sequencer, you can make it available to other programs by invoking <code>close</code>.</p><p><a name="124616" id="124616"></a> Non-default sequencers can be obtained as described in<br><a href="accessing-MIDI.html">Accessing MIDI System Resources</a>.</p><p><a name="124620" id="124620"></a></p><h2 id="Loading-a-Sequence"><a href="#Loading-a-Sequence" class="headerlink" title="Loading a Sequence"></a>Loading a Sequence</h2><p><a name="124621" id="124621"></a> Having obtained a sequencer from the system and reserved it, you then need load the data that the sequencer should play. There are three typical ways of accomplishing this:</p><ul><li><a name="124622" id="124622"></a>Reading the sequence data from a MIDI file</li><li><a name="124623" id="124623"></a>Recording it in real time by receiving MIDI messages from another device, such as a MIDI input port</li><li><a name="124624" id="124624"></a>Building it programmatically “from scratch” by adding tracks to an empty sequence and adding <code>MidiEvent</code> objects to those tracks</li></ul><p><a href="#124654">Recording and Saving Sequences</a> and <a href="#124674">Editing a Sequence</a>, respectively.) This first way actually encompasses two slightly different approaches. One approach is to feed MIDI file data to an <code>InputStream</code> that you then read directly to the sequencer by means of <code>Sequencer.setSequence(InputStream)</code>. With this approach, you don’t explicitly create a <code>Sequence</code> object. In fact, the <code>Sequencer</code> implementation might not even create a <code>Sequence</code> behind the scenes, because some sequencers have a built-in mechanism for handling data directly from a file.</p><p><a name="124632" id="124632"></a> The other approach is to create a <code>Sequence</code> explicitly. You’ll need to use this approach if you’re going to edit the sequence data before playing it. With this approach, you invoke <code>MidiSystem&#39;s</code> overloaded method <code>getSequence</code>. The method is able to get the sequence from an <code>InputStream</code>, a <code>File</code>, or a <code>URL</code>. The method returns a <code>Sequence</code> object that can then be loaded into a <code>Sequencer</code> for playback. Expanding on the previous code excerpt, here’s an example of obtaining a <code>Sequence</code> object from a <code>File</code> and loading it into our <code>sequencer</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">    File myMidiFile &#x3D; new File(&quot;seq1.mid&quot;);</span><br><span class="line">    &#x2F;&#x2F; Construct a Sequence object, and</span><br><span class="line">    &#x2F;&#x2F; load it into my sequencer.</span><br><span class="line">    Sequence mySeq &#x3D; MidiSystem.getSequence(myMidiFile);</span><br><span class="line">    sequencer.setSequence(mySeq);</span><br><span class="line">&#125; catch (Exception e) &#123;</span><br><span class="line">   &#x2F;&#x2F; Handle error and&#x2F;or return</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124642" id="124642"></a> Like <code>MidiSystem&#39;s</code> <code>getSequence</code> method, <code>setSequence</code> may throw an <code>InvalidMidiDataException</code>&#226;&#128;&#148;and, in the case of the <code>InputStream</code> variant, an <code>IOException</code>&#226;&#128;&#148;if it runs into any trouble.</p><p><a name="124643" id="124643"></a></p><h2 id="Playing-a-Sequence"><a href="#Playing-a-Sequence" class="headerlink" title="Playing a Sequence"></a>Playing a Sequence</h2><p><a name="124644" id="124644"></a> Starting and stopping a <code>Sequencer</code> is accomplished using the following methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void start()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124646" id="124646"></a> and</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124648" id="124648"></a> The <code>Sequencer.start</code> method begins playback of the sequence. Note that playback starts at the current position in a sequence. Loading an existing sequence using the <code>setSequence</code> method, described above, initializes the sequencer’s current position to the very beginning of the sequence. The <code>stop</code> method stops the sequencer, but it does not automatically rewind the current <code>Sequence</code>. Starting a stopped <code>Sequence</code> without resetting the position simply resumes playback of the sequence from the current position. In this case, the <code>stop</code> method has served as a pause operation. However, there are various <code>Sequencer</code> methods for setting the current sequence position to an arbitrary value before playback is started. (We’ll discuss these methods below.)</p><p><a name="124649" id="124649"></a> As mentioned earlier, a <code>Sequencer</code> typically has one or more <code>Transmitter</code> objects, through which it sends <code>MidiMessages</code> to a <code>Receiver</code>. It is through these <code>Transmitters</code> that a <code>Sequencer</code> plays the <code>Sequence</code>, by emitting appropriately timed <code>MidiMessages</code> that correspond to the <code>MidiEvents</code> contained in the current <code>Sequence</code>. Therefore, part of the setup procedure for playing back a <code>Sequence</code> is to invoke the <code>setReceiver</code> method on the <code>Sequencer&#39;s</code> <code>Transmitter</code> object, in effect wiring its output to the device that will make use of the played-back data. For more details on <code>Transmitters</code> and <code>Receivers</code>, refer back to<br><a href="MIDI-messages.html">Transmitting and Receiving MIDI Messages</a>.</p><p><a name="124654" id="124654"></a></p><h2 id="Recording-and-Saving-Sequences"><a href="#Recording-and-Saving-Sequences" class="headerlink" title="Recording and Saving Sequences"></a>Recording and Saving Sequences</h2><p><a name="124655" id="124655"></a> To capture MIDI data to a <code>Sequence</code>, and subsequently to a file, you need to perform some additional steps beyond those described above. The following outline shows the steps necessary to set up for recording to a <code>Track</code> in a <code>Sequence</code>:</p><ol><li><a name="124656" id="124656"></a>Use <code>MidiSystem.getSequencer</code> to get a new sequencer to use for recording, as above.</li><li><a name="124657" id="124657"></a>Set up the “wiring” of the MIDI connections. The object that is transmitting the MIDI data to be recorded should be configured, through its <code>setReceiver</code> method, to send data to a <code>Receiver</code> associated with the recording <code>Sequencer</code>.<li><a name="124658" id="124658"></a>Create a new `Sequence` object, which will store the recorded data. When you create the `Sequence` object, you must specify the global timing information for the sequence. For example:<pre><code>   Sequence mySeq;   try&#123;       mySeq = new Sequence(Sequence.PPQ, 10);   &#125; catch (Exception ex) &#123;        ex.printStackTrace();    &#125;</code></pre><a name="124666" id="124666"></a> The constructor for `Sequence` takes as arguments a `divisionType` and a timing resolution. The `divisionType` argument specifies the units of the resolution argument. In this case, we've specified that the timing resolution of the `Sequence` we're creating will be 10 pulses per quarter note. An additional optional argument to the `Sequence` constructor is a number of tracks argument, which would cause the initial sequence to begin with the specified number of (initially empty) `Tracks`. Otherwise the `Sequence` will be created with no initial `Tracks`; they can be added later as needed.</li></li><li><a name="124667" id="124667"></a>Create an empty <code>Track</code> in the <code>Sequence</code>, with <code>Sequence.createTrack</code>. This step is unnecessary if the <code>Sequence</code> was created with initial <code>Tracks</code>.</li><li><a name="124668" id="124668"></a>Using <code>Sequencer.setSequence</code>, select our new <code>Sequence</code> to receive the recording. The <code>setSequence</code> method ties together an existing <code>Sequence</code> with the <code>Sequencer</code>, which is somewhat analogous to loading a tape onto a tape recorder.</li><li><a name="124669" id="124669"></a>Invoke <code>Sequencer.recordEnable</code> for each <code>Track</code> to be recorded. If necessary, get a reference to the available <code>Tracks</code> in the <code>Sequence</code> by invoking <code>Sequence.getTracks</code>.</li><li><a name="124670" id="124670"></a>Invoke <code>startRecording</code> on the <code>Sequencer</code>.</li><li><a name="124671" id="124671"></a>When done recording, invoke <code>Sequencer.stop</code> or <code>Sequencer.stopRecording</code>.</li><li><a name="124672" id="124672"></a>Save the recorded <code>Sequence</code> to a MIDI file with <code>MidiSystem.write</code>. The <code>write</code> method of <code>MidiSystem</code> takes a <code>Sequence</code> as one of its arguments, and will write that <code>Sequence</code> to a stream or file.</li></ol><p><a name="124674" id="124674"></a></p><h2 id="Editing-a-Sequence"><a href="#Editing-a-Sequence" class="headerlink" title="Editing a Sequence"></a>Editing a Sequence</h2><p><a name="124675" id="124675"></a> Many application programs allow a sequence to be created by loading it from a file, and quite a few also allow a sequence to be created by capturing it from live MIDI input (that is, recording). Some programs, however, will need to create MIDI sequences from scratch, whether programmatically or in response to user input. Full-featured sequencer programs permit the user to manually construct new sequences, as well as to edit existing ones.</p><p><a name="124676" id="124676"></a> These data-editing operations are achieved in the Java Sound API not by <code>Sequencer</code> methods, but by methods of the data objects themselves: <code>Sequence</code>, <code>Track</code>, and <code>MidiEvent</code>. You can create an empty sequence using one of the <code>Sequence</code> constructors, and then add tracks to it by invoking the following <code>Sequence</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Track createTrack() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If your program allows the user to edit sequences, you’ll need this <code>Sequence</code> method to remove tracks:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean deleteTrack(Track track) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124680" id="124680"></a> Once the sequence contains tracks, you can modify the contents of the tracks by invoking methods of the <code>Track</code> class. The <code>MidiEvents</code> contained in the <code>Track</code> are stored as a <code>java.util.Vector</code> in the <code>Track</code> object, and <code>Track</code> provides a set of methods for accessing, adding, and removing the events in the list. The methods <code>add</code> and <code>remove</code> are fairly self-explanatory, adding or removing a specified <code>MidiEvent</code> from a <code>Track</code>. A <code>get</code> method is provided, which takes an index into the <code>Track&#39;s</code> event list and returns the <code>MidiEvent</code> stored there. In addition, there are <code>size</code> and <code>tick</code> methods, which respectively return the number of <code>MidiEvents</code> in the track, and the track’s duration, expressed as a total number of <code>Ticks</code>.</p><p><a name="124681" id="124681"></a> To create a new event before adding it to the track, you’ll of course use the <code>MidiEvent</code> constructor. To specify or modify the MIDI message embedded in the event, you can invoke the <code>setMessage</code> method of the appropriate <code>MidiMessage</code> subclass (<code>ShortMessage</code>, <code>SysexMessage</code>, or <code>MetaMessage</code>). To modify the time that the event should occur, invoke <code>MidiEvent.setTick</code>.</p><p><a name="124682" id="124682"></a> In combination, these low-level methods provide the basis for the editing functionality needed by a full-featured sequencer program.</p><p><a name="124684" id="124684"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Using-Sequencer-Methods&quot;&gt;&lt;a href=&quot;#Using-Sequencer-Methods&quot; class=&quot;headerlink&quot; title=&quot;Using Sequencer Methods&quot;&gt;&lt;/a&gt;Using Sequencer M</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Using Files and Format Converters</title>
    <link href="http://example.com/wiki/2021-03-04-Using%20Files%20and%20Format%20Converters/"/>
    <id>http://example.com/wiki/2021-03-04-Using%20Files%20and%20Format%20Converters/</id>
    <published>2021-03-04T14:32:16.561Z</published>
    <updated>2021-03-04T15:17:55.858Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Using-Files-and-Format-Converters"><a href="#Using-Files-and-Format-Converters" class="headerlink" title="Using Files and Format Converters"></a>Using Files and Format Converters</h1><p><a name="117603" id="117603"></a> Most application programs that deal with sound need to read sound files or audio streams. This is common functionality, regardless of what the program may subsequently do with the data it reads (such as play, mix, or process it). Similarly, many programs need to write sound files (or streams). In some cases, the data that has been read (or that will be written) needs to be converted to a different format.</p><p><a name="117604" id="117604"></a> As was briefly mentioned in<br><a href="accessing.html">Accessing Audio System Resources</a>, the Java Sound API provides application developers with various facilities for file input/output and format translations. Application programs can read, write, and translate between a variety of sound file formats and audio data formats.</p><p><a name="114517" id="114517"></a><br><a href="sampled-overview.html">Overview of the Sampled Package</a> introduced the main classes related to sound files and audio data formats. As a review:</p><ul><li><a name="114884" id="114884"></a>A stream of audio data, as might be read from or written to a file, is represented by an <code>AudioInputStream</code> object. (<code>AudioInputStream</code> inherits from <code>java.io.InputStream</code>.)<li><a name="114885" id="114885"></a>The format of this audio data is represented by an `AudioFormat` object.<a name="117222" id="117222"></a>This format specifies how the audio samples themselves are arranged, but not the structure of a file that they might be stored in. In other words, an `AudioFormat` describes "raw" audio data, such as the system might hand your program after capturing it from a microphone input or after parsing it from a sound file. An `AudioFormat` includes such information as the encoding, the byte order, the number of channels, the sampling rate, and the number of bits per sample.</li></li><li><a name="114521" id="114521"></a>There are several well-known, standard formats for sound files, such as WAV, AIFF, or AU. The different types of sound file have different structures for storing audio data as well as for storing descriptive information about the audio data. A sound file format is represented in the Java Sound API by an <code>AudioFileFormat</code> object. The <code>AudioFileFormat</code> includes an <code>AudioFormat</code> object to describe the format of the audio data stored in the file, and also includes information about the file type and the length of the data in the file.</li><li><a name="114522" id="114522"></a>The <code>AudioSystem</code> class provides methods for (1) storing a stream of audio data from an <code>AudioInputStream</code> into an audio file of a particular type (in other words, writing a file), (2) extracting a stream of audio bytes (an <code>AudioInputStream</code>) from an audio file (in other words, reading a file), and (3) converting audio data from one data format to another. This page, which is divided into three sections, explains these three kinds of activity.</li></ul><p><a name="114524" id="114524"></a></p><p>an implementation of the Java Sound API does not necessarily provide comprehensive facilities for reading, writing, and converting audio in different data and file formats. It might support only the most common data and file formats. However, service providers can develop and distribute conversion services that extend this set, as you’ll later see in <a href="SPI-providing-sampled.html">Providing Sampled-Audio Services</a>. The <code>AudioSystem</code> class supplies methods that allow application programs to learn what conversions are available, as described later under <a href="#114640">Converting File and Data Formats</a>. <a name="114527" id="114527"></a></p><h2 id="Reading-Sound-Files"><a href="#Reading-Sound-Files" class="headerlink" title="Reading Sound Files"></a>Reading Sound Files</h2><p><a name="114529" id="114529"></a> The <code>AudioSystem</code> class provides two types of file-reading services:</p><ul><li><a name="114530" id="114530"></a>Information about the format of the audio data stored in the sound file</li><li><a name="114531" id="114531"></a>A stream of formatted audio data that can be read from the sound file</li></ul><p><a name="118569" id="118569"></a> The first of these is given by three variants of the <code>getAudioFileFormat</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static AudioFileFormat getAudioFileFormat (java.io.File file)</span><br><span class="line">static AudioFileFormat getAudioFileFormat(java.io.InputStream stream)</span><br><span class="line">static AudioFileFormat getAudioFileFormat (java.net.URL url)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As mentioned above, the returned <code>AudioFileFormat</code> object tells you the file type, the length of the data in the file, encoding, the byte order, the number of channels, the sampling rate, and the number of bits per sample.</p><p><a name="114541" id="114541"></a> The second type of file-reading functionality is given by these <code>AudioSystem</code> methods</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static AudioInputStream getAudioInputStream (java.io.File file)</span><br><span class="line">static AudioInputStream getAudioInputStream (java.net.URL url)</span><br><span class="line">static AudioInputStream getAudioInputStream (java.io.InputStream stream)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods give you an object (an <code>AudioInputStream</code>) that lets you read the file’s audio data, using one of the read methods of <code>AudioInputStream</code>. We’ll see an example momentarily.</p><p><a name="114549" id="114549"></a> Suppose you’re writing a sound-editing application that allows the user to load sound data from a file, display a corresponding waveform or spectrogram, edit the sound, play back the edited data, and save the result in a new file. Or perhaps your program will read the data stored in a file, apply some kind of signal processing (such as an algorithm that slows the sound down without changing its pitch), and then play the processed audio. In either case, you need to get access to the data contained in the audio file. Assuming that your program provides some means for the user to select or specify an input sound file, reading that file’s audio data involves three steps:</p><ol><li><a name="114551" id="114551"></a>Get an <code>AudioInputStream</code> object from the file.</li><li><a name="114552" id="114552"></a>Create a byte array in which you’ll store successive chunks of data from the file.</li><li><a name="114553" id="114553"></a>Repeatedly read bytes from the audio input stream into the array. On each iteration, do something useful with the bytes in the array (for example, you might play them, filter them, analyze them, display them, or write them to another file).</li></ol><p><a name="114555" id="114555"></a> The following code snippet outlines these steps:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int totalFramesRead &#x3D; 0;</span><br><span class="line">File fileIn &#x3D; new File(somePathName);</span><br><span class="line">&#x2F;&#x2F; somePathName is a pre-existing string whose value was</span><br><span class="line">&#x2F;&#x2F; based on a user selection.</span><br><span class="line">try &#123;</span><br><span class="line">  AudioInputStream audioInputStream &#x3D; </span><br><span class="line">    AudioSystem.getAudioInputStream(fileIn);</span><br><span class="line">  int bytesPerFrame &#x3D; </span><br><span class="line">    audioInputStream.getFormat().getFrameSize();</span><br><span class="line">    if (bytesPerFrame &#x3D;&#x3D; AudioSystem.NOT_SPECIFIED) &#123;</span><br><span class="line">    &#x2F;&#x2F; some audio formats may have unspecified frame size</span><br><span class="line">    &#x2F;&#x2F; in that case we may read any amount of bytes</span><br><span class="line">    bytesPerFrame &#x3D; 1;</span><br><span class="line">  &#125; </span><br><span class="line">  &#x2F;&#x2F; Set an arbitrary buffer size of 1024 frames.</span><br><span class="line">  int numBytes &#x3D; 1024 * bytesPerFrame; </span><br><span class="line">  byte[] audioBytes &#x3D; new byte[numBytes];</span><br><span class="line">  try &#123;</span><br><span class="line">    int numBytesRead &#x3D; 0;</span><br><span class="line">    int numFramesRead &#x3D; 0;</span><br><span class="line">    &#x2F;&#x2F; Try to read numBytes bytes from the file.</span><br><span class="line">    while ((numBytesRead &#x3D; </span><br><span class="line">      audioInputStream.read(audioBytes)) !&#x3D; -1) &#123;</span><br><span class="line">      &#x2F;&#x2F; Calculate the number of frames actually read.</span><br><span class="line">      numFramesRead &#x3D; numBytesRead &#x2F; bytesPerFrame;</span><br><span class="line">      totalFramesRead +&#x3D; numFramesRead;</span><br><span class="line">      &#x2F;&#x2F; Here, do something useful with the audio data that&#39;s </span><br><span class="line">      &#x2F;&#x2F; now in the audioBytes array...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (Exception ex) &#123; </span><br><span class="line">    &#x2F;&#x2F; Handle the error...</span><br><span class="line">  &#125;</span><br><span class="line">&#125; catch (Exception e) &#123;</span><br><span class="line">  &#x2F;&#x2F; Handle the error...</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Let’s take a look at what’s happening in the above code sample. First, the outer try clause instantiates an <code>AudioInputStream</code> object through the call to the <code>AudioSystem.getAudioInputStream(File)</code> method. This method transparently performs all of the testing required to determine whether the specified file is actually a sound file of a type that is supported by the Java Sound API. If the file being inspected (<code>fileIn</code> in this example) is not a sound file, or is a sound file of some unsupported type, an <code>UnsupportedAudioFileException</code> exception is thrown. This behavior is convenient, in that the application programmer need not be bothered with testing file attributes, nor with adhering to any file-naming conventions. Instead, the <code>getAudioInputStream</code> method takes care of all the low-level parsing and verification that is required to validate the input file. <a name="114595" id="114595"></a> The outer <code>try</code> clause then creates a byte array, <code>audioBytes</code>, of an arbitrary fixed length. We make sure that its length in bytes equals an integral number of frames, so that we won’t end up reading only part of a frame or, even worse, only part of a sample. This byte array will serve as a buffer to temporarily hold a chunk of audio data as it’s read from the stream. If we knew we would be reading nothing but very short sound files, we could make this array the same length as the data in the file, by deriving the length in bytes from the length in frames, as returned by <code>AudioInputStream&#39;s getFrameLength</code> method. (Actually, we’d probably just use a <code>Clip</code> object instead.) But to avoid running out of memory in the general case, we instead read the file in chunks, one buffer at a time.</p><p><a name="114597" id="114597"></a> The inner <code>try</code> clause contains a <code>while</code> loop, which is where we read the audio data from the <code>AudioInputStream</code> into the byte array. You should add code in this loop to handle the audio data in this array in whatever way is appropriate for your program’s needs. If you’re applying some kind of signal processing to the data, you’ll probably need to query the <code>AudioInputStream&#39;s AudioFormat</code> further, to learn the number of bits per sample and so on.</p><p><a name="114599" id="114599"></a> Note that the method <code>AudioInputStream.read(byte[])</code> returns the number of <strong>bytes</strong> read&#226;&#128;&#148;not the number of samples or frames. This method returns -1 when there’s no more data to read. Upon detecting this condition, we break from the <code>while</code> loop.</p><p><a name="114602" id="114602"></a></p><h2 id="Writing-Sound-Files"><a href="#Writing-Sound-Files" class="headerlink" title="Writing Sound Files"></a>Writing Sound Files</h2><p><a name="114604" id="114604"></a> The previous section described the basics of reading a sound file, using specific methods of the <code>AudioSystem</code> and <code>AudioInputStream</code> classes. This section describes how to write audio data out to a new file.</p><p><a name="114606" id="114606"></a> The following <code>AudioSystem</code> method creates a disk file of a specified file type. The file will contain the audio data that’s in the specified <code>AudioInputStream</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static int write(AudioInputStream in, </span><br><span class="line">  AudioFileFormat.Type fileType, File out)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Note that the second argument must be one of the file types supported by the system (for example, AU, AIFF, or WAV), otherwise the <code>write</code> method will throw an <code>IllegalArgumentException</code>. To avoid this, you can test whether or not a particular <code>AudioInputStream</code> may be written to a particular type of file, by invoking this <code>AudioSystem</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static boolean isFileTypeSupported</span><br><span class="line">  (AudioFileFormat.Type fileType, AudioInputStream stream)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>which will return <code>true</code> only if the particular combination is supported.</p><p><a name="114618" id="114618"></a> More generally, you can learn what types of file the system can write by invoking one of these <code>AudioSystem</code> methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">static AudioFileFormat.Type[] getAudioFileTypes() </span><br><span class="line">static AudioFileFormat.Type[] getAudioFileTypes(AudioInputStream stream) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first of these returns all the types of file that the system can write, and the second returns only those that the system can write from the given audio input stream.</p><p><a name="119705" id="119705"></a> The following excerpt demonstrates one technique for creating an output file from an <code>AudioInputStream</code> using the <code>write</code> method mentioned above.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">File fileOut &#x3D; new File(someNewPathName);</span><br><span class="line">AudioFileFormat.Type fileType &#x3D; fileFormat.getType();</span><br><span class="line">if (AudioSystem.isFileTypeSupported(fileType, </span><br><span class="line">    audioInputStream)) &#123;</span><br><span class="line">  AudioSystem.write(audioInputStream, fileType, fileOut);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first statement above, creates a new <code>File</code> object, <code>fileOut</code>, with a user- or program-specified pathname. The second statement gets a file type from a pre-existing <code>AudioFileFormat</code> object called <code>fileFormat</code>, which might have been obtained from another sound file, such as the one that was read in <a href="#114527">Reading Sound Files</a> above. (You could instead supply whatever supported file type you want, instead of getting the file type from elsewhere. For example, you might delete the second statement and replace the other two occurrences of <code>fileType</code> in the code above with <code>AudioFileFormat.Type.WAVE</code>.)</p><p><a name="114635" id="114635"></a> The third statement tests whether a file of the designated type can be written from a desired <code>AudioInputStream</code>. Like the file format, this stream might have been derived from the sound file previously read. (If so, presumably you’ve processed or altered its data in some way, because otherwise there are easier ways to simply copy a file.) Or perhaps the stream contains bytes that have been freshly captured from the microphone input.</p><p><a name="114637" id="114637"></a> Finally, the stream, file type, and output file are passed to the <code>AudioSystem</code>.<code>write</code> method, to accomplish the goal of writing the file.</p><p><a name="114640" id="114640"></a></p><h2 id="Converting-File-and-Data-Formats"><a href="#Converting-File-and-Data-Formats" class="headerlink" title="Converting File and Data Formats"></a>Converting File and Data Formats</h2><p><a name="114642" id="114642"></a> Recall from<br><a href="sampled-overview.html#formatted">What is Formatted Audio Data?</a>, that the Java Sound API distinguishes between audio <strong>file</strong> formats and audio <strong>data</strong> formats. The two are more or less independent. Roughly speaking, the data format refers to the way in which the computer represents each raw data point (sample), while the file format refers to the organization of a sound file as stored on a disk. Each sound file format has a particular structure that defines, for example, the information stored in the file’s header. In some cases, the file format also includes structures that contain some form of meta-data, in addition to the actual “raw” audio samples. The remainder of this page examines methods of the Java Sound API that enable a variety of file-format and data-format conversions.</p><p><a name="114644" id="114644"></a></p><h2 id="Converting-from-One-File-Format-to-Another"><a href="#Converting-from-One-File-Format-to-Another" class="headerlink" title="Converting from One File Format to Another"></a>Converting from One File Format to Another</h2><p><a name="114646" id="114646"></a> This section covers the fundamentals of converting audio file types in the Java Sound API. Once again we pose a hypothetical program whose purpose, this time, is to read audio data from an arbitrary input file and write it into a file whose type is AIFF. Of course, the input file must be of a type that the system is capable of reading, and the output file must be of a type that the system is capable of writing. (In this example, we assume that the system is capable of writing AIFF files.) The example program doesn’t do any data format conversion. If the input file’s data format can’t be represented as an AIFF file, the program simply notifies the user of that problem. On the other hand, if the input sound file is an already an AIFF file, the program notifies the user that there is no need to convert it.</p><p><a name="114648" id="114648"></a> The following function implements the logic just described:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void ConvertFileToAIFF(String inputPath, </span><br><span class="line">  String outputPath) &#123;</span><br><span class="line">  AudioFileFormat inFileFormat;</span><br><span class="line">  File inFile;</span><br><span class="line">  File outFile;</span><br><span class="line">  try &#123;</span><br><span class="line">    inFile &#x3D; new File(inputPath);</span><br><span class="line">    outFile &#x3D; new File(outputPath);     </span><br><span class="line">  &#125; catch (NullPointerException ex) &#123;</span><br><span class="line">    System.out.println(&quot;Error: one of the </span><br><span class="line">      ConvertFileToAIFF&quot; +&quot; parameters is null!&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line">  try &#123;</span><br><span class="line">    &#x2F;&#x2F; query file type</span><br><span class="line">    inFileFormat &#x3D; AudioSystem.getAudioFileFormat(inFile);</span><br><span class="line">    if (inFileFormat.getType() !&#x3D; AudioFileFormat.Type.AIFF) </span><br><span class="line">    &#123;</span><br><span class="line">      &#x2F;&#x2F; inFile is not AIFF, so let&#39;s try to convert it.</span><br><span class="line">      AudioInputStream inFileAIS &#x3D; </span><br><span class="line">        AudioSystem.getAudioInputStream(inFile);</span><br><span class="line">      inFileAIS.reset(); &#x2F;&#x2F; rewind</span><br><span class="line">      if (AudioSystem.isFileTypeSupported(</span><br><span class="line">             AudioFileFormat.Type.AIFF, inFileAIS)) &#123;</span><br><span class="line">         &#x2F;&#x2F; inFileAIS can be converted to AIFF. </span><br><span class="line">         &#x2F;&#x2F; so write the AudioInputStream to the</span><br><span class="line">         &#x2F;&#x2F; output file.</span><br><span class="line">         AudioSystem.write(inFileAIS,</span><br><span class="line">           AudioFileFormat.Type.AIFF, outFile);</span><br><span class="line">         System.out.println(&quot;Successfully made AIFF file, &quot;</span><br><span class="line">           + outFile.getPath() + &quot;, from &quot;</span><br><span class="line">           + inFileFormat.getType() + &quot; file, &quot; +</span><br><span class="line">           inFile.getPath() + &quot;.&quot;);</span><br><span class="line">         inFileAIS.close();</span><br><span class="line">         return; &#x2F;&#x2F; All done now</span><br><span class="line">       &#125; else</span><br><span class="line">         System.out.println(&quot;Warning: AIFF conversion of &quot; </span><br><span class="line">           + inFile.getPath()</span><br><span class="line">           + &quot; is not currently supported by AudioSystem.&quot;);</span><br><span class="line">    &#125; else</span><br><span class="line">      System.out.println(&quot;Input file &quot; + inFile.getPath() +</span><br><span class="line">          &quot; is AIFF.&quot; + &quot; Conversion is unnecessary.&quot;);</span><br><span class="line">  &#125; catch (UnsupportedAudioFileException e) &#123;</span><br><span class="line">    System.out.println(&quot;Error: &quot; + inFile.getPath()</span><br><span class="line">        + &quot; is not a supported audio file type!&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125; catch (IOException e) &#123;</span><br><span class="line">    System.out.println(&quot;Error: failure attempting to read &quot; </span><br><span class="line">      + inFile.getPath() + &quot;!&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="114706" id="114706"></a> As mentioned, the purpose of this example function, <code>ConvertFileToAIFF</code>, is to query an input file to determine whether it’s an AIFF sound file, and if it isn’t, to try to convert it to one, producing a new copy whose pathname is specified by the second argument. (As an exercise, you might try making this function more general, so that instead of always converting to AIFF, the function converts to the file type specified by a new function argument.) Note that the audio data format of the copy&#226;&#128;&#148;that is, the new file-mimics the audio data format of original input file.</p><p><a name="114708" id="114708"></a> Most of this function is self-explanatory and is not specific to the Java Sound API. There are, however, a few Java Sound API methods used by the routine that are crucial for sound file-type conversions. These method invocations are all found in the second <code>try</code> clause, above, and include the following:</p><ul><li><a name="114710" id="114710"></a><code>AudioSystem.getAudioFileFormat</code>: used here to determine whether the input file is already an AIFF type. If so, the function quickly returns; otherwise the conversion attempt proceeds.</li><li><a name="114712" id="114712"></a><code>AudioSystem.isFileTypeSupported</code>: Indicates whether the system can write a file of the specified type that contains audio data from the specified <code>AudioInputStream.</code> In our example, this method returns <code>true</code> if the specified audio input file can be converted to AIFF audio file format. If <code>AudioFileFormat.Type.AIFF</code> isn’t supported, <code>ConvertFileToAIFF</code> issues a warning that the input file can’t be converted, then returns.</li><li><a name="114714" id="114714"></a><code>AudioSystem.write</code>: used here to write the audio data from the AudioInputStream <code>inFileAIS</code> to the output file <code>outFile</code>.</li></ul><p><a name="114716" id="114716"></a> The second of these methods, <code>isFileTypeSupported</code>, helps to determine, in advance of the write, whether a particular input sound file can be converted to a particular output sound file type. In the next section we will see how, with a few modifications to this <code>ConvertFileToAIFF</code> sample routine, we can convert the audio data format, as well as the sound file type.</p><p><a name="114718" id="114718"></a></p><h2 id="Converting-Audio-between-Different-Data-Formats"><a href="#Converting-Audio-between-Different-Data-Formats" class="headerlink" title="Converting Audio between Different Data Formats"></a>Converting Audio between Different Data Formats</h2><p><a name="114720" id="114720"></a> The previous section showed how to use the Java Sound API to convert a file from one <strong>file</strong> format (that is, one type of sound file) to another. This section explores some of the methods that enable audio <strong>data</strong> format conversions.</p><p><a name="114722" id="114722"></a> In the previous section, we read data from a file of an arbitrary type, and saved it in an AIFF file. Note that although we changed the type of file used to store the data, we didn’t change the format of the audio data itself. (Most common audio file types, including AIFF, can contain audio data of various formats.) So if the original file contained CD-quality audio data (16-bit sample size, 44.1-kHz sample rate, and two channels), so would our output AIFF file.</p><p><a name="114724" id="114724"></a> Now suppose that we want to specify the <strong>data</strong> format of the output file, as well as the file type. For example, perhaps we are saving many long files for use on the Internet, and are concerned about the amount of disk space and download time required by our files. We might choose to create smaller AIFF files that contain lower-resolution data-for example, data that has an 8-bit sample size, an 8-kHz sample rate, and a single channel.</p><p><a name="114726" id="114726"></a> Without going into as much coding detail as before, let’s explore some of the methods used for data format conversion, and consider the modifications that we would need to make to the <code>ConvertFileToAIFF</code> function to accomplish the new goal.</p><p><a name="118599" id="118599"></a> The principal method for audio data conversion is, once again, found in the <code>AudioSystem</code> class. This method is a variant of <code>getAudioInputStream</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioInputStream getAudioInputStream(AudioFormat</span><br><span class="line">    format, AudioInputStream stream)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This function returns an <code>AudioInputStream</code> that is the result of converting the <code>AudioInputStream</code>, <code>stream</code>, using the indicated <code>AudioFormat</code>, <code>format</code>. If the conversion isn’t supported by <code>AudioSystem</code>, this function throws an <code>IllegalArgumentException</code>.</p><p><a name="114734" id="114734"></a> To avoid that, we can first check whether the system can perform the required conversion by invoking this <code>AudioSystem</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isConversionSupported(AudioFormat targetFormat,</span><br><span class="line">    AudioFormat sourceFormat)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this case, we’d pass <code>stream.getFormat()</code> as the second argument.</p><p><a name="119726" id="119726"></a> To create a specific <code>AudioFormat</code> object, we use one of the two <code>AudioFormat</code> constructors shown below, either:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioFormat(float sampleRate, int sampleSizeInBits,</span><br><span class="line">    int channels, boolean signed, boolean bigEndian)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>which constructs an <code>AudioFormat</code> with a linear PCM encoding and the given parameters, or:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioFormat(AudioFormat.Encoding encoding, </span><br><span class="line">    float sampleRate, int sampleSizeInBits, int channels,</span><br><span class="line">    int frameSize, float frameRate, boolean bigEndian) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>which also constructs an <code>AudioFormat</code>, but lets you specify the encoding, frame size, and frame rate, in addition to the other parameters.</p><p><a name="114751" id="114751"></a> Now, armed with the methods above, let’s see how we might extend our <code>ConvertFileToAIFF</code> function to perform the desired “low-res” audio data format conversion. First, we would construct an <code>AudioFormat</code> object describing the desired output audio data format. The following statement would suffice and could be inserted near the top of the function:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioFormat outDataFormat &#x3D; new AudioFormat((float) 8000.0,</span><br><span class="line">(int) 8, (int) 1, true, false);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Since the <code>AudioFormat</code> constructor above is describing a format with 8-bit samples, the last parameter to the constructor, which specifies whether the samples are big or little endian, is irrelevant. (Big versus little endian is only an issue if the sample size is greater than a single byte.)</p><p><a name="114758" id="114758"></a> The following example shows how we would use this new <code>AudioFormat</code> to convert the <code>AudioInputStream</code>, <code>inFileAIS</code>, that we created from the input file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioInputStream lowResAIS;         </span><br><span class="line">  if (AudioSystem.isConversionSupported(outDataFormat,   </span><br><span class="line">    inFileAIS.getFormat())) &#123;</span><br><span class="line">    lowResAIS &#x3D; AudioSystem.getAudioInputStream</span><br><span class="line">      (outDataFormat, inFileAIS);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>It wouldn’t matter too much where we inserted this code, as long as it was after the construction of <code>inFileAIS</code>. Without the <code>isConversionSupported</code> test, the call would fail and throw an <code>IllegalArgumentException</code> if the particular conversion being requested was unsupported. (In this case, control would transfer to the appropriate <code>catch</code> clause in our function.)</p><p><a name="114769" id="114769"></a> So by this point in the process, we would have produced a new <code>AudioInputStream</code>, resulting from the conversion of the original input file (in its <code>AudioInputStream</code> form) to the desired low-resolution audio data format as defined by <code>outDataFormat</code>.</p><p><a name="114771" id="114771"></a> The final step to produce the desired low-resolution, AIFF sound file would be to replace the <code>AudioInputStream</code> parameter in the call to <code>AudioSystem.write</code> (that is, the first parameter) with our converted stream, <code>lowResAIS</code>, as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">AudioSystem.write(lowResAIS, AudioFileFormat.Type.AIFF, </span><br><span class="line">  outFile);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These few modifications to our earlier function produce something that converts both the audio data and the file format of any specified input file, assuming of course that the system supports the conversion. <a name="114777" id="114777"></a></p><h2 id="Learning-What-Conversions-Are-Available"><a href="#Learning-What-Conversions-Are-Available" class="headerlink" title="Learning What Conversions Are Available"></a>Learning What Conversions Are Available</h2><p><a name="114779" id="114779"></a> Several <code>AudioSystem</code> methods test their parameters to determine whether the system supports a particular data format conversion or file-writing operation. (Typically, each method is paired with another that performs the data conversion or writes the file.) One of these query methods, <code>AudioSystem.isFileTypeSupported</code>, was used in our example function, <code>ConvertFileToAIFF</code>, to determine whether the system was capable of writing the audio data to an AIFF file. A related <code>AudioSystem</code> method, <code>getAudioFileTypes(AudioInputStream)</code>, returns the complete list of supported file types for the given stream, as an array of <code>AudioFileFormat.Type</code> instances. The method: BEGINCODE boolean isConversionSupported(AudioFormat.Encoding encoding,<br /><br>AudioFormat format) </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Using-Files-and-Format-Converters&quot;&gt;&lt;a href=&quot;#Using-Files-and-Format-Converters&quot; class=&quot;headerlink&quot; title=&quot;Using Files and Format Con</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Using Advanced Sequencer Features</title>
    <link href="http://example.com/wiki/2021-03-04-Using%20Advanced%20Sequencer%20Features/"/>
    <id>http://example.com/wiki/2021-03-04-Using%20Advanced%20Sequencer%20Features/</id>
    <published>2021-03-04T14:32:16.505Z</published>
    <updated>2021-03-04T15:17:55.854Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Using-Advanced-Sequencer-Features"><a href="#Using-Advanced-Sequencer-Features" class="headerlink" title="Using Advanced Sequencer Features"></a>Using Advanced Sequencer Features</h1><p><a name="124685" id="124685"></a> So far, we’ve focused on simple playback and recording of MIDI data. This section will briefly describe some of the more advanced features available through methods of the <code>Sequencer</code> interface and the <code>Sequence</code> class.</p><p><a name="124686" id="124686"></a></p><h2 id="Moving-to-an-Arbitrary-Position-in-the-Sequence"><a href="#Moving-to-an-Arbitrary-Position-in-the-Sequence" class="headerlink" title="Moving to an Arbitrary Position in the Sequence"></a>Moving to an Arbitrary Position in the Sequence</h2><p><a name="124687" id="124687"></a> There are two <code>Sequencer</code> methods that obtain the sequencer’s current position in the sequence. The first of these:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">long getTickPosition()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124689" id="124689"></a> returns the position measured in MIDI ticks from the beginning of the sequence. The second method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">long getMicrosecondPosition()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>returns the current position in microseconds. This method assumes that the sequence is being played at the default rate as stored in the MIDI file or in the <code>Sequence</code>. It does <strong>not</strong> return a different value if you’ve changed the playback speed as described below.</p><p><a name="124692" id="124692"></a> You can similarly set the sequencer’s current position according to one unit or the other:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setTickPosition(long tick)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setMicrosecondPosition(long microsecond)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124696" id="124696"></a></p><h2 id="Changing-the-Playback-Speed"><a href="#Changing-the-Playback-Speed" class="headerlink" title="Changing the Playback Speed"></a>Changing the Playback Speed</h2><p><a name="124697" id="124697"></a> As indicated earlier, a sequence’s speed is indicated by its tempo, which can vary over the course of the sequence. A sequence can contain events that encapsulate standard MIDI tempo-change messages. When the sequencer processes such an event, it changes the speed of playback to reflect the indicated tempo. In addition, you can programmatically change the tempo by invoking any of these <code>Sequencer</code> methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void setTempoInBPM(float bpm)</span><br><span class="line">public void setTempoInMPQ(float mpq)</span><br><span class="line">public void setTempoFactor(float factor)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first two of these methods set the tempo in beats per minute or microseconds per quarter note, respectively. The tempo will stay at the specified value until one of these methods is invoked again, or until a tempo-change event is encountered in the sequence, at which point the current tempo is overridden by the newly specified one.</p><p><a name="124702" id="124702"></a> The third method, <code>setTempoFactor</code>, is different in nature. It scales whatever tempo is set for the sequencer (whether by tempo-change events or by one of the first two methods above). The default scalar is 1.0 (no change). Although this method causes the playback or recording to be faster or slower than the nominal tempo (unless the factor is 1.0), it doesn’t alter the nominal tempo. In other words, the tempo values returned by <code>getTempoInBPM</code> and <code>getTempoInMPQ</code> are unaffected by the tempo factor, even though the tempo factor does affect the actual rate of playback or recording. Also, if the tempo is changed by a tempo-change event or by one of the first two methods, it still gets scaled by whatever tempo factor was last set. If you load a new sequence, however, the tempo factor is reset to 1.0.</p><p><a name="124703" id="124703"></a> Note that all these tempo-change directives are ineffectual when the sequence’s division type is one of the SMPTE types, instead of PPQ.</p><p><a name="124704" id="124704"></a></p><h2 id="Muting-or-Soloing-Individual-Tracks-in-the-Sequence"><a href="#Muting-or-Soloing-Individual-Tracks-in-the-Sequence" class="headerlink" title="Muting or Soloing Individual Tracks in the Sequence"></a>Muting or Soloing Individual Tracks in the Sequence</h2><p><a name="124705" id="124705"></a> It’s often convenient for users of sequencers to be able to turn off certain tracks, to hear more clearly exactly what is happening in the music. A full-featured sequencer program lets the user choose which tracks should sound during playback. (Speaking more precisely, since sequencers don’t actually create sound themselves, the user chooses which tracks will contribute to the stream of MIDI messages that the sequencer produces.) Typically, there are two types of graphical controls on each track: a <strong>mute</strong> button and a <strong>solo</strong> button. If the mute button is activated, that track will not sound under any circumstances, until the mute button is deactivated. Soloing is a less well-known feature. It’s roughly the opposite of muting. If the solo button on any track is activated, only tracks whose solo buttons are activated will sound. This feature lets the user quickly audition a small number of tracks without having to mute all the other tracks. The mute button typically takes priority over the solo button: if both are activated, the track doesn’t sound.</p><p><a name="124706" id="124706"></a> Using <code>Sequencer</code> methods, muting or soloing tracks (as well as querying a track’s current mute or solo state) is easily accomplished. Let’s assume we have obtained the default <code>Sequencer</code> and that we’ve loaded sequence data into it. Muting the fifth track in the sequence would be accomplished as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sequencer.setTrackMute(4, true);</span><br><span class="line">boolean muted &#x3D; sequencer.getTrackMute(4);</span><br><span class="line">if (!muted) &#123; </span><br><span class="line">    return;         &#x2F;&#x2F; muting failed</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>There are a couple of things to note about the above code snippet. First, tracks of a sequence are numbered starting with 0 and ending with the total number of tracks minus 1. Also, the second argument to <code>setTrackMute</code> is a boolean. If it’s true, the request is to mute the track; otherwise the request is to unmute the specified track. Lastly, in order to test that the muting took effect, we invoke the <code>Sequencer getTrackMute</code> method, passing it the track number we’re querying. If it returns <code>true</code>, as we’d expect in this case, then the mute request worked. If it returns <code>false</code>, then it failed.</p><p><a name="124713" id="124713"></a> Mute requests may fail for various reasons. For example, the track number specified in the <code>setTrackMute</code> call might exceed the total number of tracks, or the sequencer might not support muting. By calling <code>getTrackMute</code>, we can determine if our request succeeded or failed.</p><p><a name="124714" id="124714"></a> As an aside, the boolean that’s returned by <code>getTrackMute</code> can, indeed, tell us if a failure occurred, but it can’t tell us why it occurred. We could test to see if a failure was caused by passing an invalid track number to the <code>setTrackMute</code> method. To do this, we would call the <code>getTracks</code> method of <code>Sequence</code>, which returns an array containing all of the tracks in the sequence. If the track number specified in the <code>setTrackMute</code> call exceeds the length of this array, then we know we specified an invalid track number.</p><p><a name="124715" id="124715"></a> If the mute request succeeded, then in our example, the fifth track will not sound when the sequence is playing, nor will any other tracks that are currently muted.</p><p><a name="124716" id="124716"></a> The method and techniques for soloing a track are very similar to those for muting. To solo a track, invoke the <code>setTrackSolo</code> method of <code>Sequence:</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setTrackSolo(int track, boolean bSolo)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As in <code>setTrackMute</code>, the first argument specifies the zero-based track number, and the second argument, if <code>true</code>, specifies that the track should be in solo mode; otherwise the track should not be soloed.</p><p><a name="124719" id="124719"></a> By default, a track is neither muted nor soloed.</p><p><a name="124721" id="124721"></a></p><h2 id="Synchronizing-with-Other-MIDI-Devices"><a href="#Synchronizing-with-Other-MIDI-Devices" class="headerlink" title="Synchronizing with Other MIDI Devices"></a>Synchronizing with Other MIDI Devices</h2><p><a name="124722" id="124722"></a> <code>Sequencer</code> has an inner class called <code>Sequencer.SyncMode</code>. A <code>SyncMode</code> object represents one of the ways in which a MIDI sequencer’s notion of time can be synchronized with a master or slave device. If the sequencer is being synchronized to a master, the sequencer revises its current time in response to certain MIDI messages from the master. If the sequencer has a slave, the sequencer similarly sends MIDI messages to control the slave’s timing.</p><p><a name="124723" id="124723"></a> There are three predefined modes that specify possible masters for a sequencer: <code>INTERNAL_CLOCK</code>, <code>MIDI_SYNC</code>, and <code>MIDI_TIME_CODE</code>. The latter two work if the sequencer receives MIDI messages from another device. In these two modes, the sequencer’s time gets reset based on system real-time timing clock messages or MIDI time code (MTC) messages, respectively. (See the MIDI specification for more information about these types of message.) These two modes can also be used as slave modes, in which case the sequencer sends the corresponding types of MIDI messages to its receiver. A fourth mode, <code>NO_SYNC</code>, is used to indicate that the sequencer should not send timing information to its receivers.</p><p><a name="124724" id="124724"></a> By calling the <code>setMasterSyncMode</code> method with a supported <code>SyncMode</code> object as the argument, you can specify how the sequencer’s timing is controlled. Likewise, the <code>setSlaveSyncMode</code> method determines what timing information the sequencer will send to its receivers. This information controls the timing of devices that use the sequencer as a master timing source.</p><p><a name="124725" id="124725"></a></p><h2 id="Specifying-Special-Event-Listeners"><a href="#Specifying-Special-Event-Listeners" class="headerlink" title="Specifying Special Event Listeners"></a>Specifying Special Event Listeners</h2><p><a name="124726" id="124726"></a> Each track of a sequence can contain many different kinds of <code>MidiEvents</code>. Such events include Note On and Note Off messages, program changes, control changes, and meta events. The Java Sound API specifies “listener” interfaces for the last two of these event types (control change events and meta events). You can use these interfaces to receive notifications when such events occur during playback of a sequence.</p><p><a name="124727" id="124727"></a> Objects that support the <code>ControllerEventListener</code> interface can receive notification when a <code>Sequencer</code> processes particular control-change messages. A control-change message is a standard type of MIDI message that represents a change in the value of a MIDI controller, such as a pitch-bend wheel or a data slider. (See the MIDI specification for the complete list of control-change messages.) When such a message is processed during playback of a sequence, the message instructs any device (probably a synthesizer) that’s receiving the data from the sequencer to update the value of some parameter. The parameter usually controls some aspect of sound synthesis, such as the pitch of the currently sounding notes if the controller was the pitch-bend wheel. When a sequence is being recorded, the control-change message means that a controller on the external physical device that created the message has been moved, or that such a move has been simulated in software.</p><p><a name="124728" id="124728"></a> Here’s how the <code>ControllerEventListener</code> interface is used. Let’s assume that you’ve developed a class that implements the <code>ControllerEventListener</code> interface, meaning that your class contains the following method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void controlChange(ShortMessage msg)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Let’s also assume that you’ve created an instance of your class and assigned it to a variable called <code>myListener</code>. If you include the following statements somewhere within your program:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int[] controllersOfInterest &#x3D; &#123; 1, 2, 4 &#125;;</span><br><span class="line">sequencer.addControllerEventListener(myListener,</span><br><span class="line">    controllersOfInterest);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>then your class’s <code>controlChange</code> method will be invoked every time the sequencer processes a control-change message for MIDI controller numbers 1, 2, or 4. In other words, when the <code>Sequencer</code> processes a request to set the value of any of the registered controllers, the <code>Sequencer</code> will invoke your class’s <code>controlChange</code> method. (Note that the assignments of MIDI controller numbers to specific control devices is detailed in the MIDI 1.0 Specification.)</p><p><a name="124734" id="124734"></a> The <code>controlChange</code> method is passed a <code>ShortMessage</code> containing the controller number affected, and the new value to which the controller was set. You can obtain the controller number using the <code>ShortMessage.getData1</code> method, and the new setting of the controller’s value using the <code>ShortMessage.getData2</code> method.</p><p><a name="124735" id="124735"></a> The other kind of special event listener is defined by the <code>MetaEventListener</code> interface. Meta messages, according to the Standard MIDI Files 1.0 specification, are messages that are not present in MIDI wire protocol but that can be embedded in a MIDI file. They are not meaningful to a synthesizer, but can be interpreted by a sequencer. Meta messages include instructions (such as tempo change commands), lyrics or other text, and other indicators (such as end-of-track).</p><p><a name="124736" id="124736"></a> The <code>MetaEventListener</code> mechanism is analogous to <code>ControllerEventListener</code>. Implement the <code>MetaEventListener</code> interface in any class whose instances need to be notified when a <code>MetaMessage</code> is processed by the sequencer. This involves adding the following method to the class:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void meta(MetaMessage msg)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124738" id="124738"></a> You register an instance of this class by passing it as the argument to the <code>Sequencer addMetaEventListener</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean b &#x3D; sequencer.addMetaEventListener</span><br><span class="line">        (myMetaListener);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This is slightly different from the approach taken by the <code>ControllerEventListener</code> interface, because you have to register to receive all <code>MetaMessages,</code> not just selected ones of interest. If the sequencer encounters a <code>MetaMessage</code> in its sequence, it will invoke <code>myMetaListener.meta</code>, passing it the <code>MetaMessage</code> encountered. The <code>meta</code> method can invoke <code>getType</code> on its <code>MetaMessage</code> argument to obtain an integer from 0 to 127 that indicates the message type, as defined by the Standard MIDI Files 1.0 specification.</p><p>&#160;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Using-Advanced-Sequencer-Features&quot;&gt;&lt;a href=&quot;#Using-Advanced-Sequencer-Features&quot; class=&quot;headerlink&quot; title=&quot;Using Advanced Sequencer F</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Transmitting and Receiving MIDI Messages</title>
    <link href="http://example.com/wiki/2021-03-04-Transmitting%20and%20Receiving%20MIDI%20Messages/"/>
    <id>http://example.com/wiki/2021-03-04-Transmitting%20and%20Receiving%20MIDI%20Messages/</id>
    <published>2021-03-04T14:32:16.496Z</published>
    <updated>2021-03-04T15:17:55.848Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transmitting-and-Receiving-MIDI-Messages"><a href="#Transmitting-and-Receiving-MIDI-Messages" class="headerlink" title="Transmitting and Receiving MIDI Messages"></a>Transmitting and Receiving MIDI Messages</h1><p><a name="120461" id="120461"></a></p><h2 id="Understanding-Devices-Transmitters-and-Receivers"><a href="#Understanding-Devices-Transmitters-and-Receivers" class="headerlink" title="Understanding Devices, Transmitters, and Receivers"></a>Understanding Devices, Transmitters, and Receivers</h2><p>The Java Sound API specifies a message-routing architecture for MIDI data that’s flexible and easy to use, once you understand how it works. The system is based on a module-connection design: distinct modules, each of which performs a specific task, can be interconnected (networked), enabling data to flow from one module to another.</p><p><a name="121778" id="121778"></a> The base module in the Java Sound API’s messaging system is the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiDevice.html"><code>MidiDevice</code></a> interface. <code>MidiDevices</code> include sequencers (which record, play, load, and edit sequences of time-stamped MIDI messages), synthesizers (which generate sounds when triggered by MIDI messages), and MIDI input and output ports, through which data comes from and goes to external MIDI devices. The functionality typically required of MIDI ports is described by the base <code>MidiDevice</code> interface. The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Sequencer.html"><code>Sequencer</code></a> and<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Synthesizer.html"><code>Synthesizer</code></a> interfaces extend the <code>MidiDevice</code> interface to describe the additional functionality characteristic of MIDI sequencers and synthesizers, respectively. Concrete classes that function as sequencers or synthesizers should implement these interfaces.</p><p><a name="121816" id="121816"></a> A <code>MidiDevice</code> typically owns one or more ancillary objects that implement the <code>Receiver</code> or <code>Transmitter</code> interfaces. These interfaces represent the “plugs” or “portals” that connect devices together, permitting data to flow into and out of them. By connecting a <code>Transmitter</code> of one <code>MidiDevice</code> to a <code>Receiver</code> of another, you can create a network of modules in which data flows from one to another.</p><p><a name="121833" id="121833"></a> The <code>MidiDevice</code> interface includes methods for determining how many transmitter and receiver objects the device can support concurrently, and other methods for accessing those objects. A MIDI output port normally has at least one <code>Receiver</code> through which the outgoing messages may be received; similarly, a synthesizer normally responds to messages sent to its <code>Receiver</code> or <code>Receivers</code>. A MIDI input port normally has at least one <code>Transmitter</code>, which propagates the incoming messages. A full-featured sequencer supports both <code>Receivers</code>, which receive messages during recording, and <code>Transmitters</code>, which send messages during playback.</p><p><a name="120470" id="120470"></a> The <code>Transmitter</code> interface includes methods for setting and querying the receivers to which the transmitter sends its <code>MidiMessages</code>. Setting the receiver establishes the connection between the two. The <code>Receiver</code> interface contains a method that sends a <code>MidiMessage</code> to the receiver. Typically, this method is invoked by a <code>Transmitter</code>. Both the <code>Transmitter</code> and <code>Receiver</code> interfaces include a <code>close</code> method that frees up a previously connected transmitter or receiver, making it available for a different connection.</p><p><a name="120474" id="120474"></a> We’ll now examine how to use transmitters and receivers. Before getting to the typical case of connecting two devices (such as hooking a sequencer to a synthesizer), we’ll examine the simpler case where you send a MIDI message directly from your application program to a device. Studying this simple scenario should make it easier to understand how the Java Sound API arranges for sending MIDI messages between two devices.</p><p><a name="sending" id="sending"></a></p><h2 id="Sending-a-Message-to-a-Receiver-without-Using-a-Transmitter"><a href="#Sending-a-Message-to-a-Receiver-without-Using-a-Transmitter" class="headerlink" title="Sending a Message to a Receiver without Using a Transmitter"></a>Sending a Message to a Receiver without Using a Transmitter</h2><p><a name="121187" id="121187"></a> Let’s say you want to create a MIDI message from scratch and then send it to some receiver. You can create a new, blank <code>ShortMessage</code> and then fill it with MIDI data using the following <code>ShortMessage</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setMessage(int command, int channel, int data1,</span><br><span class="line">         int data2) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Once you have a message ready to send, you can send it to a <code>Receiver</code> object, using this <code>Receiver</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void send(MidiMessage message, long timeStamp)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The time-stamp argument will be explained momentarily. For now, we’ll just mention that its value can be set to -1 if you don’t care about specifying a precise time. In this case, the device receiving the message will try to respond to the message as soon as possible.</p><p><a name="121753" id="121753"></a> An application program can obtain a receiver for a <code>MidiDevice</code> by invoking the device’s <code>getReceiver</code> method. If the device can’t provide a receiver to the program (typically because all the device’s receivers are already in use), a <code>MidiUnavailableException</code> is thrown. Otherwise, the receiver returned from this method is available for immediate use by the program. When the program has finished using the receiver, it should call the receiver’s <code>close</code> method. If the program attempts to invoke methods on a receiver after calling <code>close</code>, an <code>IllegalStateException</code> may be thrown.</p><p><a name="121858" id="121858"></a> As a concrete simple example of sending a message without using a transmitter, let’s send a Note On message to the default receiver, which is typically associated with a device such as the MIDI output port or a synthesizer. We do this by creating a suitable <code>ShortMessage</code> and passing it as an argument to <code>Receiver&#39;s</code> <code>send</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ShortMessage myMsg &#x3D; new ShortMessage();</span><br><span class="line">&#x2F;&#x2F; Start playing the note Middle C (60), </span><br><span class="line">&#x2F;&#x2F; moderately loud (velocity &#x3D; 93).</span><br><span class="line">myMsg.setMessage(ShortMessage.NOTE_ON, 0, 60, 93);</span><br><span class="line">long timeStamp &#x3D; -1;</span><br><span class="line">Receiver       rcvr &#x3D; MidiSystem.getReceiver();</span><br><span class="line">rcvr.send(myMsg, timeStamp);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This code uses a static integer field of <code>ShortMessage</code>, namely, <code>NOTE_ON</code>, for use as the MIDI message’s status byte. The other parts of the MIDI message are given explicit numeric values as arguments to the <code>setMessage</code> method. The zero indicates that the note is to be played using MIDI channel number 1; the 60 indicates the note Middle C; and the 93 is an arbitrary key-down velocity value, which typically indicates that the synthesizer that eventually plays the note should play it somewhat loudly. (The MIDI specification leaves the exact interpretation of velocity up to the synthesizer’s implementation of its current instrument.) This MIDI message is then sent to the receiver with a time stamp of -1. We now need to examine exactly what the time stamp parameter means, which is the subject of the next section. <a name="understanding_time" id="understanding_time"></a></p><h2 id="Understanding-Time-Stamps"><a href="#Understanding-Time-Stamps" class="headerlink" title="Understanding Time Stamps"></a>Understanding Time Stamps</h2><p><a name="120509" id="120509"></a> As you already know, the MIDI specification has different parts. One part describes MIDI “wire” protocol (messages sent between devices in real time), and another part describes Standard MIDI Files (messages stored as events in “sequences”). In the latter part of the specification, each event stored in a standard MIDI file is tagged with a timing value that indicates when that event should be played. By contrast, messages in MIDI wire protocol are always supposed to be processed immediately, as soon as they’re received by a device, so they have no accompanying timing values.</p><p><a name="120511" id="120511"></a> The Java Sound API adds an additional twist. It comes as no surprise that timing values are present in the <code>MidiEvent</code> objects that are stored in sequences (as might be read from a MIDI file), just as in the Standard MIDI Files specification. But in the Java Sound API, even the messages sent between devices&#226;&#128;&#148;in other words, the messages that correspond to MIDI wire protocol&#226;&#128;&#148;can be given timing values, known as <strong>time stamps</strong>. It is these time stamps that concern us here.</p><h3 id="Time-Stamps-on-Messages-Sent-to-Devices"><a href="#Time-Stamps-on-Messages-Sent-to-Devices" class="headerlink" title="Time Stamps on Messages Sent to Devices"></a>Time Stamps on Messages Sent to Devices</h3><p><a name="120519" id="120519"></a> The time stamp that can optionally accompany messages sent between devices in the Java Sound API is quite different from the timing values in a standard MIDI file. The timing values in a MIDI file are often based on musical concepts such as beats and tempo, and each event’s timing measures the time elapsed since the previous event. In contrast, the time stamp on a message sent to a device’s <code>Receiver</code> object always measures absolute time in microseconds. Specifically, it measures the number of microseconds elapsed since the device that owns the receiver was opened.</p><p><a name="120521" id="120521"></a> This kind of time stamp is designed to help compensate for latencies introduced by the operating system or by the application program. It’s important to realize that these time stamps are used for minor adjustments to timing, not to implement complex queues that can schedule events at completely arbitrary times (as <code>MidiEvent</code> timing values do).</p><p><a name="122050" id="122050"></a> The time stamp on a message sent to a device (through a <code>Receiver</code>) can provide precise timing information to the device. The device might use this information when it processes the message. For example, it might adjust the event’s timing by a few milliseconds to match the information in the time stamp. On the other hand, not all devices support time stamps, so the device might completely ignore the message’s time stamp.</p><p><a name="122051" id="122051"></a> Even if a device supports time stamps, it might not schedule the event for exactly the time that you requested. You can’t expect to send a message whose time stamp is very far in the future and have the device handle it as you intended, and you certainly can’t expect a device to correctly schedule a message whose time stamp is in the past! It’s up to the device to decide how to handle time stamps that are too far off in the future or are in the past. The sender doesn’t know what the device considers to be too far off, or whether the device had any problem with the time stamp. This ignorance mimics the behavior of external MIDI hardware devices, which send messages without ever knowing whether they were received correctly. (MIDI wire protocol is unidirectional.)</p><p><a name="120527" id="120527"></a> Some devices send time-stamped messages (via a <code>Transmitter</code>). For example, the messages sent by a MIDI input port might be stamped with the time the incoming message arrived at the port. On some systems, the event-handling mechanisms cause a certain amount of timing precision to be lost during subsequent processing of the message. The message’s time stamp allows the original timing information to be preserved.</p><p><a name="122114" id="122114"></a> To learn whether a device supports time stamps, invoke the following method of <code>MidiDevice</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">long getMicrosecondPosition()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method returns -1 if the device ignores time stamps. Otherwise, it returns the device’s current notion of time, which you as the sender can use as an offset when determining the time stamps for messages you subsequently send. For example, if you want to send a message with a time stamp for five milliseconds in the future, you can get the device’s current position in microseconds, add 5000 microseconds, and use that as the time stamp. Keep in mind that the <code>MidiDevice&#39;s</code> notion of time always places time zero at the time the device was opened.</p><p><a name="120533" id="120533"></a> Now, with all that explanation of time stamps as a background, let’s return to the <code>send</code> method of <code>Receiver</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void send(MidiMessage message, long timeStamp)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The <code>timeStamp</code> argument is expressed in microseconds, according to the receiving device’s notion of time. If the device doesn’t support time stamps, it simply ignores the <code>timeStamp</code> argument. You aren’t required to time-stamp the messages you send to a receiver. You can use -1 for the <code>timeStamp</code> argument to indicate that you don’t care about adjusting the exact timing; you’re just leaving it up to the receiving device to process the message as soon as it can. However, it’s not advisable to send -1 with some messages and explicit time stamps with other messages sent to the same receiver. Doing so is likely to cause irregularities in the resultant timing. <a name="122132" id="122132"></a></p><h2 id="Connecting-Transmitters-to-Receivers"><a href="#Connecting-Transmitters-to-Receivers" class="headerlink" title="Connecting Transmitters to Receivers"></a>Connecting Transmitters to Receivers</h2><p><a name="120542" id="120542"></a> We’ve seen how you can send a MIDI message directly to a receiver, without using a transmitter. Now let’s look at the more common case, where you aren’t creating MIDI messages from scratch, but are simply connecting devices together so that one of them can send MIDI messages to the other.</p><p><a name="120544" id="120544"></a></p><h3 id="Connecting-to-a-Single-Device"><a href="#Connecting-to-a-Single-Device" class="headerlink" title="Connecting to a Single Device"></a>Connecting to a Single Device</h3><p><a name="120546" id="120546"></a> The specific case we’ll take as our first example is connecting a sequencer to a synthesizer. After this connection is made, starting the sequencer running will cause the synthesizer to generate audio from the events in the sequencer’s current sequence. For now, we’ll ignore the process of loading a sequence from a MIDI file into the sequencer. Also, we won’t go into the mechanism of playing the sequence. Loading and playing sequences is discussed in detail in<br><a href="MIDI-seq-intro.html">Playing, Recording, and Editing MIDI Sequences</a>. Loading instruments into the synthesizer is discussed in<br><a href="MIDI-synth.html">Synthesizing Sound</a>. For now, all we’re interested in is how to make the connection between the sequencer and the synthesizer. This will serve as an illustration of the more general process of connecting one device’s transmitter to another device’s receiver.</p><p><a name="121529" id="121529"></a> For simplicity, we’ll use the default sequencer and the default synthesizer.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Sequencer           seq;</span><br><span class="line">Transmitter         seqTrans;</span><br><span class="line">Synthesizer         synth;</span><br><span class="line">Receiver         synthRcvr;</span><br><span class="line">try &#123;</span><br><span class="line">      seq     &#x3D; MidiSystem.getSequencer();</span><br><span class="line">      seqTrans &#x3D; seq.getTransmitter();</span><br><span class="line">      synth   &#x3D; MidiSystem.getSynthesizer();</span><br><span class="line">      synthRcvr &#x3D; synth.getReceiver(); </span><br><span class="line">      seqTrans.setReceiver(synthRcvr);      </span><br><span class="line">&#125; catch (MidiUnavailableException e) &#123;</span><br><span class="line">      &#x2F;&#x2F; handle or throw exception</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>An implementation might actually have a single object that serves as both the default sequencer and the default synthesizer. In other words, the implementation might use a class that implements both the <code>Sequencer</code> interface and the <code>Synthesizer</code> interface. In that case, it probably wouldn’t be necessary to make the explicit connection that we did in the code above. For portability, though, it’s safer not to assume such a configuration. If desired, you can test for this condition, of course:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">if (seq instanceof Synthesizer)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="122187" id="122187"></a> although the explicit connection above should work in any case.</p><p><a name="120569" id="120569"></a></p><h3 id="Connecting-to-More-than-One-Device"><a href="#Connecting-to-More-than-One-Device" class="headerlink" title="Connecting to More than One Device"></a>Connecting to More than One Device</h3><p><a name="121563" id="121563"></a> The previous code example illustrated a one-to-one connection between a transmitter and a receiver. But, what if you need to send the same MIDI message to multiple receivers? For example, suppose you want to capture MIDI data from an external device to drive the internal synthesizer while simultaneously recording the data to a sequence. This form of connection, sometimes referred to as “fan out” or as a “splitter,” is straightforward. The following statements show how to create a fan-out connection, through which the MIDI messages arriving at the MIDI input port are sent to both a <code>Synthesizer</code> object and a <code>Sequencer</code> object. We assume you’ve already obtained and opened the three devices: the input port, sequencer, and synthesizer. (To obtain the input port, you’ll need to iterate over all the items returned by <code>MidiSystem.getMidiDeviceInfo</code>.)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Synthesizer  synth;</span><br><span class="line">Sequencer    seq;</span><br><span class="line">MidiDevice   inputPort;</span><br><span class="line">&#x2F;&#x2F; [obtain and open the three devices...]</span><br><span class="line">Transmitter   inPortTrans1, inPortTrans2;</span><br><span class="line">Receiver            synthRcvr;</span><br><span class="line">Receiver            seqRcvr;</span><br><span class="line">try &#123;</span><br><span class="line">      inPortTrans1 &#x3D; inputPort.getTransmitter();</span><br><span class="line">      synthRcvr &#x3D; synth.getReceiver(); </span><br><span class="line">      inPortTrans1.setReceiver(synthRcvr);</span><br><span class="line">      inPortTrans2 &#x3D; inputPort.getTransmitter();</span><br><span class="line">      seqRcvr &#x3D; seq.getReceiver(); </span><br><span class="line">      inPortTrans2.setReceiver(seqRcvr);</span><br><span class="line">&#125; catch (MidiUnavailableException e) &#123;</span><br><span class="line">      &#x2F;&#x2F; handle or throw exception</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This code introduces a dual invocation of the <code>MidiDevice.getTransmitter</code> method, assigning the results to <code>inPortTrans1</code> and <code>inPortTrans2</code>. As mentioned earlier, a device can own multiple transmitters and receivers. Each time <code>MidiDevice.getTransmitter()</code> is invoked for a given device, another transmitter is returned, until no more are available, at which time an exception will be thrown.</p><p><a name="122228" id="122228"></a> To learn how many transmitters and receivers a device supports, you can use the following <code>MidiDevice</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int getMaxTransmitters()</span><br><span class="line">int &#96;getMaxReceivers&#96;()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods return the total number owned by the device, not the number currently available.</p><p><a name="120602" id="120602"></a> A transmitter can transmit MIDI messages to only one receiver at a time. (Every time you call <code>Transmitter&#39;s setReceiver</code> method, the existing <code>Receiver</code>, if any, is replaced by the newly specified one. You can tell whether the transmitter currently has a receiver by invoking <code>Transmitter.getReceiver</code>.) However, if a device has multiple transmitters, it can send data to more than one device at a time, by connecting each transmitter to a different receiver, as we saw in the case of the input port above.</p><p><a name="122286" id="122286"></a> Similarly, a device can use its multiple receivers to receive from more than one device at a time. The multiple-receiver code that’s required is straightforward, being directly analogous to the multiple-transmitter code above. It’s also possible for a single receiver to receive messages from more than one transmitter at a time.</p><p><a name="120605" id="120605"></a></p><h3 id="Closing-Connections"><a href="#Closing-Connections" class="headerlink" title="Closing Connections"></a>Closing Connections</h3><p><a name="120607" id="120607"></a> Once you’re done with a connection, you can free up its resources by invoking the <code>close</code> method for each transmitter and receiver that you’ve obtained. The <code>Transmitter</code> and <code>Receiver</code> interfaces each have a <code>close</code> method. Note that invoking <code>Transmitter.setReceiver</code> doesn’t close the transmitter’s current receiver. The receiver is left open, and it can still receive messages from any other transmitter that’s connected to it.</p><p><a name="122314" id="122314"></a> If you’re also done with the devices, you can similarly make them available to other application programs by invoking <code>MidiDevice.close()</code>. Closing a device automatically closes all its transmitters and receivers.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transmitting-and-Receiving-MIDI-Messages&quot;&gt;&lt;a href=&quot;#Transmitting-and-Receiving-MIDI-Messages&quot; class=&quot;headerlink&quot; title=&quot;Transmitting</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Synthesizing Sound</title>
    <link href="http://example.com/wiki/2021-03-04-Synthesizing%20Sound/"/>
    <id>http://example.com/wiki/2021-03-04-Synthesizing%20Sound/</id>
    <published>2021-03-04T14:32:16.440Z</published>
    <updated>2021-03-04T15:17:55.844Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Synthesizing-Sound"><a href="#Synthesizing-Sound" class="headerlink" title="Synthesizing Sound"></a>Synthesizing Sound</h1><p><a name="121725" id="121725"></a> Most programs that avail themselves of the Java Sound API’s MIDI package do so to synthesize sound. The entire apparatus of MIDI files, events, sequences, and sequencers, which was previously discussed, nearly always has the goal of eventually sending musical data to a synthesizer to convert into audio. (Possible exceptions include programs that convert MIDI into musical notation that can be read by a musician, and programs that send messages to external MIDI-controlled devices such as mixing consoles.)</p><p><a name="121727" id="121727"></a> The <code>Synthesizer</code> interface is therefore fundamental to the MIDI package. This page shows how to manipulate a synthesizer to play sound. Many programs will simply use a sequencer to send MIDI file data to the synthesizer, and won’t need to invoke many <code>Synthesizer</code> methods directly. However, it’s possible to control a synthesizer directly, without using sequencers or even <code>MidiMessage</code> objects, as explained near the end of this page.</p><p><a name="121729" id="121729"></a> The synthesis architecture might seem complex for readers who are unfamiliar with MIDI. Its API includes three interfaces:</p><li><a name="121730" id="121730"></a> [`Synthesizer`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Synthesizer.html)</li><li><a name="121731" id="121731"></a> [`MidiChannel`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiChannel.html)</li><li><a name="121732" id="121732"></a> [`Soundbank`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Soundbank.html)</li><p><a name="121733" id="121733"></a> and four classes:</p><li><a name="121734" id="121734"></a> [`Instrument`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Instrument.html)</li><li><a name="121735" id="121735"></a> [`Patch`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Patch.html)</li><li><a name="121736" id="121736"></a> [`SoundbankResource`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/SoundbankResource.html)</li><li><a name="121737" id="121737"></a> [`VoiceStatus`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/VoiceStatus.html)</li><p><a name="121739" id="121739"></a>As orientation for all this API, the next section explains some of the basics of MIDI synthesis and how they’re reflected in the Java Sound API. Subsequent sections give a more detailed look at the API.</p><p><a name="121742" id="121742"></a></p><h2 id="Understanding-MIDI-Synthesis"><a href="#Understanding-MIDI-Synthesis" class="headerlink" title="Understanding MIDI Synthesis"></a>Understanding MIDI Synthesis</h2><p><a name="121745" id="121745"></a> How does a synthesizer generate sound? Depending on its implementation, it may use one or more sound-synthesis techniques. For example, many synthesizers use wavetable synthesis. A wavetable synthesizer reads stored snippets of audio from memory, playing them at different sample rates and looping them to create notes of different pitches and durations. For example, to synthesize the sound of a saxophone playing the note C#4 (MIDI note number 61), the synthesizer might access a very short snippet from a recording of a saxophone playing the note Middle C (MIDI note number 60), and then cycle repeatedly through this snippet at a slightly faster sample rate than it was recorded at, which creates a long note of a slightly higher pitch. Other synthesizers use techniques such as frequency modulation (FM), additive synthesis, or physical modeling, which don’t make use of stored audio but instead generate audio from scratch using different algorithms.</p><p><a name="121747" id="121747"></a></p><h3 id="Instruments"><a href="#Instruments" class="headerlink" title="Instruments"></a>Instruments</h3><p><a name="123181" id="123181"></a> What all synthesis techniques have in common is the ability to create many sorts of sounds. Different algorithms, or different settings of parameters within the same algorithm, create different-sounding results. An <strong>instrument</strong> is a specification for synthesizing a certain type of sound. That sound may emulate a traditional musical instrument, such as a piano or violin; it may emulate some other kind of sound source, for instance, a telephone or helicopter; or it may emulate no “real-world” sound at all. A specification called General MIDI defines a standard list of 128 instruments, but most synthesizers allow other instruments as well. Many synthesizers provide a collection of built-in instruments that are always available for use; some synthesizers also support mechanisms for loading additional instruments.</p><p><a name="123190" id="123190"></a> An instrument may be vendor-specific&#226;&#128;&#148;in other words, applicable to only one synthesizer or several models from the same vendor. This incompatibility results when two different synthesizers use different sound-synthesis techniques, or different internal algorithms and parameters even if the fundamental technique is the same. Because the details of the synthesis technique are often proprietary, incompatibility is common. The Java Sound API includes ways to detect whether a given synthesizer supports a given instrument.</p><p><a name="121753" id="121753"></a> An instrument can usually be considered a preset; you don’t have to know anything about the details of the synthesis technique that produces its sound. However, you can still vary aspects of its sound. Each Note On message specifies the pitch and volume of an individual note. You can also alter the sound through other MIDI commands such as controller messages or system-exclusive messages.</p><p><a name="121755" id="121755"></a></p><h3 id="Channels"><a href="#Channels" class="headerlink" title="Channels"></a>Channels</h3><p><a name="121759" id="121759"></a> Many synthesizers are <strong>multimbral</strong> (sometimes called <strong>polytimbral</strong>), meaning that they can play the notes of different instruments simultaneously. (<strong>Timbre</strong> is the characteristic sound quality that enables a listener to distinguish one kind of musical instrument from other kinds.) Multimbral synthesizers can emulate an entire ensemble of real-world instruments, instead of only one instrument at a time. MIDI synthesizers normally implement this feature by taking advantage of the different MIDI channels on which the MIDI specification allows data to be transmitted. In this case, the synthesizer is actually a collection of sound-generating units, each emulating a different instrument and responding independently to messages that are received on a different MIDI channel. Since the MIDI specification provides only 16 channels, a typical MIDI synthesizer can play up to 16 different instruments at once. The synthesizer receives a stream of MIDI commands, many of which are channel commands. (Channel commands are targeted to a particular MIDI channel; for more information, see the MIDI specification.) If the synthesizer is multitimbral, it routes each channel command to the correct sound-generating unit, according to the channel number indicated in the command.</p><p><a name="121761" id="121761"></a> In the Java Sound API, these sound-generating units are instances of classes that implement the <code>MidiChannel</code> interface. A <code>synthesizer</code> object has at least one <code>MidiChannel</code> object. If the synthesizer is multimbral, it has more than one, normally 16. Each <code>MidiChannel</code> represents an independent sound-generating unit.</p><p><a name="121763" id="121763"></a> Because a synthesizer’s <code>MidiChannel</code> objects are more or less independent, the assignment of instruments to channels doesn’t have to be unique. For example, all 16 channels could be playing a piano timbre, as though there were an ensemble of 16 pianos. Any grouping is possible&#226;&#128;&#148;for instance, channels 1, 5, and 8 could be playing guitar sounds, while channels 2 and 3 play percussion and channel 12 has a bass timbre. The instrument being played on a given MIDI channel can be changed dynamically; this is known as a <strong>program change</strong>.</p><p><a name="121765" id="121765"></a> Even though most synthesizers allow only 16 or fewer instruments to be active at a given time, these instruments can generally be chosen from a much larger selection and assigned to particular channels as required.</p><p><a name="121767" id="121767"></a></p><h3 id="Soundbanks-and-Patches"><a href="#Soundbanks-and-Patches" class="headerlink" title="Soundbanks and Patches"></a>Soundbanks and Patches</h3><p><a name="121769" id="121769"></a> Instruments are organized hierarchically in a synthesizer, by bank number and program number. Banks and programs can be thought of as rows and columns in a two-dimensional table of instruments. A bank is a collection of programs. The MIDI specification allows up to 128 programs in a bank, and up to 128 banks. However, a particular synthesizer might support only one bank, or a few banks, and might support fewer than 128 programs per bank.</p><p><a name="121771" id="121771"></a> In the Java Sound API, there’s a higher level to the hierarchy: a soundbank. Soundbanks can contain up to 128 banks, each containing up to 128 instruments. Some synthesizers can load an entire soundbank into memory.</p><p><a name="121773" id="121773"></a> To select an instrument from the current soundbank, you specify a bank number and a program number. The MIDI specification accomplishes this with two MIDI commands: bank select and program change. In the Java Sound API, the combination of a bank number and program number is encapsulated in a <code>Patch</code> object. You change a MIDI channel’s current instrument by specifying a new patch. The patch can be considered the two-dimensional index of the instruments in the current soundbank.</p><p><a name="121775" id="121775"></a> You might be wondering if soundbanks, too, are indexed numerically. The answer is no; the MIDI specification does not provide for this. In the Java Sound API, a <code>Soundbank</code> object can be obtained by reading a soundbank file. If the soundbank is supported by the synthesizer, its instruments can be loaded into the synthesizer individually as desired, or all at once. Many synthesizers have a built-in or default soundbank; the instruments contained in this soundbank are always available to the synthesizer.</p><p><a name="121777" id="121777"></a></p><h3 id="Voices"><a href="#Voices" class="headerlink" title="Voices"></a>Voices</h3><p><a name="121779" id="121779"></a> It’s important to distinguish between the number of <strong>timbres</strong> a synthesizer can play simultaneously and the number of <strong>notes</strong> it can play simultaneously. The former was described above under “Channels.” The ability to play multiple notes at once is referred to as <strong>polyphony</strong>. Even a synthesizer that isn’t multitimbral can generally play more than one note at a time (all having the same timbre, but different pitches). For example, playing any chord, such as a G major triad or a B minor seventh chord, requires polyphony. Any synthesizer that generates sound in real time has a limitation on the number of notes it can synthesize at once. In the Java Sound API, the synthesizer reports this limitation through the <code>getMaxPolyphony</code> method.</p><p><a name="121783" id="121783"></a> A <strong>voice</strong> is a succession of single notes, such as a melody that a person can sing. Polyphony consists of multiple voices, such as the parts sung by a choir. A 32-voice synthesizer, for example, can play 32 notes simultaneously. (However, some MIDI literature uses the word “voice” in a different sense, similar to the meaning of “instrument” or “timbre.”)</p><p><a name="121785" id="121785"></a> The process of assigning incoming MIDI notes to specific voices is known as <strong>voice allocation</strong>. A synthesizer maintains a list of voices, keeping track of which ones are active (meaning that they currently have notes sounding). When a note stops sounding, the voice becomes inactive, meaning that it’s now free to accept the next note-on request that the synthesizer receives. An incoming stream of MIDI commands can easily request more simultaneous notes than the synthesizer is capable of generating. When all the synthesizer’s voices are active, how should the next Note On request be handled? Synthesizers can implement different strategies: the most recently requested note can be ignored; or it can be played by discontinuing another note, such as the least recently started one.</p><p><a name="121787" id="121787"></a> Although the MIDI specification does not require it, a synthesizer can make public the contents of each of its voices. The Java Sound API includes a <code>VoiceStatus</code> class for this purpose.</p><p><a name="121788" id="121788"></a> A <code>VoiceStatus</code> reports on the voice’s current active or inactive status, MIDI channel, bank and program number, MIDI note number, and MIDI volume.</p><p><a name="121790" id="121790"></a> With this background, let’s examine the specifics of the Java Sound API for synthesis.</p><p><a name="121793" id="121793"></a></p><h2 id="Managing-Instruments-and-Soundbanks"><a href="#Managing-Instruments-and-Soundbanks" class="headerlink" title="Managing Instruments and Soundbanks"></a>Managing Instruments and Soundbanks</h2><p><a name="121795" id="121795"></a> In many cases, a program can make use of a <code>Synthesizer</code> object without explicitly invoking almost any of the synthesis API. For example, suppose you’re playing a standard MIDI file. You load it into a <code>Sequence</code> object, which you play by having a sequencer send the data to the default synthesizer. The data in the sequence controls the synthesizer as intended, playing all the right notes at the right times.</p><p><a name="121797" id="121797"></a> However, there are cases when this simple scenario is inadequate. The sequence contains the right music, but the instruments sound all wrong! This unfortunate situation probably arose because the creator of the MIDI file had different instruments in mind than the ones that are currently loaded into the synthesizer.</p><p><a name="123339" id="123339"></a> The MIDI 1.0 Specification provides for bank-select and program-change commands, which affect which instrument is currently playing on each MIDI channel. However, the specification does not define what instrument should reside in each patch location (bank and program number). The more recent General MIDI specification addresses this problem by defining a bank containing 128 programs that correspond to specific instrument sounds. A General MIDI synthesizer uses 128 instruments that match this specified set. Different General MIDI synthesizers can sound quite different, even when playing what’s supposed to be the same instrument. However, a MIDI file should for the most part sound similar (even if not identical), no matter which General MIDI synthesizer is playing it.</p><p><a name="123341" id="123341"></a> Nonetheless, not all creators of MIDI files want to be limited to the set of 128 timbres defined by General MIDI. This section shows how to change the instruments from the default set that the synthesizer comes with. (If there is no default, meaning that no instruments are loaded when you access the synthesizer, you’ll have to use this API to start with in any case.)</p><p><a name="121803" id="121803"></a></p><h3 id="Learning-What-Instruments-Are-Loaded"><a href="#Learning-What-Instruments-Are-Loaded" class="headerlink" title="Learning What Instruments Are Loaded"></a>Learning What Instruments Are Loaded</h3><p><a name="121805" id="121805"></a> To learn whether the instruments currently loaded into the synthesizer are the ones you want, you can invoke this <code>Synthesizer</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Instrument[] getLoadedInstruments() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>and iterate over the returned array to see exactly which instruments are currently loaded. Most likely, you would display the instruments’ names in the user interface (using the <code>getName</code> method of <code>Instrument</code>), and let the user decide whether to use those instruments or load others. The <code>Instrument</code> API includes a method that reports which soundbank the instrument belongs to. The soundbank’s name might help your program or the user ascertain exactly what the instrument is.</p><p><a name="123382" id="123382"></a> This <code>Synthesizer</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Soundbank getDefaultSoundbank() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>gives you the default soundbank. The <code>Soundbank</code> API includes methods to retrieve the soundbank’s name, vendor, and version number, by which the program or the user can verify the bank’s identity. However, you can’t assume when you first get a synthesizer that the instruments from the default soundbank have been loaded into the synthesizer. For example, a synthesizer might have a large assortment of built-in instruments available for use, but because of its limited memory it might not load them automatically. <a name="121824" id="121824"></a></p><h3 id="Loading-Different-Instruments"><a href="#Loading-Different-Instruments" class="headerlink" title="Loading Different Instruments"></a>Loading Different Instruments</h3><p><a name="122802" id="122802"></a> The user might decide to load instruments that are different from the current ones (or you might make that decision programmatically). The following method tells you which instruments come with the synthesizer (versus having to be loaded from soundbank files):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Instrument[] getAvailableInstruments()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>You can load any of these instruments by invoking:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean loadInstrument(Instrument instrument) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The instrument gets loaded into the synthesizer in the location specified by the instrument’s <code>Patch</code> object (which can be retrieved using the <code>getPatch</code> method of <code>Instrument</code>).</p><p><a name="121836" id="121836"></a> To load instruments from other soundbanks, first invoke <code>Synthesizer&#39;s</code> <code>isSupportedSoundbank</code> method to make sure that the soundbank is compatible with this synthesizer (if it isn’t, you can iterate over the system’s synthesizers to try to find one that supports the soundbank). You can then invoke one of these methods to load instruments from the soundbank:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean loadAllInstruments(Soundbank soundbank) </span><br><span class="line">boolean loadInstruments(Soundbank soundbank, </span><br><span class="line">  Patch[] patchList) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As the names suggest, the first of these loads the entire set of instruments from a given soundbank, and the second loads selected instruments from the soundbank. You could also use <code>Soundbank&#39;s</code> <code>getInstruments</code> method to access all the instruments, then iterate over them and load selected instruments one at a time using <code>loadInstrument</code>.</p><p><a name="121843" id="121843"></a> It’s not necessary for all the instruments you load to be from the same soundbank. You could use <code>loadInstrument</code> or <code>loadInstruments</code> to load certain instruments from one soundbank, another set from a different soundbank, and so on.</p><p><a name="121846" id="121846"></a> Each instrument has its own <code>Patch</code> object that specifies the location on the synthesizer where the instrument should be loaded. The location is defined by a bank number and a program number. There’s no API to change the location by changing the patch’s bank or program number.</p><p><a name="121852" id="121852"></a> However, it is possible to load an instrument into a location other than the one specified by its patch, using the following method of <code>Synthesizer</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean remapInstrument(Instrument from, Instrument to) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method unloads its first argument from the synthesizer, and places its second argument in whatever synthesizer patch location had been occupied by the first argument. <a name="121858" id="121858"></a></p><h3 id="Unloading-Instruments"><a href="#Unloading-Instruments" class="headerlink" title="Unloading Instruments"></a>Unloading Instruments</h3><p><a name="123434" id="123434"></a> Loading an instrument into a program location automatically unloads whatever instrument was already at that location, if any. You can also explicitly unload instruments without necessarily replacing them with new ones. <code>Synthesizer</code> includes three unloading methods that correspond to the three loading methods. If the synthesizer receives a program-change message that selects a program location where no instrument is currently loaded, there won’t be any sound from the MIDI channel on which the program-change message was sent.</p><p><a name="123435" id="123435"></a></p><h3 id="Accessing-Soundbank-Resources"><a href="#Accessing-Soundbank-Resources" class="headerlink" title="Accessing Soundbank Resources"></a>Accessing Soundbank Resources</h3><p><a name="123439" id="123439"></a> Some synthesizers store other information besides instruments in their soundbanks. For example, a wavetable synthesizer stores audio samples that one or more instruments can access. Because the samples might be shared by multiple instruments, they’re stored in the soundbank independently of any instrument. Both the <code>Soundbank</code> interface and the <code>Instrument</code> class provide a method call <code>getSoundbankResources</code>, which returns a list of <code>SoundbankResource</code> objects. The details of these objects are specific to the synthesizer for which the soundbank is designed. In the case of wavetable synthesis, a resource might be an object that encapsulates a series of audio samples, taken from one snippet of a sound recording. Synthesizers that use other synthesis techniques might store other kinds of objects in the synthesizer’s <code>SoundbankResources</code> array.</p><p><a name="123441" id="123441"></a></p><h2 id="Querying-the-Synthesizer’s-Capabilities-and-Current-State"><a href="#Querying-the-Synthesizer’s-Capabilities-and-Current-State" class="headerlink" title="Querying the Synthesizer’s Capabilities and Current State"></a>Querying the Synthesizer’s Capabilities and Current State</h2><p><a name="121873" id="121873"></a> The <code>Synthesizer</code> interface includes methods that return information about the synthesizer’s capabilities:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public long getLatency()</span><br><span class="line">public int getMaxPolyphony()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The latency measures the worst-case delay between the time a MIDI message is delivered to the synthesizer and the time that the synthesizer actually produces the corresponding result. For example, it might take a synthesizer a few milliseconds to begin generating audio after receiving a note-on event.</p><p><a href="#121777">Voices</a>. As mentioned in the same discussion, a synthesizer can provide information about its voices. This is accomplished through the following method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public VoiceStatus[] getVoiceStatus()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Each <code>VoiceStatus</code> in the returned array reports the voice’s current active or inactive status, MIDI channel, bank and program number, MIDI note number, and MIDI volume. The array’s length should normally be the same number returned by <code>getMaxPolyphony</code>. If the synthesizer isn’t playing, all its <code>VoiceStatus</code> objects have their active field set to <code>false</code>.</p><p><a name="121886" id="121886"></a> You can learn additional information about the current status of a synthesizer by retrieving its <code>MidiChannel</code> objects and querying their state. This is discussed more in the next section.</p><p><a name="121890" id="121890"></a></p><h2 id="Using-Channels"><a href="#Using-Channels" class="headerlink" title="Using Channels"></a>Using Channels</h2><p><a name="121892" id="121892"></a> Sometimes it’s useful or necessary to access a synthesizer’s <code>MidiChannel</code> objects directly. This section discusses such situations.</p><p><a name="121894" id="121894"></a></p><h3 id="Controlling-the-Synthesizer-without-Using-a-Sequencer"><a href="#Controlling-the-Synthesizer-without-Using-a-Sequencer" class="headerlink" title="Controlling the Synthesizer without Using a Sequencer"></a>Controlling the Synthesizer without Using a Sequencer</h3><p><a name="121896" id="121896"></a> When using a sequence, such as one read from a MIDI file, you don’t need to send MIDI commands to the synthesizer yourself. Instead, you just load the sequence into a sequencer, connect the sequencer to the synthesizer, and let it run. The sequencer takes care of scheduling the events, and the result is a predictable musical performance. This scenario works fine when the desired music is known in advance, which is true when it’s read from a file.</p><p><a name="121898" id="121898"></a> In some situations, however, the music is generated on the fly as it’s playing. For example, the user interface might display a musical keyboard or a guitar fretboard and let the user play notes at will by clicking with the mouse. As another example, an application might use a synthesizer not to play music per se, but to generate sound effects in response to the user’s actions. This scenario is typical of games. As a final example, the application might indeed be playing music that’s read from a file, but the user interface allows the user to interact with the music, altering it dynamically. In all these cases, the application sends commands directly to the synthesizer, since the MIDI messages need to be delivered immediately, instead of being scheduled for some determinate point in the future.</p><p><a name="121900" id="121900"></a> There are at least two ways of sending a MIDI message to the synthesizer without using a sequencer. The first is to construct a <code>MidiMessage</code> and pass it to the synthesizer using the send method of <code>Receiver</code>. For example, to produce a Middle C (MIDI note number 60) on MIDI channel 5 (one-based) immediately, you could do the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ShortMessage myMsg &#x3D; new ShortMessage();</span><br><span class="line">&#x2F;&#x2F; Play the note Middle C (60) moderately loud</span><br><span class="line">&#x2F;&#x2F; (velocity &#x3D; 93)on channel 4 (zero-based).</span><br><span class="line">myMsg.setMessage(ShortMessage.NOTE_ON, 4, 60, 93); </span><br><span class="line">Synthesizer synth &#x3D; MidiSystem.getSynthesizer();</span><br><span class="line">Receiver synthRcvr &#x3D; synth.getReceiver();</span><br><span class="line">synthRcvr.send(myMsg, -1); &#x2F;&#x2F; -1 means no time stamp</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The second way is to bypass the message-passing layer (that is, the <code>MidiMessage</code> and <code>Receiver</code> API) altogether, and interact with the synthesizer’s <code>MidiChannel</code> objects directly. You first need to retrieve the synthesizer’s <code>MidiChannel</code> objects, using the following <code>Synthesizer</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public MidiChannel[] getChannels()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>after which you can invoke the desired <code>MidiChannel</code> methods directly. This is a more immediate route than sending the corresponding <code>MidiMessages</code> to the synthesizer’s <code>Receiver</code> and letting the synthesizer handle the communication with its own <code>MidiChannels</code>. For example, the code corresponding to the preceding example would be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Synthesizer synth &#x3D; MidiSystem.getSynthesizer();</span><br><span class="line">MidiChannel chan[] &#x3D; synth.getChannels(); </span><br><span class="line">&#x2F;&#x2F; Check for null; maybe not all 16 channels exist.</span><br><span class="line">if (chan[4] !&#x3D; null) &#123;</span><br><span class="line">     chan[4].noteOn(60, 93); </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Getting-a-Channel’s-Current-State"><a href="#Getting-a-Channel’s-Current-State" class="headerlink" title="Getting a Channel’s Current State"></a>Getting a Channel’s Current State</h3><p><a name="121939" id="121939"></a> The <code>MidiChannel</code> interface provides methods that correspond one-to-one to each of the “channel voice” or “channel mode” messages defined by the MIDI specification. We saw one case with the use of the noteOn method in the previous example. However, in addition to these canonical methods, the Java Sound API’s <code>MidiChannel</code> interface adds some “get” methods to retrieve the value most recently set by corresponding voice or mode “set” methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int       getChannelPressure()</span><br><span class="line">int       getController(int controller)</span><br><span class="line">boolean   getMono()</span><br><span class="line">boolean   getOmni() </span><br><span class="line">int       getPitchBend() </span><br><span class="line">int       getPolyPressure(int noteNumber)</span><br><span class="line">int       getProgram()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods might be useful for displaying channel state to the user, or for deciding what values to send subsequently to the channel. <a name="121953" id="121953"></a></p><h3 id="Muting-and-Soloing-a-Channel"><a href="#Muting-and-Soloing-a-Channel" class="headerlink" title="Muting and Soloing a Channel"></a>Muting and Soloing a Channel</h3><p><a name="123492" id="123492"></a> The Java Sound API adds the notions of per-channel solo and mute, which are not required by the MIDI specification. These are similar to the solo and mute on the tracks of a MIDI sequence.</p><p><a name="123493" id="123493"></a> If mute is on, this channel will not sound, but other channels are unaffected. If solo is on, this channel, and any other soloed channel, will sound (if it isn’t muted), but no other channels will sound. A channel that is both soloed and muted will not sound. The <code>MidiChannel</code> API includes four methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean      getMute() </span><br><span class="line">boolean      getSolo()</span><br><span class="line">void         setMute(boolean muteState) </span><br><span class="line">void         setSolo(boolean soloState)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Permission-to-Play-Synthesized-Sound"><a href="#Permission-to-Play-Synthesized-Sound" class="headerlink" title="Permission to Play Synthesized Sound"></a>Permission to Play Synthesized Sound</h2><p><a name="123505" id="123505"></a> The audio produced by any installed MIDI synthesizer is typically routed through the sampled-audio system. If your program doesn’t have permission to play audio, the synthesizer’s sound won’t be heard, and a security exception will be thrown. For more information on audio permissions, see the previous discussion of Permission to Use Audio Resources<br><a href="accessing.html#113223">Permission to Use Audio Resources</a>.</p><p><a name="121968" id="121968"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Synthesizing-Sound&quot;&gt;&lt;a href=&quot;#Synthesizing-Sound&quot; class=&quot;headerlink&quot; title=&quot;Synthesizing Sound&quot;&gt;&lt;/a&gt;Synthesizing Sound&lt;/h1&gt;&lt;p&gt;&lt;a nam</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Sound</title>
    <link href="http://example.com/wiki/2021-03-04-Sound/"/>
    <id>http://example.com/wiki/2021-03-04-Sound/</id>
    <published>2021-03-04T14:32:16.375Z</published>
    <updated>2021-03-04T15:17:55.841Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Trail-Sound"><a href="#Trail-Sound" class="headerlink" title="Trail: Sound"></a>Trail: Sound</h1><p>The Java Sound API is a low-level API for effecting and controlling the input and output of sound media, including both audio and Musical Instrument Digital Interface (MIDI) data. The Java Sound API provides explicit control over the capabilities normally required for sound input and output, in a framework that promotes extensibility and flexibility. <a name="111814" id="111814"></a></p><p><a name="111816" id="111816"></a> The Java Sound API fulfills the needs of a wide range of application developers. Potential application areas include:</p><ul><li><a name="111817" id="111817"></a>Communication frameworks, such as conferencing and telephony</li><li><a name="111818" id="111818"></a>End-user content delivery systems, such as media players and music using streamed content</li><li><a name="111819" id="111819"></a>Interactive application programs, such as games and Web sites that use dynamic content</li><li><a name="111820" id="111820"></a>Content creation and editing</li><li><a name="111821" id="111821"></a>Tools, toolkits, and utilities</li></ul><p><a name="111823" id="111823"></a> &lt;!–</p><h2> How Does the Java Sound API Relate to Other Interfaces? </h2>--><p><a name="111825" id="111825"></a> The Java Sound API provides the lowest level of sound support on the Java platform. It provides application programs with a great amount of control over sound operations, and it is extensible. For example, the Java Sound API supplies mechanisms for installing, accessing, and manipulating system resources such as audio mixers, MIDI synthesizers, other audio or MIDI devices, file readers and writers, and sound format converters. The Java Sound API does not include sophisticated sound editors or graphical tools, but it provides capabilities upon which such programs can be built. It emphasizes low-level control beyond that commonly expected by the end user.</p><p><a name="111827" id="111827"></a></p><p>The Java Sound API includes support for both digital audio and MIDI data. These two major modules of functionality are provided in separate packages:</p><li><a name="111834" id="111834"></a> [`javax.sound.sampled`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/package-summary.html) &#8211; This package specifies interfaces for capture, mixing, and playback of digital (sampled) audio.</li><li><a name="111837" id="111837"></a> [`javax.sound.midi`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/package-summary.html) &#8211; This package provides interfaces for MIDI synthesis, sequencing, and event transport.</li><p>Two other packages permit service providers (as opposed to application developers) to create custom software components that extend the capabilities of an implementation of the Java Sound API: <a name="111842" id="111842"></a></p><li>[`javax.sound.sampled.spi`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/package-summary.html)</li><li><a name="111843" id="111843"></a> [`javax.sound.midi.spi`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/package-summary.html)<a name="111845" id="111845"></a></li><p>This page introduces the sampled-audio system, the MIDI system, and the SPI packages. Each package is then discussed in greater detail later in the tutorial. <a name="111847" id="111847"></a></p><p>There are other Java platform APIs that also have sound-related elements. The<br><a href="http://www.oracle.com/technetwork/java/javase/tech/index-jsp-140239.html">Java Media Framework API (JMF)</a> is a higher-level API that is currently available as a Standard Extension to the Java platform. JMF specifies a unified architecture, messaging protocol, and programming interface for capturing and playing back time-based media. JMF provides a simpler solution for basic media-player application programs, and it enables synchronization between different media types, such as audio and video. On the other hand, programs that focus on sound can benefit from the Java Sound API, especially if they require more advanced features, such as the ability to carefully control buffered audio playback or directly manipulate a MIDI synthesizer. Other Java APIs with sound aspects include Java 3D and APIs for telephony and speech. An implementation of any of these APIs might use an implementation of the Java Sound API internally, but is not required to do so.</p><h2 id="What-is-Sampled-Audio"><a href="#What-is-Sampled-Audio" class="headerlink" title="What is Sampled Audio?"></a>What is Sampled Audio?</h2><p><a name="112308" id="112308"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/package-summary.html"><code>javax.sound.sampled</code></a> package handles digital audio data, which the Java Sound API refers to as sampled audio. <strong>Samples</strong> are successive snapshots of a signal. In the case of audio, the signal is a sound wave. A microphone converts the acoustic signal into a corresponding analog electrical signal, and an analog-to-digital converter transforms that analog signal into a sampled digital form. The following figure shows a brief moment in a sound recording.</p><p>A Sampled Sound Wave</p><p>This graph plots sound pressure (amplitude) on the vertical axis, and time on the horizontal axis. The amplitude of the analog sound wave is measured periodically at a certain rate, resulting in the discrete samples (the red data points in the figure) that comprise the digital audio signal. The center horizontal line indicates zero amplitude; points above the line are positive-valued samples, and points below are negative. The accuracy of the digital approximation of the analog signal depends on its resolution in time (the <strong>sampling rate</strong>) and its <strong>quantization</strong>, or resolution in amplitude (the number of bits used to represent each sample). As a point of reference, the audio recorded for storage on compact discs is sampled 44,100 times per second and represented with 16 bits per sample.</p><p><a name="115938" id="115938"></a> The term “sampled audio” is used here slightly loosely. A sound wave could be sampled at discrete intervals while being left in an analog form. For purposes of the Java Sound API, however, “sampled audio” is equivalent to “digital audio.”</p><p><a name="115936" id="115936"></a> Typically, sampled audio on a computer comes from a sound recording, but the sound could instead be synthetically generated (for example, to create the sounds of a touch-tone telephone). The term “sampled audio” refers to the type of data, not its origin.</p><p><a name="111869" id="111869"></a> The Java Sound API does not assume a specific audio hardware configuration; it is designed to allow different sorts of audio components to be installed on a system and accessed by the API. The Java Sound API supports common functionality such as input and output from a sound card (for example, for recording and playback of sound files) as well as mixing of multiple streams of audio. Here is one example of a typical audio architecture:</p><p>A Typical Audio Architecture</p><p>In this example, a device such as a sound card has various input and output ports, and mixing is provided in the software. The mixer might receive data that has been read from a file, streamed from a network, generated on the fly by an application program, or produced by a MIDI synthesizer. The mixer combines all its audio inputs into a single stream, which can be sent to an output device for rendering.</p><h2 id="What-is-MIDI"><a href="#What-is-MIDI" class="headerlink" title="What is MIDI?"></a>What is MIDI?</h2><p><a name="111878" id="111878"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/package-summary.html">javax.sound.midi</a> package contains APIs for transporting and sequencing MIDI events, and for synthesizing sound from those events.</p><p><a name="111880" id="111880"></a> Whereas sampled audio is a direct representation of a sound itself, <em>MIDI data</em> can be thought of as a recipe for creating a sound, especially a musical sound. MIDI data, unlike audio data, does not describe sound directly. Instead, it describes events that affect the sounds (or actions) performed by a MIDI-enabled device or instrument, such as a synthesizer. MIDI data is analogous to a graphical user interface’s keyboard and mouse events. In the case of MIDI, the events can be thought of as actions upon a musical keyboard, along with actions on various pedals, sliders, switches, and knobs on that musical instrument. These events need not actually originate with a hardware musical instrument; they can be simulated in software, and they can be stored in MIDI files. A program that can create, edit, and perform these files is called a sequencer. Many computer sound cards include MIDI-controllable music synthesizer chips to which sequencers can send their MIDI events. Synthesizers can also be implemented entirely in software. The synthesizers interpret the MIDI events that they receive and produce audio output. Usually the sound synthesized from MIDI data is musical sound (as opposed to speech, for example). MIDI synthesizers are also capable of generating various kinds of sound effects.</p><p><a name="111884" id="111884"></a> Some sound cards include MIDI input and output ports to which external MIDI hardware devices (such as keyboard synthesizers or other instruments) can be connected. From a MIDI input port, an application program can receive events generated by an external MIDI-equipped musical instrument. The program might play the musical performance using the computer’s internal synthesizer, save it to disk as a MIDI file, or render it into musical notation. A program might use a MIDI output port to play an external instrument, or to control other external devices such as recording equipment.</p><p>The following diagram illustrates the functional relationships between the major components in a possible MIDI configuration based on the Java Sound API. (As with audio, the Java Sound API permits a variety of MIDI software devices to be installed and interconnected. The system shown here is just one potential scenario.) The flow of data between components is indicated by arrows. The data can be in a standard file format, or (as indicated by the key in the lower right corner of the diagram), it can be audio, raw MIDI bytes, or time-tagged MIDI messages.</p><p>A Possible MIDI Configuration</p><p>In this example, the application program prepares a musical performance by loading a musical score that’s stored as a standard MIDI file on a disk (left side of the diagram). Standard MIDI files contain tracks, each of which is a list of time-tagged MIDI events. Most of the events represent musical notes (pitches and rhythms). This MIDI file is read and then “performed” by a software sequencer. A sequencer performs its music by sending MIDI messages to some other device, such as an internal or external synthesizer. The synthesizer itself may read a soundbank file containing instructions for emulating the sounds of certain musical instruments. If not, the synthesizer will play the notes stored in the MIDI file using whatever instrument sounds are already loaded into it.</p><p><a name="111898" id="111898"></a> As illustrated, the MIDI events must be translated into raw (non-time-tagged) MIDI before being sent through a MIDI output port to an external MIDI instrument. Similarly, raw MIDI data coming into the computer from an external MIDI source (a keyboard instrument, in the diagram) is translated into time-tagged MIDI messages that can control a synthesizer, or that a sequencer can store for later use.</p><h2 id="Service-Provider-Interfaces"><a href="#Service-Provider-Interfaces" class="headerlink" title="Service Provider Interfaces"></a>Service Provider Interfaces</h2><p><a name="111902" id="111902"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/package-summary.html"><code>javax.sound.sampled.spi</code></a> and<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/package-summary.html"><code>javax.sound.midi.spi</code></a> packages contain APIs that let software developers create new audio or MIDI resources that can be provided separately to the user and “plugged in” to an existing implementation of the Java Sound API. Here are some examples of services (resources) that can be added in this way:</p><ul><li><a name="111903" id="111903"></a>An audio mixer</li><li><a name="112110" id="112110"></a>A MIDI synthesizer</li><li><a name="111906" id="111906"></a>A file parser that can read or write a new type of audio or MIDI file</li><li><a name="111907" id="111907"></a>A converter that translates between different sound data formats</li></ul><p><a name="111910" id="111910"></a> In some cases, services are software interfaces to the capabilities of hardware devices, such as sound cards, and the service provider might be the same as the vendor of the hardware. In other cases, the services exist purely in software. For example, a synthesizer or a mixer could be an interface to a chip on a sound card, or it could be implemented without any hardware support at all.</p><p><a name="111912" id="111912"></a> An implementation of the Java Sound API contains a basic set of services, but the service provider interface (SPI) packages allow third parties to create new services. These third-party services are integrated into the system in the same way as the built-in services. The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioSystem.html"><code>AudioSystem</code></a> class and the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiSystem.html"><code>MidiSystem</code></a> class act as coordinators that let application programs access the services explicitly or implicitly. Often the existence of a service is completely transparent to an application program that uses it. The service-provider mechanism benefits users of application programs based on the Java Sound API, because new sound features can be added to a program without requiring a new release of the JDK or runtime environment, and, in many cases, without even requiring a new release of the application program itself.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Trail-Sound&quot;&gt;&lt;a href=&quot;#Trail-Sound&quot; class=&quot;headerlink&quot; title=&quot;Trail: Sound&quot;&gt;&lt;/a&gt;Trail: Sound&lt;/h1&gt;&lt;p&gt;The Java Sound API is a low-leve</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Providing Sampled-Audio Services</title>
    <link href="http://example.com/wiki/2021-03-04-Providing%20Sampled-Audio%20Services/"/>
    <id>http://example.com/wiki/2021-03-04-Providing%20Sampled-Audio%20Services/</id>
    <published>2021-03-04T14:32:16.362Z</published>
    <updated>2021-03-04T15:17:55.838Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Providing-Sampled-Audio-Services"><a href="#Providing-Sampled-Audio-Services" class="headerlink" title="Providing Sampled-Audio Services"></a>Providing Sampled-Audio Services</h1><p><a name="118162" id="118162"></a> As you know, the Java Sound API includes two packages, <code>javax.sound.sampled.spi</code> and <code>javax.sound.midi.spi</code>, that define abstract classes to be used by developers of sound services. By implementing and installing a subclass of one of these abstract classes, a service provider registers the new service, extending the functionality of the runtime system. This page tells you how to go about using the <code>javax.sound.sampled.spi</code> package to provide new services for handling sampled audio.</p><p><a name="118164" id="118164"></a></p><p><a name="118166" id="118166"></a> There are four abstract classes in the <code>javax.sound.sampled.spi</code> package, representing four different types of services that you can provide for the sampled-audio system:</p><li><a name="118167" id="118167"></a> [`AudioFileWriter`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/AudioFileWriter.html) provides sound file-writing services. These services make it possible for an application program to write a stream of audio data to a file of a particular type.</li><li><a name="118168" id="118168"></a> [`AudioFileReader`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/AudioFileReader.html) provides file-reading services. These services enable an application program to ascertain a sound file's characteristics, and to obtain a stream from which the file's audio data can be read.</li><li><a name="118169" id="118169"></a> [`FormatConversionProvider`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/FormatConversionProvider.html) provides services for converting audio data formats. These services allow an application program to translate audio streams from one data format to another.</li><li><a name="118170" id="118170"></a> [`MixerProvider`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/spi/MixerProvider.html) provides management of a particular kind of mixer. This mechanism allows an application program to obtain information about, and access instances of, a given kind of mixer.<a name="118172" id="118172"></a></li><p>To recapitulate earlier discussions, service providers can extend the functionality of the runtime system. A typical SPI class has two types of methods: ones that respond to queries about the types of services available from a particular provider, and ones that either perform the new service directly, or return instances of objects that actually provide the service. The runtime environment’s service-provider mechanism provides <strong>registration</strong> of installed services with the audio system, and <strong>management</strong> of the new service provider classes.</p><p><a name="120967" id="120967"></a> In essence there is a double isolation of the service instances from the application developer. An application program never directly creates instances of the service objects, such as mixers or format converters, that it needs for its audio processing tasks. Nor does the program even directly request these objects from the SPI classes that administer them. The application program makes requests to the <code>AudioSystem</code> object in the <code>javax.sound.sampled</code> package, and <code>AudioSystem</code> in turn uses the SPI objects to process these queries and service requests.</p><p><a name="120968" id="120968"></a> The existence of new audio services might be completely transparent to both the user and the application programmer. All application references are through standard objects of the <code>javax.sound.sampled</code> package, primarily <code>AudioSystem</code>, and the special handling that new services might be providing is often completely hidden.</p><p><a name="120974" id="120974"></a> In this discussion, we’ll continue the previous convention of referring to new SPI subclasses by names like <code>AcmeMixer</code> and <code>AcmeMixerProvider</code>.</p><p><a name="118177" id="118177"></a></p><h2 id="Providing-Audio-File-Writing-Services"><a href="#Providing-Audio-File-Writing-Services" class="headerlink" title="Providing Audio File-Writing Services"></a>Providing Audio File-Writing Services</h2><p><a name="118179" id="118179"></a> Let’s start with <code>AudioFileWriter</code>, one of the simpler SPI classes.</p><p><a name="119866" id="119866"></a> A subclass that implements the methods of <code>AudioFileWriter</code> must provide implementations of a set of methods to handle queries about the file formats and file types supported by the class, as well as provide methods that actually write out a supplied audio data stream to a <code>File</code> or <code>OutputStream</code>.</p><p><a name="119867" id="119867"></a> <code>AudioFileWriter</code> includes two methods that have concrete implementations in the base class:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isFileTypeSupported(AudioFileFormat.Type fileType) </span><br><span class="line">boolean isFileTypeSupported(AudioFileFormat.Type fileType, AudioInputStream stream) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first of these methods informs the caller whether this file writer can write sound files of the specified type. This method is a general inquiry, it will return <code>true</code> if the file writer can write that kind of file, assuming the file writer is handed appropriate audio data. However, the ability to write a file can depend on the format of the specific audio data that’s handed to the file writer. A file writer might not support every audio data format, or the constraint might be imposed by the file format itself. (Not all kinds of audio data can be written to all kinds of sound files.) The second method is more specific, then, asking whether a particular <code>AudioInputStream</code> can be written to a particular type of file.</p><p><a name="119891" id="119891"></a> Generally, you won’t need to override these two concrete methods. Each is simply a wrapper that invokes one of two other query methods and iterates over the results returned. These other two query methods are abstract and therefore need to be implemented in the subclass:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioFileFormat.Type[] getAudioFileTypes() </span><br><span class="line">abstract AudioFileFormat.Type[] getAudioFileTypes(AudioInputStream stream) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods correspond directly to the previous two. Each returns an array of all the supported file types-all that are supported in general, in the case of the first method, and all that are supported for a specific audio stream, in the case of the second method. A typical implementation of the first method might simply return an array that the file writer’s constructor initializes. An implementation of the second method might test the stream’s <code>AudioFormat</code> object to see whether it’s a data format that the requested type of file supports.</p><p><a name="119946" id="119946"></a> The final two methods of <code>AudioFileWriter</code> do the actual file-writing work:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract int write(AudioInputStream stream, </span><br><span class="line">     AudioFileFormat.Type fileType, java.io.File out) </span><br><span class="line">abstract int write(AudioInputStream stream, </span><br><span class="line">     AudioFileFormat.Type fileType, java.io.OutputStream out) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods write a stream of bytes representing the audio data to the stream or file specified by the third argument. The details of how this is done depend on the structure of the specified type of file. The <code>write</code> method must write the file’s header and the audio data in the manner prescribed for sound files of this format (whether it’s a standard type of sound file or a new, possibly proprietary one). <a name="118201" id="118201"></a></p><h2 id="Providing-Audio-File-Reading-Services"><a href="#Providing-Audio-File-Reading-Services" class="headerlink" title="Providing Audio File-Reading Services"></a>Providing Audio File-Reading Services</h2><p><a name="120146" id="120146"></a> The <code>AudioFileReader</code> class consists of six abstract methods that your subclass needs to implement-actually, two different overloaded methods, each of which can take a <code>File</code>, <code>URL</code>, or <code>InputStream</code> argument. The first of these overloaded methods accepts queries about the file format of a specified file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioFileFormat getAudioFileFormat(java.io.File file) </span><br><span class="line">abstract AudioFileFormat getAudioFileFormat(java.io.InputStream stream) </span><br><span class="line">abstract AudioFileFormat getAudioFileFormat(java.net.URL url) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>A typical implementation of <code>getAudioFileFormat</code> method reads and parses the sound file’s header to ascertain its file format. See the description of the AudioFileFormat class to see what fields need to be read from the header, and refer to the specification for the particular file type to figure out how to parse the header.</p><p><a name="120204" id="120204"></a> Because the caller providing a stream as an argument to this method expects the stream to be unaltered by the method, the file reader should generally start by marking the stream. After reading to the end of the header, it should reset the stream to its original position.</p><p><a name="120135" id="120135"></a> The other overloaded <code>AudioFileReader</code> method provides file-reading services, by returning an AudioInputStream from which the file’s audio data can be read:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioInputStream getAudioInputStream(java.io.File file) </span><br><span class="line">abstract AudioInputStream getAudioInputStream(java.io.InputStream stream) </span><br><span class="line">abstract AudioInputStream getAudioInputStream(java.net.URL url) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Typically, an implementation of <code>getAudioInputStream</code> returns an <code>AudioInputStream</code> wound to the beginning of the file’s data chunk (after the header), ready for reading. It would be conceivable, though, for a file reader to return an <code>AudioInputStream</code> whose audio format represents a stream of data that is in some way decoded from what is contained in the file. The important thing is that the method return a formatted stream from which the audio data contained in the file can be read. The <code>AudioFormat</code> encapsulated in the returned <code>AudioInputStream</code> object will inform the caller about the stream’s data format, which is usually, but not necessarily, the same as the data format in the file itself.</p><p><a name="120248" id="120248"></a> Generally, the returned stream is an instance of <code>AudioInputStream</code>; it’s unlikely you would ever need to subclass <code>AudioInputStream</code>.</p><p><a name="118208" id="118208"></a></p><h2 id="Providing-Format-Conversion-Services"><a href="#Providing-Format-Conversion-Services" class="headerlink" title="Providing Format-Conversion Services"></a>Providing Format-Conversion Services</h2><p><a name="120439" id="120439"></a> A <code>FormatConversionProvider</code> subclass transforms an <code>AudioInputStream</code> that has one audio data format into one that has another format. The former (input) stream is referred to as the <strong>source</strong> stream, and the latter (output) stream is referred to as the <strong>target</strong> stream. Recall that an <code>AudioInputStream</code> contains an <code>AudioFormat</code>, and the <code>AudioFormat</code> in turn contains a particular type of data encoding, represented by an <code>AudioFormat.Encoding</code> object. The format and encoding in the source stream are called the source format and source encoding, and those in the target stream are likewise called the target format and target encoding.</p><p><a name="120267" id="120267"></a> The work of conversion is performed in the overloaded abstract method of <code>FormatConversionProvider</code> called <code>getAudioInputStream</code>. The class also has abstract query methods for learning about all the supported target and source formats and encodings. There are concrete wrapper methods for querying about a specific conversion.</p><p><a name="120265" id="120265"></a> The two variants of <code>getAudioInputStream</code> are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioInputStream getAudioInputStream(AudioFormat.Encoding targetEncoding, </span><br><span class="line">     AudioInputStream sourceStream) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>and</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioInputStream getAudioInputStream(AudioFormat targetFormat, </span><br><span class="line">     AudioInputStream sourceStream) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These differ in the first argument, according to whether the caller is specifying a complete target format or just the format’s encoding.</p><p><a name="120460" id="120460"></a> A typical implementation of <code>getAudioInputStream</code> works by returning a new subclass of <code>AudioInputStream</code> that wraps around the original (source) <code>AudioInputStream</code> and applies a data format conversion to its data whenever a <code>read</code> method is invoked. For example, consider the case of a new <code>FormatConversionProvider</code> subclass called <code>AcmeCodec</code>, which works with a new <code>AudioInputStream</code> subclass called <code>AcmeCodecStream</code>.</p><p><a name="118212" id="118212"></a> The implementation of <code>AcmeCodec&#39;s</code> second <code>getAudioInputStream</code> method might be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public AudioInputStream getAudioInputStream</span><br><span class="line">      (AudioFormat outputFormat, AudioInputStream stream) &#123;</span><br><span class="line">        AudioInputStream cs &#x3D; null;</span><br><span class="line">        AudioFormat inputFormat &#x3D; stream.getFormat();</span><br><span class="line">        if (inputFormat.matches(outputFormat) ) &#123;</span><br><span class="line">            cs &#x3D; stream;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            cs &#x3D; (AudioInputStream)</span><br><span class="line">                (new AcmeCodecStream(stream, outputFormat));</span><br><span class="line">            tempBuffer &#x3D; new byte[tempBufferSize];</span><br><span class="line">        &#125;</span><br><span class="line">        return cs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The actual format conversion takes place in new <code>read</code> methods of the returned <code>AcmeCodecStream</code>, a subclass of <code>AudioInputStream</code>. Again, application programs that access this returned <code>AcmeCodecStream</code> simply operate on it as an <code>AudioInputStream</code>, and don’t need to know the details of its implementation.</p><p><a name="118232" id="118232"></a> The other methods of a <code>FormatConversionProvider</code> all permit queries about the input and output encodings and formats that the object supports. The following four methods, being abstract, need to be implemented:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract AudioFormat.Encoding[] getSourceEncodings() </span><br><span class="line">abstract AudioFormat.Encoding[] getTargetEncodings() </span><br><span class="line">abstract AudioFormat.Encoding[] getTargetEncodings(</span><br><span class="line">    AudioFormat sourceFormat) </span><br><span class="line">abstract  AudioFormat[] getTargetFormats(</span><br><span class="line">    AudioFormat.Encoding targetEncoding, </span><br><span class="line">    AudioFormat sourceFormat) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As in the query methods of the <code>AudioFileReader</code> class discussed above, these queries are typically handled by checking private data of the object and, for the latter two methods, comparing them against the argument(s).</p><p><a name="120475" id="120475"></a> The remaining four <code>FormatConversionProvider</code> methods are concrete and generally don’t need to be overridden:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isConversionSupported(</span><br><span class="line">    AudioFormat.Encoding targetEncoding,</span><br><span class="line">    AudioFormat sourceFormat) </span><br><span class="line">boolean isConversionSupported(AudioFormat targetFormat, </span><br><span class="line">    AudioFormat sourceFormat) </span><br><span class="line">boolean isSourceEncodingSupported(</span><br><span class="line">    AudioFormat.Encoding sourceEncoding) </span><br><span class="line">boolean isTargetEncodingSupported(</span><br><span class="line">    AudioFormat.Encoding targetEncoding) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As with <code>AudioFileWriter.isFileTypeSupported()</code>, the default implementation of each of these methods is essentially a wrapper that invokes one of the other query methods and iterates over the results returned. <a name="120618" id="120618"></a></p><h2 id="Providing-New-Types-of-Mixers"><a href="#Providing-New-Types-of-Mixers" class="headerlink" title="Providing New Types of Mixers"></a>Providing New Types of Mixers</h2><p><a name="118235" id="118235"></a> As its name implies, a <code>MixerProvider</code> supplies instances of mixers. Each concrete <code>MixerProvider</code> subclass acts as a factory for the <code>Mixer</code> objects used by an application program. Of course, defining a new <code>MixerProvider</code> only makes sense if one or more new implementations of the <code>Mixer</code> interface are also defined. As in the <code>FormatConversionProvider</code> example above, where our <code>getAudioInputStream</code> method returned a subclass of <code>AudioInputStream</code> that performed the conversion, our new class <code>AcmeMixerProvider</code> has a method <code>getMixer</code> that returns an instance of another new class that implements the <code>Mixer</code> interface. We’ll call the latter class <code>AcmeMixer</code>. Particularly if the mixer is implemented in hardware, the provider might support only one static instance of the requested device. If so, it should return this static instance in response to each invocation of <code>getMixer</code>.</p><p><a name="118238" id="118238"></a> Since <code>AcmeMixer</code> supports the <code>Mixer</code> interface, application programs don’t require any additional information to access its basic functionality. However, if <code>AcmeMixer</code> supports functionality not defined in the <code>Mixer</code> interface, and the vendor wants to make this extended functionality accessible to application programs, the mixer should of course be defined as a public class with additional, well-documented public methods, so that a program that wishes to make use of this extended functionality can import <code>AcmeMixer</code> and cast the object returned by <code>getMixer</code> to this type.</p><p><a name="120792" id="120792"></a> The other two methods of <code>MixerProvider</code> are:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract Mixer.Info[] getMixerInfo() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>and</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isMixerSupported(Mixer.Info info) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>These methods allow the audio system to determine whether this particular provider class can produce a device that an application program needs. In other words, the <code>AudioSystem</code> object can iterate over all the installed <code>MixerProviders</code> to see which ones, if any, can supply the device that the application program has requested of the <code>AudioSystem</code>. The <code>getMixerInfo</code> method returns an array of objects containing information about the kinds of mixer available from this provider object. The system can pass these information objects, along with those from other providers, to an application program.</p><p><a name="120729" id="120729"></a> A single <code>MixerProvider</code> can provide more than one kind of mixer. When the system invokes the <code>MixerProvider&#39;s getMixerInfo</code> method, it gets a list of information objects identifying the different kinds of mixer that this provider supports. The system can then invoke <code>MixerProvider.getMixer(Mixer.Info)</code> to obtain each mixer of interest.</p><p><a name="120841" id="120841"></a> Your subclass needs to implement <code>getMixerInfo</code>, as it’s abstract. The <code>isMixerSupported</code> method is concrete and doesn’t generally need to be overridden. The default implementation simply compares the provided <code>Mixer.Info</code> to each one in the array returned by <code>getMixerInfo</code>.</p><p>&#160;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Providing-Sampled-Audio-Services&quot;&gt;&lt;a href=&quot;#Providing-Sampled-Audio-Services&quot; class=&quot;headerlink&quot; title=&quot;Providing Sampled-Audio Serv</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Providing MIDI Services</title>
    <link href="http://example.com/wiki/2021-03-04-Providing%20MIDI%20Services/"/>
    <id>http://example.com/wiki/2021-03-04-Providing%20MIDI%20Services/</id>
    <published>2021-03-04T14:32:16.307Z</published>
    <updated>2021-03-04T15:17:55.831Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Providing-MIDI-Services"><a href="#Providing-MIDI-Services" class="headerlink" title="Providing MIDI Services"></a>Providing MIDI Services</h1><p><a href="SPI-intro.html">Introduction to the Service Provider Interfaces</a> explained that the <code>javax.sound.sampled.spi</code> and <code>javax.sound.midi.spi</code> packages define abstract classes to be used by developers of sound services. By implementing a subclass of one of these abstract classes, a service provider can create a new service that extends the functionality of the runtime system. The previous section covered the use of the <code>javax.sound.sampled.spi</code> package. This section discusses how to use the <code>javax.sound.midi.spi</code> package to provide new services for handling MIDI devices and files.</p><p><a name="122556" id="122556"></a> There are four abstract classes in the <code>javax.sound.midi.spi</code> package, which represent four different types of services that you can provide for the MIDI system:</p><li><a name="122558" id="122558"></a> [`MidiFileWriter`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/MidiFileWriter.html) provides MIDI file-writing services. These services make it possible for an application program to save, to a MIDI file, a MIDI `Sequence` that it has generated or processed.</li><li><a name="122559" id="122559"></a> [`MidiFileReader`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/MidiFileReader.html) provides file-reading services that return a MIDI `Sequence` from a MIDI file for use in an application program.</li><li><a name="122560" id="122560"></a> [`MidiDeviceProvider`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/MidiDeviceProvider.html) supplies instances of one or more specific types of MIDI device, possibly including hardware devices.</li><li><a name="122561" id="122561"></a> [`SoundbankReader`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/spi/SoundbankReader.html) supplies soundbank file-reading services. Concrete subclasses of `SoundbankReader` parse a given soundbank file, producing a `Soundbank` object that can be loaded into a `Synthesizer`.</li><p>An application program will not directly create an instance of a service object&#226;&#128;&#148;whether a provider object, such as a <code>MidiDeviceProvider</code>, or an object, such as a <code>Synthesizer</code>, that is supplied by the provider object. Nor will the program directly refer to the SPI classes. Instead, the application program makes requests to the <code>MidiSystem</code> object in the <code>javax.sound.midi</code> package, and <code>MidiSystem</code> in turn uses concrete subclasses of the <code>javax.sound.midi.spi</code> classes to process these requests.</p><p><a name="122566" id="122566"></a></p><h2 id="Providing-MIDI-File-Writing-Services"><a href="#Providing-MIDI-File-Writing-Services" class="headerlink" title="Providing MIDI File-Writing Services"></a>Providing MIDI File-Writing Services</h2><p><a name="122568" id="122568"></a> There are three standard MIDI file formats, all of which an implementation of the Java Sound API can support: Type 0, Type 1, and Type 2. These file formats differ in their internal representation of the MIDI sequence data in the file, and are appropriate for different kinds of sequences. If an implementation doesn’t itself support all three types, a service provider can supply the support for the unimplemented ones. There are also variants of the standard MIDI file formats, some of them proprietary, which similarly could be supported by a third-party vendor.</p><p><a name="124323" id="124323"></a> The ability to write MIDI files is provided by concrete subclasses of <code>MidiFileWriter</code>. This abstract class is directly analogous to <code>javax.sampled.spi.AudioFileWriter</code>. Again, the methods are grouped into query methods for learning what types of files can be written, and methods for actually writing a file. As with <code>AudioFileWriter</code>, two of the query methods are concrete:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isFileTypeSupported(int fileType)</span><br><span class="line">boolean isFileTypeSupported(int fileType, Sequence sequence) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first of these provides general information about whether the file writer can ever write the specified type of MIDI file type. The second method is more specific: it asks whether a particular Sequence can be written to the specified type of MIDI file. Generally, you don’t need to override either of these two concrete methods. In the default implementation, each invokes one of two other corresponding query methods and iterates over the results returned. Being abstract, these other two query methods need to be implemented in the subclass:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract int[] getMidiFileTypes() </span><br><span class="line">abstract int[] getMidiFileTypes(Sequence sequence) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first of these returns an array of all the file types that are supported in general. A typical implementation might initialize the array in the file writer’s constructor and return the array from this method. From that set of file types, the second method finds the subset to which the file writer can write the given Sequence. In accordance with the MIDI specification, not all types of sequences can be written to all types of MIDI files.</p><p><a name="122590" id="122590"></a> The <code>write</code> methods of a <code>MidiFileWriter</code> subclass perform the encoding of the data in a given <code>Sequence</code> into the correct data format for the requested type of MIDI file, writing the coded stream to either a file or an output stream:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract int write(Sequence in, int fileType, </span><br><span class="line">                   java.io.File out) </span><br><span class="line">abstract int write(Sequence in, int fileType, </span><br><span class="line">                   java.io.OutputStream out) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To do this, the <code>write</code> method must parse the <code>Sequence</code> by iterating over its tracks, construct an appropriate file header, and write the header and tracks to the output. The MIDI file’s header format is, of course, defined by the MIDI specification. It includes such information as a “magic number” identifying this as a MIDI file, the header’s length, the number of tracks, and the sequence’s timing information (division type and resolution). The rest of the MIDI file consists of the track data, in the format defined by the MIDI specification.</p><p><a name="124482" id="124482"></a> Let’s briefly look at how the application program, MIDI system, and service provider cooperate in writing a MIDI file. In a typical situation, an application program has a particular MIDI <code>Sequence</code> to save to a file. The program queries the <code>MidiSystem</code> object to see what MIDI file formats, if any, are supported for the particular <code>Sequence</code> at hand, before attempting to write the file. The <code>MidiSystem.getMidiFileTypes(Sequence)</code> method returns an array of all the MIDI file types to which the system can write a particular sequence. It does this by invoking the corresponding <code>getMidiFileTypes</code> method for each of the installed <code>MidiFileWriter</code> services, and collecting and returning the results in an array of integers that can be thought of as a master list of all file types compatible with the given <code>Sequence</code>. When it comes to writing the <code>Sequence</code> to a file, the call to <code>MidiSystem.write</code> is passed an integer representing a file type, along with the <code>Sequence</code> to be written and the output file; <code>MidiSystem</code> uses the supplied type to decide which installed <code>MidiFileWriter</code> should handle the write request, and dispatches a corresponding <code>write</code> to the appropriate <code>MidiFileWriter</code>.</p><p><a name="122592" id="122592"></a></p><h2 id="Providing-MIDI-File-Reading-Services"><a href="#Providing-MIDI-File-Reading-Services" class="headerlink" title="Providing MIDI File-Reading Services"></a>Providing MIDI File-Reading Services</h2><p><a name="124559" id="124559"></a> The <code>MidiFileReader</code> abstract class is directly analogous to <code>javax.sampled.spi.AudioFileReader</code> class. Both consist of two overloaded methods, each of which can take a <code>File</code>, <code>URL</code>, or <code>InputStream</code> argument. The first of the overloaded methods returns the file format of a specified file. In the case of <code>MidiFileReader</code>, the API is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract MidiFileFormat getMidiFileFormat(java.io.File file) </span><br><span class="line">abstract MidiFileFormat getMidiFileFormat(</span><br><span class="line">    java.io.InputStream stream) </span><br><span class="line">abstract MidiFileFormat getMidiFileFormat(java.net.URL url) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Concrete subclasses must implement these methods to return a filled-out <code>MidiFileFormat</code> object describing the format of the specified MIDI file (or stream or URL), assuming that the file is of a type supported by the file reader and that it contains valid header information. Otherwise, an <code>InvalidMidiDataException</code> should be thrown.</p><p><a name="124619" id="124619"></a> The other overloaded method returns a MIDI <code>Sequence</code> from a given file, stream, or URL :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract Sequence getSequence(java.io.File file) </span><br><span class="line">abstract Sequence getSequence(java.io.InputStream stream) </span><br><span class="line">abstract Sequence getSequence(java.net.URL url) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The <code>getSequence</code> method performs the actual work of parsing the bytes in the MIDI input file and constructing a corresponding <code>Sequence</code> object. This is essentially the inverse of the process used by <code>MidiFileWriter.write</code>. Because there is a one-to-one correspondence between the contents of a MIDI file as defined by the MIDI specification and a <code>Sequence</code> object as defined by the Java Sound API, the details of the parsing are straightforward. If the file passed to <code>getSequence</code> contains data that the file reader can’t parse (for example, because the file has been corrupted or doesn’t conform to the MIDI specification), an <code>InvalidMidiDataException</code> should be thrown. <a name="122598" id="122598"></a></p><h2 id="Providing-Particular-MIDI-Devices"><a href="#Providing-Particular-MIDI-Devices" class="headerlink" title="Providing Particular MIDI Devices"></a>Providing Particular MIDI Devices</h2><p><a name="124755" id="124755"></a> A <code>MidiDeviceProvider</code> can be considered a factory that supplies one or more particular types of MIDI device. The class consists of a method that returns an instance of a MIDI device, as well as query methods to learn what kinds of devices this provider can supply.</p><p><a name="124728" id="124728"></a> As with the other <code>javax.sound.midi.spi</code> services, application developers get indirect access to a <code>MidiDeviceProvider</code> service through a call to <code>MidiSystem</code> methods, in this case <code>MidiSystem.getMidiDevice</code> and <code>MidiSystem.getMidiDeviceInfo</code>. The purpose of subclassing <code>MidiDeviceProvider</code> is to supply a new kind of device, so the service developer must also create an accompanying class for the device being returned&#226;&#128;&#148;just as we saw with <code>MixerProvider</code> in the <code>javax.sound.sampled.spi</code> package. There, the returned device’s class implemented the <code>javax.sound.sampled.Mixer</code> interface; here it implements the <code>javax.sound.midi.MidiDevice</code> interface. It might also implement a subinterface of <code>MidiDevice</code>, such as <code>Synthesizer</code> or <code>Sequencer</code>.</p><p><a name="124812" id="124812"></a> Because a single subclass of <code>MidiDeviceProvider</code> can provide more than one type of <code>MidiDevice</code>, the <code>getDeviceInfo</code> method of the class returns an array of <code>MidiDevice.Info</code> objects enumerating the different <code>MidiDevices</code> available:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract MidiDevice.Info[] getDeviceInfo() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The returned array can contain a single element, of course. A typical implementation of the provider might initialize an array in its constructor and return it here. This allows <code>MidiSystem</code> to iterate over all installed <code>MidiDeviceProviders</code> to construct a list of all installed devices. <code>MidiSystem</code> can then return this list (<code>MidiDevice.Info[]</code> array) to an application program.</p><p><a name="124843" id="124843"></a> <code>MidiDeviceProvider</code> also includes a concrete query method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isDeviceSupported(MidiDevice.Info info) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method permits the system to query the provider about a specific kind of device. Generally, you don’t need to override this convenience method. The default implementation iterates over the array returned by getDeviceInfo and compares the argument to each element.</p><p><a name="124862" id="124862"></a> The third and final <code>MidiDeviceProvider</code> method returns the requested device:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract MidiDevice getDevice(MidiDevice.Info info) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method should first test the argument to make sure it describes a device that this provider can supply. If it doesn’t, it should throw an <code>IllegalArgumentException</code>. Otherwise, it returns the device. <a name="122605" id="122605"></a></p><h2 id="Providing-Soundbank-File-Reading-Services"><a href="#Providing-Soundbank-File-Reading-Services" class="headerlink" title="Providing Soundbank File-Reading Services"></a>Providing Soundbank File-Reading Services</h2><p><a name="122607" id="122607"></a> A <code>SoundBank</code> is a set of <code>Instruments</code> that can be loaded into a <code>Synthesizer</code>. An <code>Instrument</code> is an implementation of a sound-synthesis algorithm that produces a particular sort of sound, and includes accompanying name and information strings. A <code>SoundBank</code> roughly corresponds to a bank in the MIDI specification, but it’s a more extensive and addressable collection; it can perhaps better be thought of as a collection of MIDI banks.</p><p><a name="122609" id="122609"></a> <code>SoundbankReader</code> consists of a single overloaded method, which the system invokes to read a <code>Soundbank</code> object from a soundbank file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">abstract Soundbank getSoundbank(java.io.File file) </span><br><span class="line">abstract Soundbank getSoundbank(java.io.InputStream stream) </span><br><span class="line">abstract Soundbank getSoundbank(java.net.URL url) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Concrete subclasses of <code>SoundbankReader</code> will work in tandem with particular provider-defined implementations of <code>SoundBank</code>, <code>Instrument</code>, and <code>Synthesizer</code> to allow the system to load a <code>SoundBank</code> from a file into an instance of a particular <code>Synthesizer</code> class. Synthesis techniques may differ wildly from one <code>Synthesizer</code> to another, and, as a consequence, the data stored in an <code>Instrument</code> or <code>SoundBank</code> providing control or specification data for the synthesis process of a <code>Synthesizer</code> can take a variety of forms. One synthesis technique may require only a few bytes of parameter data; another may be based on extensive sound samples. The resources present in a <code>SoundBank</code> will depend upon the nature of the <code>Synthesizer</code> into which they get loaded, and therefore the implementation of the <code>getSoundbank</code> method of a <code>SoundbankReader</code> subclass has access to knowledge of a particular kind of <code>SoundBank</code>. In addition, a particular subclass of <code>SoundbankReader</code> understands a particular file format for storing the <code>SoundBank</code> data. That file format may be vendor-specific and proprietary.</p><p><a name="124966" id="124966"></a> <code>SoundBank</code> is just an interface, with only weak constraints on the contents of a <code>SoundBank</code> object. The methods an object must support to implement this interface (<code>getResources</code>, <code>getInstruments</code>, <code>getVendor</code>, <code>getName</code>, etc.) impose loose requirements on the data that the object contains. For example, <code>getResources</code> and <code>getInstruments</code> can return empty arrays. The actual contents of a subclassed <code>SoundBank</code> object, in particular its instruments and its non-instrument resources, are defined by the service provider. Thus, the mechanism of parsing a soundbank file depends entirely on the specification of that particular kind of soundbank file.</p><p><a name="122614" id="122614"></a> Soundbank files are created outside the Java Sound API, typically by the vendor of the synthesizer that can load that kind of soundbank. Some vendors might supply end-user tools for creating such files.</p><p>&#160;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Providing-MIDI-Services&quot;&gt;&lt;a href=&quot;#Providing-MIDI-Services&quot; class=&quot;headerlink&quot; title=&quot;Providing MIDI Services&quot;&gt;&lt;/a&gt;Providing MIDI Se</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Processing Audio with Controls</title>
    <link href="http://example.com/wiki/2021-03-04-Processing%20Audio%20with%20Controls/"/>
    <id>http://example.com/wiki/2021-03-04-Processing%20Audio%20with%20Controls/</id>
    <published>2021-03-04T14:32:16.296Z</published>
    <updated>2021-03-04T15:17:55.828Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Processing-Audio-with-Controls"><a href="#Processing-Audio-with-Controls" class="headerlink" title="Processing Audio with Controls"></a>Processing Audio with Controls</h1><p>Previous sections have discussed how to play or capture audio samples. The implicit goal has been to deliver samples as faithfully as possible, without modification (other than possibly mixing the samples with those from other audio lines). Sometimes, however, you want to be able to modify the signal. The user might want it to sound louder, quieter, fuller, more reverberant, higher or lower in pitch, and so on. This page discusses the Java Sound API features that provide these kinds of signal processing.</p><p><a name="114172" id="114172"></a> There are two ways to apply signal processing:</p><li><a name="114174" id="114174"></a>You can use any processing supported by the mixer or its component lines, by querying for [`Control`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Control.html) objects and then setting the controls as the user desires. Typical controls supported by mixers and lines include gain, pan, and reverberation controls.</li>- <a name="114175" id="114175"></a>If the kind of processing you need isn't provided by the mixer or its lines, your program can operate directly on the audio bytes, manipulating them as desired.<p><a name="114177" id="114177"></a> This page discusses the first technique in greater detail, because there is no special API for the second technique.</p><p><a name="114180" id="114180"></a></p><h2 id="Introduction-to-Controls"><a href="#Introduction-to-Controls" class="headerlink" title="Introduction to Controls"></a>Introduction to Controls</h2><p><a name="114182" id="114182"></a> A mixer can have various sorts of signal-processing controls on some or all of its lines. For example, a mixer used for audio capture might have an input port with a gain control, and target data lines with gain and pan controls. A mixer used for audio playback might have sample-rate controls on its source data lines. In each case, the controls are all accessed through methods of the <code>Line</code> interface.</p><p><a name="114185" id="114185"></a> Because the <code>Mixer</code> interface extends <code>Line</code>, the mixer itself can have its own set of controls. These might serve as master controls affecting all the mixer’s source or target lines. For example, the mixer might have a master gain control whose value in decibels is added to the values of individual gain controls on its target lines.</p><p><a name="115789" id="115789"></a> Others of the mixer’s own controls might affect a special line, neither a source nor a target, that the mixer uses internally for its processing. For example, a global reverb control might choose the sort of reverberation to apply to a mixture of the input signals, and this “wet” (reverberated) signal would get mixed back into the “dry” signal before delivery to the mixer’s target lines.</p><p><a name="114187" id="114187"></a> If the mixer or any of its lines have controls, you might wish to expose the controls via graphical objects in your program’s user interface, so that the user can adjust the audio characteristics as desired. The controls are not themselves graphical; they just allow you to retrieve and change their settings. It’s up to you to decide what sort of graphical representations (sliders, buttons, etc.), if any, to use in your program.</p><p><a name="114189" id="114189"></a> All controls are implemented as concrete subclasses of the abstract class <code>Control</code>. Many typical audio-processing controls can be described by abstract subclasses of <code>Control</code> based on a data type (such as boolean, enumerated, or float). Boolean controls, for example, represent binary-state controls, such as on/off controls for mute or reverb. Float controls, on the other hand, are well suited to represent continuously variable controls, such as pan, balance, or volume.</p><p><a name="114191" id="114191"></a> The Java Sound API specifies the following abstract subclasses of <code>Control</code>:</p><li><a name="114193" id="114193"></a> [`BooleanControl`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/BooleanControl.html)&#8212; represents a binary-state (true or false) control. For example, mute, solo, and on/off switches would be good candidates for `BooleanControls`. <a name="114194" id="114194"></a></li><li>[`FloatControl`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/FloatControl.html) &#8212; data model providing control over a range of floating-point values. For example, volume and pan are `FloatControls` that could be manipulated via a dial or slider.</li><li><a name="114195" id="114195"></a> [`EnumControl`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/EnumControl.html)&#8212; offers a choice from a set of objects. For example, you might associate a set of buttons in the user interface with an `EnumControl` to select one of several preset reverberation settings.</li><li><a name="114196" id="114196"></a> [`CompoundControl`](https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/CompoundControl.html)&#226;&#128;&#148;provides access to a collection of related items, each of which is itself an instance of a `Control` subclass. `CompoundControls` represent multi-control modules such as graphic equalizers. (A graphic equalizer would typically be depicted by a set of sliders, each affecting a `FloatControl`.)</li><p><a name="114198" id="114198"></a> Each subclass of <code>Control</code> above has methods appropriate for its underlying data type. Most of the classes include methods that set and get the control’s current value(s), get the control’s label(s), and so on.</p><p><a name="115790" id="115790"></a> Of course, each class has methods that are particular to it and the data model represented by the class. For example, <code>EnumControl</code> has a method that lets you get the set of its possible values, and <code>FloatControl</code> permits you to get its minimum and maximum values, as well as the precision (increment or step size) of the control.</p><p><a name="114976" id="114976"></a> Each subclass of <code>Control</code> has a corresponding <code>Control.Type</code> subclass, which includes static instances that identify specific controls.</p><p><a name="115785" id="115785"></a> The following table shows each <code>Control</code> subclass, its corresponding <code>Control.Type</code> subclass, and the static instances that indicate specific kinds of controls:</p><p><a name="115028" id="115028"></a></p><th id="h1"><a name="114979" id="114979"></a> `Control`</th><th id="h2"><a name="114981" id="114981"></a> `Control.Type`</th><th id="h3"><a name="114983" id="114983"></a> `Control.Type` **instances**</th><td headers="h1"><a name="114992" id="114992"></a> `CompoundControl`</td><td headers="h2"><a name="114994" id="114994"></a> `CompoundControl.Type`</td><td headers="h3"><a name="114996" id="114996"></a> (none)</td><p><a name="115029" id="115029"></a></p><p><a name="117861" id="117861"></a> An implementation of the Java Sound API can provide any or all of these control types on its mixers and lines. It can also supply additional control types not defined in the Java Sound API. Such control types could be implemented via concrete subclasses of any of these four abstract subclasses, or via additional <code>Control</code> subclasses that don’t inherit from any of these four abstract subclasses. An application program can query each line to find what controls it supports.</p><p><a name="117862" id="117862"></a></p><h3 id="Getting-a-Line-that-Has-the-Desired-Controls"><a href="#Getting-a-Line-that-Has-the-Desired-Controls" class="headerlink" title="Getting a Line that Has the Desired Controls"></a>Getting a Line that Has the Desired Controls</h3><p><a name="114235" id="114235"></a> In many cases, an application program will simply display whatever controls happen to be supported by the line in question. If the line doesn’t have any controls, so be it. But what if it’s important to find a line that has certain controls? In that case, you can use a <code>Line.Info</code> to obtain a line that has the right characteristics, as previously described under<br><a href="accessing.html#113154">Getting a Line of a Desired Type</a>.</p><p><a name="114237" id="114237"></a> For example, suppose you prefer an input port that lets the user set the volume of the sound input. The following code excerpt shows how one might query the default mixer to determine whether it has the desired port and control:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Port lineIn;</span><br><span class="line">FloatControl volCtrl;</span><br><span class="line">try &#123;</span><br><span class="line">  mixer &#x3D; AudioSystem.getMixer(null);</span><br><span class="line">  lineIn &#x3D; (Port)mixer.getLine(Port.Info.LINE_IN);</span><br><span class="line">  lineIn.open();</span><br><span class="line">  volCtrl &#x3D; (FloatControl) lineIn.getControl(&lt;br &#x2F;&gt;</span><br><span class="line">      FloatControl.Type.VOLUME);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Assuming getControl call succeeds, </span><br><span class="line">  &#x2F;&#x2F; we now have our LINE_IN VOLUME control.</span><br><span class="line">&#125; catch (Exception e) &#123;</span><br><span class="line">  System.out.println(&quot;Failed trying to find LINE_IN&quot;</span><br><span class="line">    + &quot; VOLUME control: exception &#x3D; &quot; + e);</span><br><span class="line">&#125;</span><br><span class="line">if (volCtrl !&#x3D; null)</span><br><span class="line">  &#x2F;&#x2F; ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Getting-the-Controls-from-the-Line"><a href="#Getting-the-Controls-from-the-Line" class="headerlink" title="Getting the Controls from the Line"></a>Getting the Controls from the Line</h3><p><a name="114257" id="114257"></a> An application program that needs to expose controls in its user interface might simply query the available lines and controls, and then display an appropriate user-interface element for every control on every line of interest. In such a case, the program’s only mission is to provide the user with “handles” on the control; not to know what those controls do to the audio signal. As long as the program knows how to map a line’s controls into user-interface elements, the Java Sound API architecture of <code>Mixer</code>, <code>Line</code>, and <code>Control</code> will generally take care of the rest.</p><p><a name="117125" id="117125"></a> For example, suppose your program plays back sound. You’re using a <code>SourceDataLine</code>, which you’ve obtained as previously described under<br><a href="accessing.html#113154">Getting a Line of a Desired Type</a>. You can access the line’s controls by invoking the following <code>Line</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Control[] getControls()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Then, for each of the controls in this returned array, you then use the following <code>Control</code> method to get the control’s type:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Control.Type getType()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Knowing the specific <code>Control.Type</code> instance, your program can display a corresponding user-interface element. Of course, choosing “a corresponding user-interface element” for a specific <code>Control.Type</code> depends on the approach taken by your program. On the one hand, you might use the same kind of element to represent all <code>Control.Type</code> instances of the same class. This would require you to query the <strong>class</strong> of the <code>Control.Type</code> instance using, for example, the <code>Object.getClass</code> method. Let’s say the result matched <code>BooleanControl.Type</code>. In this case, your program might display a generic checkbox or toggle button, but if its class matched <code>FloatControl.Type</code>, then you might display a graphic slider.</p><p><a name="116400" id="116400"></a> On the other hand, your program might distinguish between different types of controls&#226;&#128;&#148;even those of the same class&#226;&#128;&#148;and use a different user-interface element for each one. This would require you to test the <strong>instance</strong> returned by <code>Control&#39;s getType</code> method. Then if, for example, the type matched <code>BooleanControl.Type.APPLY_REVERB</code>, your program might display a checkbox; while if the type matched <code>BooleanControl.Type.MUTE</code>, you might instead display a toggle button.</p><p><a name="116401" id="116401"></a></p><h3 id="Using-a-Control-to-Change-the-Audio-Signal"><a href="#Using-a-Control-to-Change-the-Audio-Signal" class="headerlink" title="Using a Control to Change the Audio Signal"></a>Using a Control to Change the Audio Signal</h3><p><a name="116402" id="116402"></a> Now that you know how to access a control and determine its type, this section will describe how to use <code>Controls</code> to change aspects of the audio signal. This section doesn’t cover every available control; rather, it provides a few examples in this area to show you how to get started. These example include:</p><ul><li><a name="114276" id="114276"></a>Controlling a line’s mute state</li><li><a name="114277" id="114277"></a>Changing a line’s volume</li><li><a name="114278" id="114278"></a>Selecting among various reverberation presets</li></ul><p><a name="114280" id="114280"></a> Suppose that your program has accessed all of its mixers, their lines and the controls on those lines, and that it has a data structure to manage the logical associations between the controls and their corresponding user-interface elements. Then, translating the user’s manipulations of those controls into the corresponding <code>Control</code> methods becomes a fairly straightforward matter.</p><p><a name="114282" id="114282"></a> The following subsections describe some of the methods that must be invoked to affect the changes to specific controls.</p><p><a name="114284" id="114284"></a></p><h3 id="Controlling-a-Line’s-Mute-State"><a href="#Controlling-a-Line’s-Mute-State" class="headerlink" title="Controlling a Line’s Mute State"></a>Controlling a Line’s Mute State</h3><p><a name="114286" id="114286"></a> Controlling the mute state of any line is simply a matter of calling the following <code>BooleanControl</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setValue(boolean value)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>(Presumably, the program knows, by referring to its control-management data structures, that the mute is an instance of a <code>BooleanControl</code>.) To mute the signal that’s passing through the line, the program invokes the method above, specifying <code>true</code> as the value. To turn muting off, permitting the signal to flow through the line, the program invokes the method with the parameter set to <code>false</code>. <a name="114292" id="114292"></a></p><h3 id="Changing-a-Line’s-Volume"><a href="#Changing-a-Line’s-Volume" class="headerlink" title="Changing a Line’s Volume"></a>Changing a Line’s Volume</h3><p><a name="114294" id="114294"></a> Let’s assume your program associates a particular graphic slider with a particular line’s volume control. The value of a volume control (i.e., <code>FloatControl.Type.VOLUME</code>) is set using the following <code>FloatControl</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setValue(float newValue)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Detecting that the user moved the slider, the program gets the slider’s current value and passes it, as the parameter <code>newValue</code>, to the method above. This changes the volume of the signal flowing though the line that “owns” the control. <a name="114300" id="114300"></a></p><h3 id="Selecting-among-Various-Reverberation-Presets"><a href="#Selecting-among-Various-Reverberation-Presets" class="headerlink" title="Selecting among Various Reverberation Presets"></a>Selecting among Various Reverberation Presets</h3><p><a name="114302" id="114302"></a> Let’s suppose that our program has a mixer with a line that has a control of type <code>EnumControl.Type.REVERB</code>. Calling the <code>EnumControl</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">java.lang.Objects[] getValues()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>on that control produces an array of <code>ReverbType</code> objects. If desired, the particular parameter settings of each of these objects can be accessed using the following <code>ReverbType</code> methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int getDecayTime() </span><br><span class="line">int getEarlyReflectionDelay() </span><br><span class="line">float getEarlyReflectionIntensity() </span><br><span class="line">int getLateReflectionDelay() </span><br><span class="line">float getLateReflectionIntensity() </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>For example, if a program only wants a single reverb setting that sounds like a cavern, it can iterate over the <code>ReverbType</code> objects until it finds one for which <code>getDecayTime</code> returns a value greater than 2000. For a thorough explanation of these methods, including a table of representative return values, see the API reference documentation for <code>javax.sound.sampled.ReverbType</code>.</p><p><a name="114316" id="114316"></a> Typically, though, a program will create a user-interface element, for example, a radio button, for each of the <code>ReverbType</code> objects within the array returned by the <code>getValues</code> method. When the user clicks on one of these radio buttons, the program invokes the <code>EnumControl</code> method</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void setValue(java.lang.Object value) </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>where <code>value</code> is set to the <code>ReverbType</code> that corresponds to the newly engaged button. The audio signal sent through the line that “owns” this <code>EnumControl</code> will then be reverberated according to the parameter settings that constitute the control’s current <code>ReverbType</code> (i.e., the particular <code>ReverbType</code> specified in the <code>value</code> argument of the <code>setValue</code> method).</p><p><a name="114323" id="114323"></a> So, from our application program’s perspective, enabling a user to move from one reverberation preset (i.e., ReverbType) to another is simply a matter of connecting each element of the array returned by <code>getValues</code> to a distinct radio button.</p><p><a name="114326" id="114326"></a></p><h3 id="Manipulating-the-Audio-Data-Directly"><a href="#Manipulating-the-Audio-Data-Directly" class="headerlink" title="Manipulating the Audio Data Directly"></a>Manipulating the Audio Data Directly</h3><p><a name="114328" id="114328"></a> The <code>Control</code> API allows an implementation of the Java Sound API, or a third-party provider of a mixer, to supply arbitrary sorts of signal processing through controls. But what if no mixer offers the kind of signal processing you need? It will take more work, but you might be able to implement the signal processing in your program. Because the Java Sound API gives you access to the audio data as an array of bytes, you can alter these bytes in any way you choose.</p><p><a name="114330" id="114330"></a> If you’re processing incoming sound, you can read the bytes from a <code>TargetDataLine</code> and then manipulate them. An algorithmically trivial example that can yield sonically intriguing results is to play a sound backwards by arranging its frames in reverse order. This trivial example may not be of much use for your program, but there are numerous sophisticated digital signal processing (DSP) techniques that might be more appropriate. Some examples are equalization, dynamic-range compression, peak limiting, and time stretching or compression, as well as special effects such as delay, chorus, flanging, distortion, and so on.</p><p><a name="114332" id="114332"></a> To play back processed sound, you can place your manipulated array of bytes into a <code>SourceDataLine</code> or <code>Clip</code>. Of course, the array of bytes need not be derived from an existing sound. You can synthesize sounds from scratch, although this requires some knowledge of acoustics or else access to sound-synthesis functions. For either processing or synthesis, you may want to consult an audio DSP textbook for the algorithms in which you’re interested, or else import a third-party library of signal-processing functions into your program. For playback of synthesized sound, consider whether the <code>Synthesizer</code> API in the <code>javax.sound.midi</code> package meets your needs instead. You’ll learn about more about <code>javax.sound.midi</code> later under<br><a href="MIDI-synth.html">Synthesizing Sound</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Processing-Audio-with-Controls&quot;&gt;&lt;a href=&quot;#Processing-Audio-with-Controls&quot; class=&quot;headerlink&quot; title=&quot;Processing Audio with Controls&quot;&gt;</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Playing Back Audio</title>
    <link href="http://example.com/wiki/2021-03-04-Playing%20Back%20Audio/"/>
    <id>http://example.com/wiki/2021-03-04-Playing%20Back%20Audio/</id>
    <published>2021-03-04T14:32:16.241Z</published>
    <updated>2021-03-04T15:17:55.824Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Playing-Back-Audio"><a href="#Playing-Back-Audio" class="headerlink" title="Playing Back Audio"></a>Playing Back Audio</h1><p>Playback is sometimes referred to as <strong>presentation</strong> or <strong>rendering</strong>. These are general terms that are applicable to other kinds of media besides sound. The essential feature is that a sequence of data is delivered somewhere for eventual perception by a user. If the data is time-based, as sound is, it must be delivered at the correct rate. With sound even more than video, it’s important that the rate of data flow be maintained, because interruptions to sound playback often produce loud clicks or irritating distortion. The Java Sound API is designed to help application programs play sounds smoothly and continuously, even very long sounds.</p><p><a name="113599" id="113599"></a> Earlier you saw how to obtain a line from the audio system or from a mixer. Here you will learn how to play sound through a line.</p><p><a name="113601" id="113601"></a> As you know, there are two kinds of line that you can use for playing sound: a<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Clip.html"><code>Clip</code></a> and a<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/SourceDataLine.html"><code>SourceDataLine</code></a>. The primary difference between the two is that with a <code>Clip</code> you specify all the sound data at one time, before playback, whereas with a <code>SourceDataLine</code> you keep writing new buffers of data continuously during playback. Although there are many situations in which you could use either a <code>Clip</code> or a <code>SourceDataLine</code>, the following criteria help identify which kind of line is better suited for a particular situation:</p><li><a name="113603" id="113603"></a>Use a `Clip` when you have non-real-time sound data that can be preloaded into memory. <a name="115889" id="115889"></a>For example, you might read a short sound file into a clip. If you want the sound to play back more than once, a `Clip` is more convenient than a `SourceDataLine`, especially if you want the playback to loop (cycle repeatedly through all or part of the sound). If you need to start the playback at an arbitrary position in the sound, the `Clip` interface provides a method to do that easily. Finally, playback from a `Clip` generally has less latency than buffered playback from a `SourceDataLine`. In other words, because the sound is preloaded into a clip, playback can start immediately instead of having to wait for the buffer to be filled.</li><li><a name="113605" id="113605"></a>Use a `SourceDataLine` for streaming data, such as a long sound file that won't all fit in memory at once, or a sound whose data can't be known in advance of playback. <a name="115858" id="115858"></a>As an example of the latter case, suppose you're monitoring sound input&#226;&#128;&#148;that is, playing sound back as it's being captured. If you don't have a mixer that can send input audio right back out an output port, your application program will have to take the captured data and send it to an audio-output mixer. In this case, a `SourceDataLine` is more appropriate than a `Clip`. Another example of sound that can't be known in advance occurs when you synthesize or manipulate the sound data interactively in response to the user's input. For example, imagine a game that gives aural feedback by "morphing" from one sound to another as the user moves the mouse. The dynamic nature of the sound transformation requires the application program to update the sound data continuously during playback, instead of supplying it all before playback starts.</li><p><a name="113609" id="113609"></a></p><h2 id="Using-a-Clip"><a href="#Using-a-Clip" class="headerlink" title="Using a Clip"></a>Using a Clip</h2><p><a name="117625" id="117625"></a> You obtain a <code>Clip</code> as described earlier under<br><a href="accessing.html#113154">Getting a Line of a Desired Type</a>; Construct a <code>DataLine.Info</code> object with <code>Clip.class</code> for the first argument, and pass this <code>DataLine.Info</code> as an argument to the <code>getLine</code> method of <code>AudioSystem</code> or <code>Mixer</code>.</p><p><a name="113615" id="113615"></a> Obtaining a line just means you’ve gotten a way to refer to it; <code>getLine</code> doesn’t actually reserve the line for you. Because a mixer might have a limited number of lines of the desired type available, it can happen that after you invoke <code>getLine</code> to obtain the clip, another application program jumps in and grabs the clip before you’re ready to start playback. To actually use the clip, you need to reserve it for your program’s exclusive use by invoking one of the following <code>Clip</code> methods:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void open(AudioInputStream stream)</span><br><span class="line">void open(AudioFormat format, byte[] data, int offset, int bufferSize)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Despite the <code>bufferSize</code> argument in the second <code>open</code> method above, <code>Clip</code> (unlike <code>SourceDataLine</code>) includes no methods for writing new data to the buffer. The <code>bufferSize</code> argument here just specifies how much of the byte array to load into the clip. It’s not a buffer into which you can subsequently load more data, as you can with a <code>SourceDataLine&#39;s</code> buffer.</p><p><a name="113623" id="113623"></a> After opening the clip, you can specify at what point in the data it should start playback, using <code>Clip&#39;s</code> <code>setFramePosition</code> or <code>setMicroSecondPosition</code> methods. Otherwise, it will start at the beginning. You can also configure the playback to cycle repeatedly, using the <code>setLoopPoints</code> method.</p><p><a name="113628" id="113628"></a> When you’re ready to start playback, simply invoke the <code>start</code> method. To stop or pause the clip, invoke the <code>stop</code> method, and to resume playback, invoke <code>start</code> again. The clip remembers the media position where it stopped playback, so there’s no need for explicit pause and resume methods. If you don’t want it to resume where it left off, you can “rewind” the clip to the beginning (or to any other position, for that matter) using the frame- or microsecond-positioning methods mentioned above.</p><p><a name="113630" id="113630"></a> A <code>Clip&#39;s</code> volume level and activity status (active versus inactive) can be monitored by invoking the <code>DataLine</code> methods <code>getLevel</code> and <code>isActive</code>, respectively. An active <code>Clip</code> is one that is currently playing sound.</p><p><a name="113634" id="113634"></a></p><h2 id="Using-a-SourceDataLine"><a href="#Using-a-SourceDataLine" class="headerlink" title="Using a SourceDataLine"></a>Using a SourceDataLine</h2><p><a name="116360" id="116360"></a> Obtaining a <code>SourceDataLine</code> is similar to obtaining a <code>Clip</code>. <a name="116365" id="116365"></a> Opening the <code>SourceDataLine</code> is also similar to opening a <code>Clip</code>, in that the purpose is once again to reserve the line. However, you use a different method, inherited from <code>DataLine</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void open(AudioFormat format)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Notice that when you open a <code>SourceDataLine</code>, you don’t associate any sound data with the line yet, unlike opening a <code>Clip</code>. Instead, you just specify the format of the audio data you want to play. The system chooses a default buffer length.</p><p><a name="113646" id="113646"></a> You can also stipulate a certain buffer length in bytes, using this variant:</p><p><a name="113648" id="113648"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void open(AudioFormat format, int bufferSize)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>For consistency with similar methods, the <code>bufferSize</code> argument is expressed in bytes, but it must correspond to an integral number of frames.</p><p><a name="113660" id="113660"></a> Instead of using the open method described above, it’s also possible to open a <code>SourceDataLine</code> using <code>Line&#39;s</code> <code>open()</code> method, without arguments. In this case, the line is opened with its default audio format and buffer size. However, you can’t change these later. If you want to know the line’s default audio format and buffer size, you can invoke <code>DataLine&#39;s</code> <code>getFormat</code> and <code>getBufferSize</code> methods, even before the line has ever been opened.</p><p><a name="113663" id="113663"></a> Once the <code>SourceDataLine</code> is open, you can start playing sound. You do this by invoking <code>DataLine&#39;s</code> start method, and then writing data repeatedly to the line’s playback buffer.</p><p><a name="113667" id="113667"></a> The start method permits the line to begin playing sound as soon as there’s any data in its buffer. You place data in the buffer by the following method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int write(byte[] b, int offset, int length)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The offset into the array is expressed in bytes, as is the array’s length.</p><p><a href="#113711">Monitoring a Line’s Status</a>. The line is now considered active, so the <code>isActive</code> method of <code>DataLine</code> will return <code>true</code>. Notice that all this happens only once the buffer contains data to play, not necessarily right when the start method is invoked. If you invoked <code>start</code> on a new <code>SourceDataLine</code> but never wrote data to the buffer, the line would never be active and a <code>START</code> event would never be sent. (However, in this case, the <code>isRunning</code> method of <code>DataLine</code> would return <code>true</code>.)</p><p><a name="113675" id="113675"></a> So how do you know how much data to write to the buffer, and when to send the second batch of data? Fortunately, you don’t need to time the second invocation of write to synchronize with the end of the first buffer! Instead, you can take advantage of the <code>write</code> method’s blocking behavior:</p><ul><li><a name="113676" id="113676"></a>The method returns as soon as the data has been written to the buffer. It doesn’t wait until all the data in the buffer has finished playing. (If it did, you might not have time to write the next buffer without creating a discontinuity in the audio.)</li><li><a name="113677" id="113677"></a>It’s all right to try to write more data than the buffer will hold. In this case, the method blocks (doesn’t return) until all the data you requested has actually been placed in the buffer. In other words, one buffer’s worth of your data at a time will be written to the buffer and played, until the remaining data all fits in the buffer, at which point the method returns. Whether or not the method blocks, it returns as soon as the last buffer’s worth of data from this invocation can be written. Again, this means that your code will in all likelihood regain control before playback of the last buffer’s worth of data has finished.</li><li><a name="113678" id="113678"></a>While in many contexts it is fine to write more data than the buffer will hold, if you want to be certain that the next write issued does not block, you can limit the number of bytes you write to the number that <code>DataLine&#39;s</code> <code>available</code> method returns.</li></ul><p><a name="113680" id="113680"></a> Here’s an example of iterating through chunks of data that are read from a stream, writing one chunk at a time to the <code>SourceDataLine</code> for playback:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#x2F;&#x2F; read chunks from a stream and write them to a source data </span><br><span class="line">line </span><br><span class="line">line.start();</span><br><span class="line">while (total &lt; totalToRead &amp;&amp; !stopped)&#125;</span><br><span class="line">    numBytesRead &#x3D; stream.read(myData, 0, numBytesToRead);</span><br><span class="line">    if (numBytesRead &#x3D;&#x3D; -1) break;</span><br><span class="line">    total +&#x3D; numBytesRead; </span><br><span class="line">    line.write(myData, 0, numBytesRead);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If you don’t want the <code>write</code> method to block, you can first invoke the <code>available</code> method (inside the loop) to find out how many bytes can be written without blocking, and then limit the <code>numBytesToRead</code> variable to this number, before reading from the stream. In the example given, though, blocking won’t matter much, since the write method is invoked inside a loop that won’t complete until the last buffer is written in the final loop iteration. Whether or not you use the blocking technique, you’ll probably want to invoke this playback loop in a separate thread from the rest of the application program, so that your program doesn’t appear to freeze when playing a long sound. On each iteration of the loop, you can test whether the user has requested playback to stop. Such a request needs to set the <code>stopped</code> boolean, used in the code above, to <code>true</code>.</p><p><a name="113696" id="113696"></a> Since <code>write</code> returns before all the data has finished playing, how do you learn when the playback has actually completed? One way is to invoke the <code>drain</code> method of <code>DataLine</code> after writing the last buffer’s worth of data. This method blocks until all the data has been played. When control returns to your program, you can free up the line, if desired, without fear of prematurely cutting off the playback of any audio samples:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">line.write(b, offset, numBytesToWrite); </span><br><span class="line">&#x2F;&#x2F;this is the final invocation of write</span><br><span class="line">line.drain();</span><br><span class="line">line.stop();</span><br><span class="line">line.close();</span><br><span class="line">line &#x3D; null;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>You can intentionally stop playback prematurely, of course. For example, the application program might provide the user with a Stop button. Invoke <code>DataLine&#39;s stop</code> method to stop playback immediately, even in the middle of a buffer. This leaves any unplayed data in the buffer, so that if you subsequently invoke <code>start</code>, the playback resumes where it left off. If that’s not what you want to happen, you can discard the data left in the buffer by invoking <code>flush</code>.</p><p><a name="113708" id="113708"></a> A <code>SourceDataLine</code> generates a <code>STOP</code> event whenever the flow of data has been stopped, whether this stoppage was initiated by the drain method, the stop method, or the flush method, or because the end of a playback buffer was reached before the application program invoked <code>write</code> again to provide new data. A <code>STOP</code> event doesn’t necessarily mean that the <code>stop</code> method was invoked, and it doesn’t necessarily mean that a subsequent invocation of <code>isRunning</code> will return <code>false</code>. It does, however, mean that <code>isActive</code> will return <code>false</code>. (When the <code>start</code> method has been invoked, the <code>isRunning</code> method will return <code>true</code>, even if a <code>STOP</code> event is generated, and it will begin to return <code>false</code> only once the <code>stop</code> method is invoked.) It’s important to realize that <code>START</code> and <code>STOP</code> events correspond to <code>isActive</code>, not to <code>isRunning</code>.</p><p><a name="113711" id="113711"></a></p><h2 id="Monitoring-a-Line’s-Status"><a href="#Monitoring-a-Line’s-Status" class="headerlink" title="Monitoring a Line’s Status"></a>Monitoring a Line’s Status</h2><p><a name="113713" id="113713"></a> Once you have started a sound playing, how do you find when it’s finished? We saw one solution above, invoking the <code>drain</code> method after writing the last buffer of data, but that approach is applicable only to a <code>SourceDataLine</code>. Another approach, which works for both <code>SourceDataLines</code> and <code>Clips</code>, is to register to receive notifications from the line whenever the line changes its state. These notifications are generated in the form of <code>LineEvent</code> objects, of which there are four types: <code>OPEN</code>, <code>CLOSE</code>, <code>START</code>, and <code>STOP</code>.</p><p><a name="113715" id="113715"></a> Any object in your program that implements the <code>LineListener</code> interface can register to receive such notifications. To implement the <code>LineListener</code> interface, the object simply needs an update method that takes a <code>LineEvent</code> argument. To register this object as one of the line’s listeners, you invoke the following <code>Line</code> method:</p><p><a name="113717" id="113717"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public void addLineListener(LineListener listener)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Whenever the line opens, closes, starts, or stops, it sends an <code>update</code> message to all its listeners. Your object can query the <code>LineEvent</code> that it receives. First you might invoke <code>LineEvent.getLine</code> to make sure the line that stopped is the one you care about. In the case we’re discussing here, you want to know if the sound is finished, so you see whether the <code>LineEvent</code> is of type <code>STOP</code>. If it is, you might check the sound’s current position, which is also stored in the <code>LineEvent</code> object, and compare it to the sound’s length (if known) to see whether it reached the end and wasn’t stopped by some other means (such as the user’s clicking a Stop button, although you’d probably be able to determine that cause elsewhere in your code).</p><p><a name="113721" id="113721"></a> Along the same lines, if you need to know when the line is opened, closed, or started, you use the same mechanism. <code>LineEvents</code> are generated by different kinds of lines, not just <code>Clips</code> and <code>SourceDataLines</code>. However, in the case of a <code>Port</code> you can’t count on getting an event to learn about a line’s open or closed state. For example, a <code>Port</code> might be initially open when it’s created, so you don’t invoke the <code>open</code> method and the <code>Port</code> doesn’t ever generate an <code>OPEN</code> event. (See the previous discussion of<br><a href="accessing.html#113216">Selecting Input and Output Ports</a>.)</p><p><a name="113725" id="113725"></a></p><h2 id="Synchronizing-Playback-on-Multiple-Lines"><a href="#Synchronizing-Playback-on-Multiple-Lines" class="headerlink" title="Synchronizing Playback on Multiple Lines"></a>Synchronizing Playback on Multiple Lines</h2><p><a name="113727" id="113727"></a> If you’re playing back multiple tracks of audio simultaneously, you probably want to have them all start and stop at exactly the same time. Some mixers facilitate this behavior with their <code>synchronize</code> method, which lets you apply operations such as <code>open</code>, <code>close</code>, <code>start</code>, and <code>stop</code> to a group of data lines using a single command, instead of having to control each line individually. Furthermore, the degree of accuracy with which operations are applied to the lines is controllable.</p><p><a name="113729" id="113729"></a> To find out whether a particular mixer offers this feature for a specified group of data lines, invoke the <code>Mixer</code> interface’s <code>isSynchronizationSupported</code> method:</p><p><a name="113731" id="113731"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">boolean isSynchronizationSupported(Line[] lines, boolean  maintainSync)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The first parameter specifies a group of specific data lines, and the second parameter indicates the accuracy with which synchronization must be maintained. If the second parameter is <code>true</code>, the query is asking whether the mixer is capable of maintaining sample-accurate precision in controlling the specified lines <strong>at all times</strong>; otherwise, precise synchronization is required only during start and stop operations, not throughout playback.</p><p><a name="113736" id="113736"></a></p><h2 id="Processing-the-Outgoing-Audio"><a href="#Processing-the-Outgoing-Audio" class="headerlink" title="Processing the Outgoing Audio"></a>Processing the Outgoing Audio</h2><p><a name="113738" id="113738"></a> Some source data lines have signal-processing controls, such as gain, pan, reverb, and sample-rate controls. Similar controls, especially gain controls, might be present on the output ports as well. For more information on how to determine whether a line has such controls, and how to use them if it does, see<br><a href="controls.html">Processing Audio with Controls</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Playing-Back-Audio&quot;&gt;&lt;a href=&quot;#Playing-Back-Audio&quot; class=&quot;headerlink&quot; title=&quot;Playing Back Audio&quot;&gt;&lt;/a&gt;Playing Back Audio&lt;/h1&gt;&lt;p&gt;Playba</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Overview of the Sampled Package</title>
    <link href="http://example.com/wiki/2021-03-04-Overview%20of%20the%20Sampled%20Package/"/>
    <id>http://example.com/wiki/2021-03-04-Overview%20of%20the%20Sampled%20Package/</id>
    <published>2021-03-04T14:32:16.175Z</published>
    <updated>2021-03-04T15:17:55.818Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview-of-the-Sampled-Package"><a href="#Overview-of-the-Sampled-Package" class="headerlink" title="Overview of the Sampled Package"></a>Overview of the Sampled Package</h1><p><a name="112330" id="112330"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/package-summary.html"><code>javax.sound.sampled</code></a> package is fundamentally concerned with audio transport &#8212; in other words, the Java Sound API focuses on playback and capture. The central task that the Java Sound API addresses is how to move bytes of formatted audio data into and out of the system. This task involves opening audio input and output devices and managing buffers that get filled with real-time sound data. It can also involve mixing multiple streams of audio into one stream (whether for input or output). The transport of sound into or out of the system has to be correctly handled when the user requests that the flow of sound be started, paused, resumed, or stopped.</p><p><a name="112332" id="112332"></a> To support this focus on basic audio input and output, the Java Sound API provides methods for converting between various audio data formats, and for reading and writing common types of sound files. However, it does not attempt to be a comprehensive sound-file toolkit. A particular implementation of the Java Sound API need not support an extensive set of file types or data format conversions. Third-party service providers can supply modules that “plug in” to an existing implementation to support additional file types and conversions.</p><p><a name="112334" id="112334"></a></p><p><a name="112336" id="112336"></a> The Java Sound API can handle audio transport in both a streaming, buffered fashion and an in-memory, unbuffered fashion. “Streaming” is used here in a general sense to refer to real-time handling of audio bytes; it does not refer to the specific, well-known case of sending audio over the Internet in a certain format. In other words, a stream of audio is simply a continuous set of audio bytes that arrive more or less at the same rate that they are to be handled (played, recorded, etc.). Operations on the bytes commence before all the data has arrived. In the streaming model, particularly in the case of audio input rather than audio output, you do not necessarily know in advance how long the sound is and when it will finish arriving. You simply handle one buffer of audio data at a time, until the operation is halted. In the case of audio output (playback), you also need to buffer data if the sound you want to play is too large to fit in memory all at once. In other words, you deliver your audio bytes to the sound engine in chunks, and it takes care of playing each sample at the right time. Mechanisms are provided that make it easy to know how much data to deliver in each chunk.</p><p><a name="112339" id="112339"></a> The Java Sound API also permits unbuffered transport in the case of playback only, assuming you already have all the audio data at hand and it is not too large to fit in memory. In this situation, there is no need for the application program to buffer the audio, although the buffered, real-time approach is still available if desired. Instead, the entire sound can be preloaded at once into memory for subsequent playback. Because all the sound data is loaded in advance, playback can start immediately &#8212; for example, as soon as the user clicks a Start button. This can be an advantage compared to the buffered model, where the playback has to wait for the first buffer to fill. In addition, the in-memory, unbuffered model allows sounds to be easily looped (cycled) or set to arbitrary positions in the data.</p><p>To play or capture sound using the Java Sound API, you need at least three things: formatted audio data, a mixer, and a line. The following provides an overview of these concepts.</p><h2 id="What-is-Formatted-Audio-Data"><a href="#What-is-Formatted-Audio-Data" class="headerlink" title="What is Formatted Audio Data?"></a><a name="formatted" id="formatted">What is Formatted Audio Data?</a></h2><p>Formatted audio data refers to sound in any of a number of standard formats. The Java Sound API distinguishes between <strong>data formats</strong> and <strong>file formats</strong>. <a name="112352" id="112352"></a></p><h3 id="Data-Formats"><a href="#Data-Formats" class="headerlink" title="Data Formats"></a>Data Formats</h3><p><a name="112354" id="112354"></a> A data format tells you how to interpret a series of bytes of “raw” sampled audio data, such as samples that have already been read from a sound file, or samples that have been captured from the microphone input. You might need to know, for example, how many bits constitute one sample (the representation of the shortest instant of sound), and similarly you might need to know the sound’s sample rate (how fast the samples are supposed to follow one another). When setting up for playback or capture, you specify the data format of the sound you are capturing or playing.</p><p><a name="112356" id="112356"></a> In the Java Sound API, a data format is represented by an<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioFormat.html"><code>AudioFormat</code></a> object, which includes the following attributes:</p><ul><li><a name="112358" id="112358"></a>Encoding technique, usually pulse code modulation (PCM)</li><li><a name="112359" id="112359"></a>Number of channels (1 for mono, 2 for stereo, etc.)</li><li><a name="112360" id="112360"></a>Sample rate (number of samples per second, per channel)</li><li><a name="112361" id="112361"></a>Number of bits per sample (per channel)</li><li><a name="112362" id="112362"></a>Frame rate</li><li><a name="112363" id="112363"></a>Frame size in bytes</li><li><a name="112364" id="112364"></a>Byte order (big-endian or little-endian)</li></ul><p><a name="114719" id="114719"></a> PCM is one kind of encoding of the sound waveform. The Java Sound API includes two PCM encodings that use linear quantization of amplitude, and signed or unsigned integer values. Linear quantization means that the number stored in each sample is directly proportional (except for any distortion) to the original sound pressure at that instant&#226;&#128;&#148;and similarly proportional to the displacement of a loudspeaker or eardrum that is vibrating with the sound at that instant. Compact discs, for example, use linear PCM-encoded sound. Mu-law encoding and a-law encoding are common nonlinear encodings that provide a more compressed version of the audio data; these encodings are typically used for telephony or recordings of speech. A nonlinear encoding maps the original sound’s amplitude to the stored value using a nonlinear function, which can be designed to give more amplitude resolution to quiet sounds than to loud sounds.</p><p><a name="112368" id="112368"></a> A frame contains the data for all channels at a particular time. For PCM-encoded data, the frame is simply the set of simultaneous samples in all channels, for a given instant in time, without any additional information. In this case, the frame rate is equal to the sample rate, and the frame size in bytes is the number of channels multiplied by the sample size in bits, divided by the number of bits in a byte.</p><p><a name="112370" id="112370"></a> For other kinds of encodings, a frame might contain additional information besides the samples, and the frame rate might be completely different from the sample rate. For example, consider the MP3 (MPEG-1 Audio Layer 3) encoding, which is not explicitly mentioned in the current version of the Java Sound API, but which could be supported by an implementation of the Java Sound API or by a third-party service provider. In MP3, each frame contains a bundle of compressed data for a series of samples, not just one sample per channel. Because each frame encapsulates a whole series of samples, the frame rate is slower than the sample rate. The frame also contains a header. Despite the header, the frame size in bytes is less than the size in bytes of the equivalent number of PCM frames. (After all, the purpose of MP3 is to be more compact than PCM data.) For such an encoding, the sample rate and sample size refer to the PCM data that the encoded sound will eventually be converted into before being delivered to a digital-to-analog converter (DAC).</p><p><a name="112373" id="112373"></a></p><h3 id="File-Formats"><a href="#File-Formats" class="headerlink" title="File Formats"></a>File Formats</h3><p><a name="112375" id="112375"></a> A file format specifies the structure of a sound file, including not only the format of the raw audio data in the file, but also other information that can be stored in the file. Sound files come in various standard varieties, such as WAVE (also known as WAV, and often associated with PCs), AIFF (often associated with Macintoshes), and AU (often associated with UNIX systems). The different types of sound file have different structures. For example, they might have a different arrangement of data in the file’s “header.” A header contains descriptive information that typically precedes the file’s actual audio samples, although some file formats allow successive “chunks” of descriptive and audio data. The header includes a specification of the data format that was used for storing the audio in the sound file. Any of these types of sound file can contain various data formats (although usually there is only one data format within a given file), and the same data format can be used in files that have different file formats.</p><p><a name="112377" id="112377"></a> In the Java Sound API, a file format is represented by an<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioFileFormat.html"><code>AudioFileFormat</code></a> object, which contains:</p><ul><li><a name="112379" id="112379"></a>The file type (WAVE, AIFF, etc.)</li><li><a name="112380" id="112380"></a>The file’s length in bytes</li><li><a name="112381" id="112381"></a>The length, in frames, of the audio data contained in the file</li><li><a name="112382" id="112382"></a>An AudioFormat object that specifies the data format of the audio data contained in the file</li></ul><p><a name="112383" id="112383"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioSystem.html"><code>AudioSystem</code></a> class provides methods for reading and writing sounds in different file formats, and for converting between different data formats. Some of the methods let you access a file’s contents through a kind of stream called an<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioInputStream.html"><code>AudioInputStream</code></a> . An <code>AudioInputStream</code> is a subclass of the<br><a href="https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html"><code>InputStream</code></a> class, which encapsulates a series of bytes that can be read sequentially. To its superclass, the <code>AudioInputStream</code> class adds knowledge of the bytes’ audio data format (represented by an <code>AudioFormat</code> object). By reading a sound file as an <code>AudioInputStream</code>, you get immediate access to the samples, without having to worry about the sound file’s structure (its header, chunks, etc.). A single method invocation gives you all the information you need about the data format and the file type.</p><p><a name="112387" id="112387"></a></p><h2 id="What-is-a-Mixer"><a href="#What-is-a-Mixer" class="headerlink" title="What is a Mixer?"></a>What is a Mixer?</h2><p><a name="112389" id="112389"></a> Many application programming interfaces (APIs) for sound make use of the notion of an audio <strong>device</strong>. A device is often a software interface to a physical input/output device. For example, a sound-input device might represent the input capabilities of a sound card, including a microphone input, a line-level analog input, and perhaps a digital audio input.</p><p><a name="112391" id="112391"></a> In the Java Sound API, devices are represented by<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Mixer.html"><code>Mixer</code></a> objects. The purpose of a mixer is to handle one or more streams of audio input and one or more streams of audio output. In the typical case, it actually mixes together multiple incoming streams into one outgoing stream. A <code>Mixer</code> object can represent the sound-mixing capabilities of a physical device such as a sound card, which might need to mix the sound coming in to the computer from various inputs, or the sound coming from application programs and going to outputs.</p><p><a name="113681" id="113681"></a> Alternatively, a <code>Mixer</code> object can represent sound-mixing capabilities that are implemented entirely in software, without any inherent interface to physical devices.</p><p><a name="112393" id="112393"></a> In the Java Sound API, a component such as the microphone input on a sound card is not itself considered a device &#8212; that is, a mixer &#8212; but rather a <strong>port</strong> into or out of the mixer. A port typically provides a single stream of audio into or out of the mixer (although the stream can be multichannel, such as stereo). The mixer might have several such ports. For example, a mixer representing a sound card’s output capabilities might mix several streams of audio together, and then send the mixed signal to any or all of various output ports connected to the mixer. These output ports could be (for example) a headphone jack, a built-in speaker, or a line-level output.</p><p><a name="112395" id="112395"></a> To understand the notion of a mixer in the Java Sound API, it helps to visualize a physical mixing console, such as those used in live concerts and recording studios.</p><p><a name="112397" id="112397"></a></p><p>A Physical Mixing Console</p><p>A physical mixer has “strips” (also called “slices”), each representing a path through which a single audio signal goes into the mixer for processing. The strip has knobs and other controls by which you can control the volume and pan (placement in the stereo image) for the signal in that strip. Also, the mixer might have a separate bus for effects such as reverb, and this bus can be connected to an internal or external reverberation unit. Each strip has a potentiometer that controls how much of that strip’s signal goes into the reverberated mix. The reverberated (“wet”) mix is then mixed with the “dry” signals from the strips. A physical mixer sends this final mixture to an output bus, which typically goes to a tape recorder (or disk-based recording system) and/or speakers.</p><p><a name="112401" id="112401"></a> Imagine a live concert that is being recorded in stereo. Cables (or wireless connections) coming from the many microphones and electric instruments on stage are plugged into the inputs of the mixing console. Each input goes to a separate strip of the mixer, as illustrated. The sound engineer decides on the settings of the gain, pan, and reverb controls. The output of all the strips and the reverb unit are mixed together into two channels. These two channels go to two outputs on the mixer, into which cables are plugged that connect to the stereo tape recorder’s inputs. The two channels are perhaps also sent via an amplifier to speakers in the hall, depending on the type of music and the size of the hall.</p><p><a name="112403" id="112403"></a> Now imagine a recording studio, in which each instrument or singer is recorded to a separate track of a multitrack tape recorder. After the instruments and singers have all been recorded, the recording engineer performs a “mixdown” to combine all the taped tracks into a two-channel (stereo) recording that can be distributed on compact discs. In this case, the input to each of the mixer’s strips is not a microphone, but one track of the multitrack recording. Once again, the engineer can use controls on the strips to decide each track’s volume, pan, and reverb amount. The mixer’s outputs go once again to a stereo recorder and to stereo speakers, as in the example of the live concert.</p><p><a name="112817" id="112817"></a> These two examples illustrate two different uses of a mixer: to capture multiple input channels, combine them into fewer tracks, and save the mixture, or to play back multiple tracks while mixing them down to fewer tracks.</p><p><a name="112407" id="112407"></a> In the Java Sound API, a mixer can similarly be used for input (capturing audio) or output (playing back audio). In the case of input, the <strong>source</strong> from which the mixer gets audio for mixing is one or more input ports. The mixer sends the captured and mixed audio streams to its <strong>target</strong>, which is an object with a buffer from which an application program can retrieve this mixed audio data. In the case of audio output, the situation is reversed. The mixer’s source for audio is one or more objects containing buffers into which one or more application programs write their sound data; and the mixer’s target is one or more output ports.</p><p><a name="113943" id="113943"></a></p><h2 id="What-is-a-Line"><a href="#What-is-a-Line" class="headerlink" title="What is a Line?"></a>What is a Line?</h2><p><a name="113931" id="113931"></a> The metaphor of a physical mixing console is also useful for understanding the Java Sound API’s concept of a <strong>line</strong>.</p><p><a name="113932" id="113932"></a> A line is an element of the digital audio “pipeline” that is, a path for moving audio into or out of the system. Usually the line is a path into or out of a mixer (although technically the mixer itself is also a kind of line).</p><p><a name="112416" id="112416"></a> Audio input and output ports are lines. These are analogous to the microphones and speakers connected to a physical mixing console. Another kind of line is a data path through which an application program can get input audio from, or send output audio to, a mixer. These data paths are analogous to the tracks of the multitrack recorder connected to the physical mixing console.</p><p><a name="112418" id="112418"></a> One difference between lines in the Java Sound API and those of a physical mixer is that the audio data flowing through a line in the Java Sound API can be mono or multichannel (for example, stereo). By contrast, each of a physical mixer’s inputs and outputs is typically a single channel of sound. To get two or more channels of output from the physical mixer, two or more physical outputs are normally used (at least in the case of analog sound; a digital output jack is often multichannel). In the Java Sound API, the number of channels in a line is specified by the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/AudioFormat.html"><code>AudioFormat</code></a> of the data that is currently flowing through the line.</p><p><a name="112421" id="112421"></a></p><p><a name="112423" id="112423"></a> Let’s now examine some specific kinds of lines and mixers. The following diagram shows different types of lines in a simple audio-output system that could be part of an implementation of the Java Sound API:</p><p>A Possible Configuration of Lines for Audio Output</p><p>In this example, an application program has gotten access to some available inputs of an audio-input mixer: one or more <strong>clips</strong> and <strong>source data lines</strong>. A clip is a mixer input (a kind of line) into which you can load audio data prior to playback; a source data line is a mixer input that accepts a real-time stream of audio data. The application program preloads audio data from a sound file into the clips. It then pushes other audio data into the source data lines, a buffer at a time. The mixer reads data from all these lines, each of which may have its own reverberation, gain, and pan controls, and mixes the dry audio signals with the wet (reverberated) mix. The mixer delivers its final output to one or more output ports, such as a speaker, a headphone jack, and a line-out jack.</p><p><a name="112431" id="112431"></a> Although the various lines are depicted as separate rectangles in the diagram, they are all “owned” by the mixer, and can be considered integral parts of the mixer. The reverb, gain, and pan rectangles represent processing controls (rather than lines) that can be applied by the mixer to data flowing through the lines.</p><p><a name="112433" id="112433"></a> Note that this is just one example of a possible mixer that is supported by the API. Not all audio configurations will have all the features illustrated. An individual source data line might not support panning, a mixer might not implement reverb, and so on.</p><p><a name="112435" id="112435"></a></p><p><a name="112437" id="112437"></a> A simple audio-input system might be similar:</p><p>A Possible Configuration of Lines for Audio Input</p><p>Here, data flows into the mixer from one or more input ports, commonly the microphone or the line-in jack. Gain and pan are applied, and the mixer delivers the captured data to an application program via the mixer’s target data line. A target data line is a mixer output, containing the mixture of the streamed input sounds. The simplest mixer has just one target data line, but some mixers can deliver captured data to multiple target data lines simultaneously. <a name="112445" id="112445"></a></p><h3 id="The-Line-Interface-Hierarchy"><a href="#The-Line-Interface-Hierarchy" class="headerlink" title="The Line Interface Hierarchy"></a><a name="lineHierarchy" id="lineHierarchy">The Line Interface Hierarchy</a></h3><p><a name="112447" id="112447"></a> Now that we’ve seen some functional pictures of what lines and mixers are, let’s discuss them from a slightly more programmatic perspective. Several types of line are defined by subinterfaces of the basic<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Line.html"><code>Line</code></a> interface. The interface hierarchy is shown below.</p><p>The Line Interface Hierarchy</p><p><a name="112452" id="112452"></a> The base interface,<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Line.html"><code>Line</code></a>, describes the minimal functionality common to all lines:</p><ul><li><a name="112454" id="112454"></a>Controls &#8211; Data lines and ports often have a set of controls that affect the audio signal passing through the line. The Java Sound API specifies control classes that can be used to manipulate aspects of sound such as: gain (which affects the signal’s volume in decibels), pan (which affects the sound’s right-left positioning, reverb (which adds reverberation to the sound to emulate different kinds of room acoustics), and sample rate (which affects the rate of playback as well as the sound’s pitch).</li><li><a name="112458" id="112458"></a>Open or closed status &#8211; Successful opening of a line guarantees that resources have been allocated to the line. A mixer has a finite number of lines, so at some point multiple application programs (or the same one) might vie for usage of the mixer’s lines. Closing a line indicates that any resources used by the line may now be released.<li><a name="112462" id="112462"></a>Events &#8211; A line generates events when it opens or closes. Subinterfaces of `Line` can introduce other types of events. When a line generates an event, the event is sent to all objects that have registered to "listen" for events on that line. An application program can create these objects, register them to listen for line events, and react to the events as desired.<a name="112466" id="112466"></a></li></li></ul><p>We’ll now examine the subinterfaces of the <code>Line</code> interface.</p><p><a name="112468" id="112468"></a><br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Port.html"><code>Ports</code></a> are simple lines for input or output of audio to or from audio devices. As mentioned earlier, some common types of ports are the microphone, line input, CD-ROM drive, speaker, headphone, and line output.</p><p><a name="112470" id="112470"></a> The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Mixer.html"><code>Mixer</code></a> interface represents a mixer, of course, which as we have seen represents either a hardware or a software device. The <code>Mixer</code> interface provides methods for obtaining a mixer’s lines. These include source lines, which feed audio to the mixer, and target lines, to which the mixer delivers its mixed audio. For an audio-input mixer, the source lines are input ports such as the microphone input, and the target lines are<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/TargetDataLine.html"><code>TargetDataLines</code></a> (described below), which deliver audio to the application program. For an audio-output mixer, on the other hand, the source lines are<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/Clip.html"><code>Clips</code></a> or<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/SourceDataLine.html"><code>SourceDataLines</code></a> (described below), to which the application program feeds audio data, and the target lines are output ports such as the speaker.</p><p><a name="112472" id="112472"></a> A <code>Mixer</code> is defined as having one or more source lines and one or more target lines. Note that this definition means that a mixer need not actually mix data; it might have only a single source line. The <code>Mixer</code> API is intended to encompass a variety of devices, but the typical case supports mixing.</p><p><a name="112474" id="112474"></a> The <code>Mixer</code> interface supports synchronization; that is, you can specify that two or more of a mixer’s lines be treated as a synchronized group. Then you can start, stop, or close all those data lines by sending a single message to any line in the group, instead of having to control each line individually. With a mixer that supports this feature, you can obtain sample-accurate synchronization between lines.</p><p><a name="112476" id="112476"></a> The generic <code>Line</code> interface does not provide a means to start and stop playback or recording. For that you need a data line. The<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/sampled/DataLine.html"><code>DataLine</code></a> interface supplies the following additional media-related features beyond those of a <code>Line</code>:</p><ul><li><p>Audio format &#8211; Each data line has an audio format associated with its data stream.</p></li><li><p>Media position &#8211; A data line can report its current position in the media, expressed in sample frames. This represents the number of sample frames captured by or rendered from the data line since it was opened.</p></li><li><p>Buffer size &#8211; This is the size of the data line’s internal buffer in bytes. For a source data line, the internal buffer is one to which data can be written, and for a target data line it’s one from which data can be read.</p></li><li><p>Level (the current amplitude of the audio signal)</p></li><li><p>Start and stop playback or capture</p></li><li><p>Pause and resume playback or capture</p></li><li><p>Flush (discard unprocessed data from the queue)</p></li><li><p>Drain (block until all unprocessed data has been drained from the queue, and the data line’s buffer has become empty)</p></li><li><p>Active status &#8211; A data line is considered active if it is engaged in active presentation or capture of audio data to or from a mixer.</p></li><li><p>Events &#8211; <code>START</code> and <code>STOP</code> events are produced when active presentation or capture of data from or to the data line starts or stops.</p></li></ul><p>A <code>TargetDataLine</code> receives audio data from a mixer. Commonly, the mixer has captured audio data from a port such as a microphone; it might process or mix this captured audio before placing the data in the target data line’s buffer. The <code>TargetDataLine</code> interface provides methods for reading the data from the target data line’s buffer and for determining how much data is currently available for reading.</p><p><a name="112501" id="112501"></a> A <code>SourceDataLine</code> receives audio data for playback. It provides methods for writing data to the source data line’s buffer for playback, and for determining how much data the line is prepared to receive without blocking.</p><p><a name="112503" id="112503"></a> A <code>Clip</code> is a data line into which audio data can be loaded prior to playback. Because the data is pre-loaded rather than streamed, the clip’s duration is known before playback, and you can choose any starting position in the media. Clips can be looped, meaning that upon playback, all the data between two specified loop points will repeat a specified number of times, or indefinitely.</p><p><a name="112505" id="112505"></a> This section has introduced most of the important interfaces and classes of the sampled-audio API. Subsequent sections show how you can access and use these objects in your application program.</p><p>&#160;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Overview-of-the-Sampled-Package&quot;&gt;&lt;a href=&quot;#Overview-of-the-Sampled-Package&quot; class=&quot;headerlink&quot; title=&quot;Overview of the Sampled Packag</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Overview of the MIDI Package</title>
    <link href="http://example.com/wiki/2021-03-04-Overview%20of%20the%20MIDI%20Package/"/>
    <id>http://example.com/wiki/2021-03-04-Overview%20of%20the%20MIDI%20Package/</id>
    <published>2021-03-04T14:32:16.120Z</published>
    <updated>2021-03-04T15:17:55.814Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overview-of-the-MIDI-Package"><a href="#Overview-of-the-MIDI-Package" class="headerlink" title="Overview of the MIDI Package"></a>Overview of the MIDI Package</h1><p><a name="118771" id="118771"></a> The<br><a href="index.html">introduction</a> gave a glimpse into the MIDI capabilities of the Java Sound API. The discussion that follows provides a more detailed introduction to the Java Sound API’s MIDI architecture, which is accessed through the <code>javax.sound.midi</code> package. Some basic features of MIDI itself are explained, as a refresher or introduction, to place the Java Sound API’s MIDI features in context. It then goes on to discuss the Java Sound API’s approach to MIDI, as a preparation for the programming tasks that are explained in subsequent sections. The following discussion of the MIDI API is divided into two main areas: data and devices.</p><h2 id="A-MIDI-Refresher-Wires-and-Files"><a href="#A-MIDI-Refresher-Wires-and-Files" class="headerlink" title="A MIDI Refresher: Wires and Files"></a>A MIDI Refresher: Wires and Files</h2><p><a name="118776" id="118776"></a> The Musical Instrument Digital Interface (MIDI) standard defines a communication protocol for electronic music devices, such as electronic keyboard instruments and personal computers. MIDI data can be transmitted over special cables during a live performance, and can also be stored in a standard type of file for later playback or editing.</p><p><a name="118778" id="118778"></a> This section reviews some MIDI basics, without reference to the Java Sound API. The discussion is intended as a refresher for readers acquainted with MIDI, and as a brief introduction for those who are not, to provide background for the subsequent discussion of the Java Sound API’s MIDI package. If you have a thorough understanding of MIDI, you can safely skip this section. Before writing substantial MIDI applications, programmers who are unfamiliar with MIDI will probably need a fuller description of MIDI than can be included in this tutorial. See the Complete MIDI 1.0 Detailed Specification, which is available only in hard copy from<br><a href="http://www.midi.org/">http://www.midi.org</a> (although you might find paraphrased or summarized versions on the Web).</p><p><a name="118780" id="118780"></a> MIDI is both a hardware specification and a software specification. To understand MIDI’s design, it helps to understand its history. MIDI was originally designed for passing musical events, such as key depressions, between electronic keyboard instruments such as synthesizers. Hardware devices known as sequencers stored sequences of notes that could control a synthesizer, allowing musical performances to be recorded and subsequently played back. Later, hardware interfaces were developed that connected MIDI instruments to a computer’s serial port, allowing sequencers to be implemented in software. More recently, computer sound cards have incorporated hardware for MIDI I/O and for synthesizing musical sound. Today, many users of MIDI deal only with sound cards, never connecting to external MIDI devices. CPUs have become fast enough that synthesizers, too, can be implemented in software. A sound card is needed only for audio I/O and, in some applications, for communicating with external MIDI devices.</p><p><a name="119058" id="119058"></a> The brief hardware portion of the MIDI specification prescribes the pinouts for MIDI cables and the jacks into which these cables are plugged. This portion need not concern us. Because devices that originally required hardware, such as sequencers and synthesizers, are now implementable in software, perhaps the only reason for most programmers to know anything about MIDI hardware devices is simply to understand the metaphors in MIDI. However, external MIDI hardware devices are still essential for some important music applications, and so the Java Sound API supports input and output of MIDI data.</p><p><a name="118784" id="118784"></a> The software portion of the MIDI specification is extensive. This portion concerns the structure of MIDI data and how devices such as synthesizers should respond to that data. It is important to understand that MIDI data can be <strong>streamed</strong> or <strong>sequenced</strong>. This duality reflects two different parts of the Complete MIDI 1.0 Detailed Specification:</p><ul><li><a name="118786" id="118786"></a>MIDI 1.0</li><li><a name="118787" id="118787"></a>Standard MIDI Files</li></ul><p><a name="118789" id="118789"></a>We’ll explain what’s meant by streaming and sequencing by examining the purpose of each of these two parts of the MIDI specification.</p><p><a name="118791" id="118791"></a></p><h2 id="Streaming-Data-in-the-MIDI-Wire-Protocol"><a href="#Streaming-Data-in-the-MIDI-Wire-Protocol" class="headerlink" title="Streaming Data in the MIDI Wire Protocol"></a>Streaming Data in the MIDI Wire Protocol</h2><p><a name="118793" id="118793"></a> The first of these two parts of the MIDI specification describes what is known informally as “MIDI wire protocol.” MIDI wire protocol, which is the original MIDI protocol, is based on the assumption that the MIDI data is being sent over a MIDI cable (the “wire”). The cable transmits digital data from one MIDI device to another. Each of the MIDI devices might be a musical instrument or a similar device, or it might be a general-purpose computer equipped with a MIDI-capable sound card or a MIDI-to-serial-port interface.</p><p><a name="118795" id="118795"></a> MIDI data, as defined by MIDI wire protocol, is organized into messages. The different kinds of message are distinguished by the first byte in the message, known as the <strong>status byte</strong>. (Status bytes are the only bytes that have the highest-order bit set to 1.) The bytes that follow the status byte in a message are known as <strong>data bytes</strong>. Certain MIDI messages, known as <strong>channel</strong> messages, have a status byte that contains four bits to specify the kind of channel message and another four bits to specify the channel number. There are therefore 16 MIDI channels; devices that receive MIDI messages can be set to respond to channel messages on all or only one of these virtual channels. Often each MIDI channel (which shouldn’t be confused with a channel of audio) is used to send the notes for a different instrument. As an example, two common channel messages are Note On and Note Off, which start a note sounding and then stop it, respectively. These two messages each take two data bytes: the first specifies the note’s pitch and the second its “velocity” (how fast the key is depressed or released, assuming a keyboard instrument is playing the note).</p><p><a name="118799" id="118799"></a> MIDI wire protocol defines a streaming model for MIDI data. A central feature of this protocol is that the bytes of MIDI data are delivered in real time&#226;&#128;&#148;in other words, they are streamed. The data itself contains no timing information; each event is processed as it’s received, and it’s assumed that it arrives at the correct time. That model is fine if the notes are being generated by a live musician, but it’s insufficient if you want to store the notes for later playback, or if you want to compose them out of real time. This limitation is understandable when you realize that MIDI was originally designed for musical performance, as a way for a keyboard musician to control more than one synthesizer, back in the days before many musicians used computers. (The first version of the specification was released in 1984.)</p><p><a name="119349" id="119349"></a></p><h2 id="Sequenced-Data-in-Standard-MIDI-Files"><a href="#Sequenced-Data-in-Standard-MIDI-Files" class="headerlink" title="Sequenced Data in Standard MIDI Files"></a>Sequenced Data in Standard MIDI Files</h2><p><a name="119350" id="119350"></a> The Standard MIDI Files part of the MIDI specification addresses the timing limitation in MIDI wire protocol. A standard MIDI file is a digital file that contains MIDI <strong>events</strong>. An event is simply a MIDI message, as defined in the MIDI wire protocol, but with an additional piece of information that specifies the event’s timing. (There are also some events that don’t correspond to MIDI wire protocol messages, as we’ll see in the next section.) The additional timing information is a series of bytes that indicates when to perform the operation described by the message. In other words, a standard MIDI file specifies not just which notes to play, but exactly when to play each of them. It’s a bit like a musical score.</p><p><a name="118806" id="118806"></a> The information in a standard MIDI file is referred to as a <strong>sequence</strong>. A standard MIDI file contains one or more <strong>tracks</strong>. Each track typically contains the notes that a single instrument would play if the music were performed by live musicians. A sequencer is a software or hardware device that can read a sequence and deliver the MIDI messages contained in it at the right time. A sequencer is a bit like an orchestra conductor: it has the information for all the notes, including their timings, and it tells some other entity when to perform the notes.</p><p><a name="118810" id="118810"></a></p><h2 id="The-Java-Sound-API’s-Representation-of-MIDI-Data"><a href="#The-Java-Sound-API’s-Representation-of-MIDI-Data" class="headerlink" title="The Java Sound API’s Representation of MIDI Data"></a>The Java Sound API’s Representation of MIDI Data</h2><p><a name="118812" id="118812"></a> Now that we’ve sketched the MIDI specification’s approach to streamed and sequenced musical data, let’s examine how the Java Sound API represents that data.</p><p><a name="119522" id="119522"></a></p><h3 id="MIDI-Messages"><a href="#MIDI-Messages" class="headerlink" title="MIDI Messages"></a>MIDI Messages</h3><p><a name="119523" id="119523"></a><br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiMessage.html"><code>MidiMessage</code></a> is an abstract class that represents a “raw” MIDI message. A “raw” MIDI message is usually a message defined by the MIDI wire protocol. It can also be one of the events defined by the Standard MIDI Files specification, but without the event’s timing information. There are three categories of raw MIDI message, represented in the Java Sound API by these three respective <code>MidiMessage</code> subclasses:</p><li><a name="118819" id="118819"></a> [`ShortMessages`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/ShortMessage.html) are the most common messages and have at most two data bytes following the status byte. The channel messages, such as Note On and Note Off, are all short messages, as are some other messages.</li><li><a name="118820" id="118820"></a> [`SysexMessages`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/SysexMessage.html) contain **system-exclusive** MIDI messages. They may have many bytes, and generally contain manufacturer-specific instructions.</li><li><a name="118821" id="118821"></a> [`MetaMessages`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MetaMessage.html) occur in MIDI files, but not in MIDI wire protocol. Meta messages contain data, such as lyrics or tempo settings, that might be useful to sequencers but that are usually meaningless for synthesizers.</li><p><a name="118823" id="118823"></a></p><h3 id="MIDI-Events"><a href="#MIDI-Events" class="headerlink" title="MIDI Events"></a>MIDI Events</h3><p><a name="119393" id="119393"></a> As we’ve seen, standard MIDI files contain events that are wrappers for “raw” MIDI messages along with timing information. An instance of the Java Sound API’s<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiEvent.html"><code>MidiEvent</code></a> class represents an event such as might be stored in a standard MIDI file.</p><p><a name="118827" id="118827"></a> The API for <code>MidiEvent</code> includes methods to set and get the event’s timing value. There’s also a method to retrieve its embedded raw MIDI message, which is an instance of a subclass of <code>MidiMessage</code>, discussed next. (The embedded raw MIDI message can be set only when constructing the <code>MidiEvent</code>.)</p><p><a name="118829" id="118829"></a></p><h3 id="Sequences-and-Tracks"><a href="#Sequences-and-Tracks" class="headerlink" title="Sequences and Tracks"></a>Sequences and Tracks</h3><p><a name="118831" id="118831"></a> As mentioned earlier, a standard MIDI file stores events that are arranged into tracks. Usually the file represents one musical composition, and usually each track represents a part such as might have been played by a single instrumentalist. Each note that the instrumentalist plays is represented by at least two events: a Note On that starts the note, and a Note Off that ends it. The track may also contain events that don’t correspond to notes, such as meta-events (which were mentioned above).</p><p><a name="118833" id="118833"></a> The Java Sound API organizes MIDI data in a three-part hierarchy:</p><li><a name="118834" id="118834"></a> [`Sequence`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Sequence.html)</li><li><a name="118835" id="118835"></a> [`Track`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Track.html)</li><li><a name="118836" id="118836"></a> [`MidiEvent`](https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiEvent.html)<a name="118837" id="118837"></a></li><p>A <code>Track</code> is a collection of <code>MidiEvents</code>, and a <code>Sequence</code> is a collection of <code>Tracks</code>. This hierarchy reflects the files, tracks, and events of the Standard MIDI Files specification. (Note: this is a hierarchy in terms of containment and ownership; it’s <strong>not</strong> a class hierarchy in terms of inheritance. Each of these three classes inherits directly from <code>java.lang.Object</code>.)</p><p><a name="118839" id="118839"></a> <code>Sequences</code> can be read from MIDI files, or created from scratch and edited by adding <code>Tracks</code> to the <code>Sequence</code> (or removing them). Similarly, <code>MidiEvents</code> can be added to or removed from the tracks in the sequence.</p><p><a name="118842" id="118842"></a></p><h2 id="The-Java-Sound-API’s-Representation-of-MIDI-Devices"><a href="#The-Java-Sound-API’s-Representation-of-MIDI-Devices" class="headerlink" title="The Java Sound API’s Representation of MIDI Devices"></a>The Java Sound API’s Representation of MIDI Devices</h2><p><a name="118844" id="118844"></a> The previous section explained how MIDI messages are represented in the Java Sound API. However, MIDI messages don’t exist in a vacuum. They’re typically sent from one device to another. A program that uses the Java Sound API can generate MIDI messages from scratch, but more often the messages are instead created by a software device, such as a sequencer, or received from outside the computer through a MIDI input port. Such a device usually sends these messages to another device, such as a synthesizer or a MIDI output port.</p><p><a name="118846" id="118846"></a></p><h3 id="The-MidiDevice-Interface"><a href="#The-MidiDevice-Interface" class="headerlink" title="The MidiDevice Interface"></a>The MidiDevice Interface</h3><p><a name="118848" id="118848"></a> In the world of external MIDI hardware devices, many devices can transmit MIDI messages to other devices and also receive messages from other devices. Similarly, in the Java Sound API, software objects that implement the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiDevice.html"><code>MidiDevice</code></a> interface can transmit and receive messages. Such an object can be implemented purely in software, or it can serve as an interface to hardware such as a sound card’s MIDI capabilities. The base <code>MidiDevice</code> interface provides all the functionality generally required by a MIDI input or output port. Synthesizers and sequencers, however, further implement one of the subinterfaces of <code>MidiDevice</code>:<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Synthesizer.html"><code>Synthesizer</code></a> or<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Sequencer.html"><code>Sequencer</code></a> , respectively.</p><p><a name="118850" id="118850"></a> The <code>MidiDevice</code> interface includes an API for opening and closing a device. It also includes an inner class called <code>MidiDevice.Info</code> that provides textual descriptions of the device, including its name, vendor, and version. If you’ve read the sampled-audio portion of this tutorial, this API will probably sound familiar, because its design is similar to that of the <code>javax.sampled.Mixer</code> interface, which represents an audio device and which has an analogous inner class, <code>Mixer.Info</code>.</p><p><a name="118852" id="118852"></a></p><h3 id="Transmitters-and-Receivers"><a href="#Transmitters-and-Receivers" class="headerlink" title="Transmitters and Receivers"></a>Transmitters and Receivers</h3><p><a name="118854" id="118854"></a> Most MIDI devices are capable of sending <code>MidiMessages</code>, receiving them, or both. The way a device sends data is via one or more transmitter objects that it “owns.” Similarly, the way a device receives data is via one or more of its receiver objects. The transmitter objects implement the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Transmitter.html"><code>Transmitter</code></a> interface, and the receivers implement the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Receiver.html"><code>Receiver</code></a> interface.</p><p><a name="118856" id="118856"></a> Each transmitter can be connected to only one receiver at a time, and vice versa. A device that sends its MIDI messages to multiple other devices simultaneously does so by having multiple transmitters, each connected to a receiver of a different device. Similarly, a device that can receive MIDI messages from more than one source at a time must do so via multiple receivers.</p><p><a name="118858" id="118858"></a></p><h3 id="Sequencers"><a href="#Sequencers" class="headerlink" title="Sequencers"></a>Sequencers</h3><p><a name="118860" id="118860"></a> A sequencer is a device for capturing and playing back sequences of MIDI events. It has transmitters, because it typically sends the MIDI messages stored in the sequence to another device, such as a synthesizer or MIDI output port. It also has receivers, because it can capture MIDI messages and store them in a sequence. To its superinterface, <code>MidiDevice</code>, <code>Sequencer</code> adds methods for basic MIDI sequencing operations. A sequencer can load a sequence from a MIDI file, query and set the sequence’s tempo, and synchronize other devices to it. An application program can register an object to be notified when the sequencer processes certain kinds of events.</p><p><a name="118862" id="118862"></a></p><h3 id="Synthesizers"><a href="#Synthesizers" class="headerlink" title="Synthesizers"></a>Synthesizers</h3><p><a name="119601" id="119601"></a> A <code>Synthesizer</code> is a device for generating sound. It’s the only object in the <code>javax.sound.midi</code> package that produces audio data. A synthesizer device controls a set of MIDI channel objects &#8212; typically 16 of them, since the MIDI specification calls for 16 MIDI channels. These MIDI channel objects are instances of a class that implements the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiChannel.html"><code>MidiChannel</code></a> interface, whose methods represent the MIDI specification’s “channel voice messages” and “channel mode messages.”</p><p><a name="118866" id="118866"></a> An application program can generate sound by directly invoking methods of a synthesizer’s MIDI channel objects. More commonly, though, a synthesizer generates sound in response to messages sent to one or more of its receivers. These messages might be sent by a sequencer or MIDI input port, for example. The synthesizer parses each message that its receivers get, and usually dispatches a corresponding command (such as <code>noteOn</code> or <code>controlChange</code>) to one of its <code>MidiChannel</code> objects, according to the MIDI channel number specified in the event.</p><p><a name="118868" id="118868"></a> The <code>MidiChannel</code> uses the note information in these messages to synthesize music. For example, a <code>noteOn</code> message specifies the note’s pitch and “velocity” (volume). However, the note information is insufficient; the synthesizer also requires precise instructions on how to create the audio signal for each note. These instructions are represented by an<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Instrument.html"><code>Instrument</code></a>. Each <code>Instrument</code> typically emulates a different real-world musical instrument or sound effect. The <code>Instruments</code> might come as presets with the synthesizer, or they might be loaded from soundbank files. In the synthesizer, the <code>Instruments</code> are arranged by bank number (these can be thought of as rows) and program number (columns).</p><p><a name="118870" id="118870"></a> This section has provided a background for understanding MIDI data, and it has introduced some of the important interfaces and classes related to MIDI in the Java Sound API. Subsequent sections show how you can access and use these objects in your application programs.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Overview-of-the-MIDI-Package&quot;&gt;&lt;a href=&quot;#Overview-of-the-MIDI-Package&quot; class=&quot;headerlink&quot; title=&quot;Overview of the MIDI Package&quot;&gt;&lt;/a&gt;Ov</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to the Service Provider Interfaces</title>
    <link href="http://example.com/wiki/2021-03-04-Introduction%20to%20the%20Service%20Provider%20Interfaces/"/>
    <id>http://example.com/wiki/2021-03-04-Introduction%20to%20the%20Service%20Provider%20Interfaces/</id>
    <published>2021-03-04T14:32:16.053Z</published>
    <updated>2021-03-04T15:17:55.808Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction-to-the-Service-Provider-Interfaces"><a href="#Introduction-to-the-Service-Provider-Interfaces" class="headerlink" title="Introduction to the Service Provider Interfaces"></a>Introduction to the Service Provider Interfaces</h1><h2 id="What-Are-Services"><a href="#What-Are-Services" class="headerlink" title="What Are Services?"></a><a name="what" id="what"></a>What Are Services?</h2><p>Services are units of sound-handling functionality that are automatically available when an application program makes use of an implementation of the Java Sound API. They consist of objects that do the work of reading, writing, mixing, processing, and converting audio and MIDI data. An implementation of the Java Sound API generally supplies a basic set of services, but mechanisms are also included in the API to support the development of new sound services by third-party developers (or by the vendor of the implementation itself). These new services can be “plugged into” an existing installed implementation to expand its functionality without requiring a new release. In the Java Sound API architecture, third-party services are integrated into the system in such a way that an application program’s interface to them is the same as the interface to the “built-in” services. In some cases, application developers who use the <code>javax.sound.sampled</code> and <code>javax.sound.midi</code> packages might not even be aware that they are employing third-party services.</p><p>Examples of potential third-party, sampled-audio services include:</p><ul><li>Sound file readers and writers</li><li>Converters that translate between different audio data formats</li><li>New audio mixers and input/output devices, whether implemented purely in software, or in hardware with a software interface</li></ul><p>Third-party MIDI services might consist of:</p><ul><li>MIDI file readers and writers</li><li>Readers for various types of soundbank files (which are often specific to particular synthesizers)</li><li>MIDI-controlled sound synthesizers, sequencers, and I/O ports, whether implemented purely in software, or in hardware with a software interface</li></ul><h2 id="How-Services-Work"><a href="#How-Services-Work" class="headerlink" title="How Services Work"></a><a name="how" id="how"></a>How Services Work</h2><p>The <code>javax.sound.sampled</code> and <code>javax.sound.midi</code> packages provide functionality to application developers who wish to include sound services in their application programs. These packages are for <strong>consumers</strong> of sound services, providing interfaces to get information about, control, and access audio and MIDI services. In addition, the Java Sound API also supplies two packages that define abstract classes to be used by <strong>providers</strong> of sound services: the <code>javax.sound.sampled.spi</code> and <code>javax.sound.midi.spi</code> packages.</p><p>Developers of new sound services implement concrete subclasses of the appropriate classes in the SPI packages. These subclasses, along with any additional classes required to support the new service, are placed in a Java Archive (JAR) archive file with a description of the included service or services. When this JAR file is installed in the user’s <code>CLASSPATH</code>, the runtime system automatically makes the new service available, extending the functionality of the Java platform’s runtime system.</p><p>Once the new service is installed, it can be accessed just like any previously installed service. Consumers of services can get information about the new service, or obtain instances of the new service class itself, by invoking methods of the <code>AudioSystem</code> and <code>MidiSystem</code> classes (in the <code>javax.sound.sampled</code> and <code>javax.sound.midi</code> packages, respectively) to return information about the new services, or to return instances of new or existing service classes themselves. Application programs need not&#226;&#128;&#148;and should not&#226;&#128;&#148;reference the classes in the SPI packages (and their subclasses) directly to make use of the installed services.</p><p>For example, suppose a hypothetical service provider called Acme Software, Inc. is interested in supplying a package that allows application programs to read a new format of sound file (but one whose audio data is in a standard data format). The SPI class <code>AudioFileReader</code> can be subclassed into a class called, say, <code>AcmeAudioFileReader</code>. In the new subclass, Acme would supply implementations of all the methods defined in <code>AudioFileReader</code>; in this case there are only two methods (with argument variants), <code>getAudioFileFormat</code> and <code>getAudioInputStream</code>. Then when an application program attempted to read a sound file that happened to be in Acme’s file format, it would invoke methods of the <code>AudioSystem</code> class in <code>javax.sound.sampled</code> to access the file and information about it. The methods <code>AudioSystem.getAudioInputStream</code> and <code>AudioSystem.getAudioFileFormat</code> provide a standard API to read audio streams; with the <code>AcmeAudioFileReader</code> class installed, this interface is extended to support the new file type transparently. Application developers don’t need direct access to the newly registered SPI classes: the <code>AudioSystem</code> object methods pass the query on to the installed <code>AcmeAudioFileReader</code> class.</p><p>What’s the point of having these “factory” classes? Why not permit the application developer to get access directly to newly provided services? That is a possible approach, but having all management and instantiation of services pass through gatekeeper system objects shields the application developer from having to know anything about the identity of installed services. Application developers just use services of value to them, perhaps without even realizing it. At the same time this architecture permits service providers to effectively manage the available resources in their packages.</p><p>Often the use of new sound services is transparent to the application program. For example, imagine a situation where an application developer wants to read in a stream of audio from a file. Assuming that <code>thePathName</code> identifies an audio input file, the program does this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">File theInFile &#x3D; new File(thePathName);</span><br><span class="line">AudioInputStream theInStream &#x3D; AudioSystem.getAudioInputStream(theInFile); </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Behind the scenes, the <code>AudioSystem</code> determines what installed service can read the file and asks it to supply the audio data as an <code>AudioInputStream</code> object. The developer might not know or even care that the input audio file is in some new file format (such as Acme’s), supported by installed third-party services. The program’s first contact with the stream is through the <code>AudioSystem</code> object, and all its subsequent access to the stream and its properties are through the methods of <code>AudioInputStream</code>. Both of these are standard objects in the <code>javax.sound.sampled</code> API; the special handling that the new file format may require is completely hidden.</p><h2 id="How-Providers-Prepare-New-Services"><a href="#How-Providers-Prepare-New-Services" class="headerlink" title="How Providers Prepare New Services"></a><a name="how_providers" id="how_providers"></a>How Providers Prepare New Services</h2><p>Service providers supply their new services in specially formatted JAR files, which are to be installed in a directory on the user’s system where the Java runtime will find them. JAR files are archive files, each containing sets of files that might be organized in hierarchical directory structures within the archive. Details about the preparation of the class files that go into these archives are discussed in the next few pages, which describe the specifics of the audio and MIDI SPI packages; here we’ll just give an overview of the process of JAR file creation.</p><p>The JAR file for a new service or services should contain a class file for each service supported in the JAR file. Following the Java platform’s convention, each class file has the name of the newly defined class, which is a concrete subclass of one of the abstract service provider classes. The JAR file also must include any supporting classes required by the new service implementation. So that the new service or services can be located by the runtime system’s service provider mechanism, the JAR file must also contain special files (described below) that map the SPI class names to the new subclasses being defined.</p><p>To continue from our example above, say Acme Software, Inc. is distributing a package of new sampled-audio services. Let’s suppose this package consists of two new services:</p><ul><li>The <code>AcmeAudioFileReader</code> class, which was mentioned above, and which is a subclass of <code>AudioFileReader</code></li><li>A subclass of <code>AudioFileWriter</code> called <code>AcmeAudioFileWriter</code>, which will write sound files in Acme’s new format</li></ul><p>Starting from an arbitrary directory&#226;&#128;&#148;let’s call it <code>/devel</code>&#226;&#128;&#148;where we want to do the build, we create subdirectories and put the new class files in them, organized in such a manner as to give the desired pathname by which the new classes will be referenced:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">com&#x2F;acme&#x2F;AcmeAudioFileReader.class</span><br><span class="line">com&#x2F;acme&#x2F;AcmeAudioFileWriter.class</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In addition, for each new SPI class being subclassed, we create a mapping file in a specially named directory <code>META-INF/services</code>. The name of the file is the name of the SPI class being subclassed, and the file contains the names of the new subclasses of that SPI abstract class.</p><p>We create the file</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">META-INF&#x2F;services&#x2F;javax.sound.sampled.spi.AudioFileReader</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>which consists of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Providers of sound file-reading services </span><br><span class="line"># (a comment line begins with a pound sign)</span><br><span class="line">com.acme.AcmeAudioFileReader</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>and also the file</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">META-INF&#x2F;services&#x2F;javax.sound.sampled.spi.AudioFileWriter</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>which consists of</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Providers of sound file-writing services </span><br><span class="line">com.acme.AcmeAudioFileWriter</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now we run <code>jar</code> from any directory with the command line:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">jar cvf acme.jar -C &#x2F;devel .</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The <code>-C</code> option causes <code>jar</code> to switch to the <code>/devel</code> directory, instead of using the directory in which the command is executed. The final period argument instructs <code>jar</code> to archive all the contents of that directory (namely, <code>/devel</code>), but not the directory itself.</p><p>This run will create the file <code>acme.jar</code> with the contents:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">com&#x2F;acme&#x2F;AcmeAudioFileReader.class</span><br><span class="line">com&#x2F;acme&#x2F;AcmeAudioFileWriter.class</span><br><span class="line">META-INF&#x2F;services&#x2F;javax.sound.sampled.spi.AudioFileReader</span><br><span class="line">META-INF&#x2F;services&#x2F;javax.sound.sampled.spi.AudioFileWriter</span><br><span class="line">META-INF&#x2F;Manifest.mf</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The file <code>Manifest.mf,</code> which is generated by the <code>jar</code> utility itself, is a list of all the files contained in the archive.</p><h2 id="How-Users-Install-New-Services"><a href="#How-Users-Install-New-Services" class="headerlink" title="How Users Install New Services"></a><a name="how_users" id="how_users"></a>How Users Install New Services</h2><p>For end users (or system administrators) who wish to get access to a new service through their application programs, installation is simple. They place the provided JAR file in a directory in their <code>CLASSPATH.</code> Upon execution, the Java runtime will find the referenced classes when needed.</p><p>It’s not an error to install more than one provider for the same service. For example, two different service providers might supply support for reading the same type of sound file. In such a case, the system arbitrarily chooses one of the providers. Users who care which provider is chosen should install only the desired one.</p><p>&#160;</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction-to-the-Service-Provider-Interfaces&quot;&gt;&lt;a href=&quot;#Introduction-to-the-Service-Provider-Interfaces&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Sequencers</title>
    <link href="http://example.com/wiki/2021-03-04-Introduction%20to%20Sequencers/"/>
    <id>http://example.com/wiki/2021-03-04-Introduction%20to%20Sequencers/</id>
    <published>2021-03-04T14:32:16.042Z</published>
    <updated>2021-03-04T15:17:55.804Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction-to-Sequencers"><a href="#Introduction-to-Sequencers" class="headerlink" title="Introduction to Sequencers"></a>Introduction to Sequencers</h1><p><a name="120892" id="120892"></a> In the world of MIDI, a <strong>sequencer</strong> is any hardware or software device that can precisely play or record a <strong>sequence</strong> of time-stamped MIDI messages. Similarly, in the Java Sound API, the<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/Sequencer.html"><code>Sequencer</code></a> abstract interface defines the properties of an object that can play and record sequences of<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiEvent.html"><code>MidiEvent</code></a> objects. A <code>Sequencer</code> typically loads these <code>MidiEvent</code> sequences from a standard MIDI file or saves them to such a file. Sequences can also be edited. The following pages explain how to use <code>Sequencer</code> objects, along with related classes and interfaces, to accomplish such tasks.</p><p>To develop an intuitive understanding of what a <code>Sequencer</code> is, think of it by analogy with a tape recorder, which a sequencer resembles in many respects. Whereas a tape recorder plays audio, a sequencer plays MIDI data. A sequence is a multi-track, linear, time-ordered recording of MIDI musical data, which a sequencer can play at various speeds, rewind, shuttle to particular points, record into, or copy to a file for storage.</p><p><a name="124503" id="124503"></a><br><a href="MIDI-messages.html">Transmitting and Receiving MIDI Messages</a> explained that devices typically have <code>Receiver</code> objects, <code>Transmitter</code> objects, or both. To <strong>play</strong> music, a device generally receives <code>MidiMessages</code> through a <code>Receiver</code>, which in turn has usually received them from a <code>Transmitter</code> that belongs to a <code>Sequencer</code>. The device that owns this <code>Receiver</code> might be a <code>Synthesizer</code>, which will generate audio directly, or it might be a MIDI output port, which transmits MIDI data through a physical cable to some external piece of equipment. Similarly, to <strong>record</strong> music, a series of time-stamped <code>MidiMessages</code> are generally sent to a <code>Receiver</code> owned by a <code>Sequencer</code>, which places them in a <code>Sequence</code> object. Typically the object sending the messages is a <code>Transmitter</code> associated with a hardware input port, and the port relays MIDI data that it gets from an external instrument. However, the device responsible for sending the messages might instead be some other <code>Sequencer</code>, or any other device that has a <code>Transmitter</code>. Furthermore, as previously described, a program can send messages without using any <code>Transmitter</code> at all.</p><p><a name="120896" id="120896"></a> A <code>Sequencer</code> itself has both <code>Receivers</code> and <code>Transmitters</code>. When it’s recording, it actually obtains <code>MidiMessages</code> via its <code>Receivers</code>. During playback, it uses its <code>Transmitters</code> to send <code>MidiMessages</code> that are stored in the <code>Sequence</code> that it has recorded (or loaded from a file).</p><p><a name="124511" id="124511"></a> One way to think of the role of a <code>Sequencer</code> in the Java Sound API is as an aggregator and “de-aggregator” of <code>MidiMessages</code>. A series of separate <code>MidiMessages</code>, each of which is independent, is sent to the <code>Sequencer</code> along with its own time stamp that marks the timing of a musical event. These <code>MidiMessages</code> are encapsulated in <code>MidiEvent</code> objects and collected in <code>Sequence</code> objects through the action of the <code>Sequencer.record</code> method. A <code>Sequence</code> is a data structure containing aggregates of <code>MidiEvents</code>, and it usually represents a series of musical notes, often an entire song or composition. On playback, the <code>Sequencer</code> again extracts the <code>MidiMessages</code> from the <code>MidiEvent</code> objects in the <code>Sequence</code> and then transmits them to one or more devices that will either render them into sound, save them, modify them, or pass them on to some other device.</p><p><a name="124522" id="124522"></a> Some sequencers might have neither transmitters nor receivers. For example, they might create <code>MidiEvents</code> from scratch as a result of keyboard or mouse events, instead of receiving <code>MidiMessages</code> through <code>Receivers</code>. Similarly, they might play music by communicating directly with an internal synthesizer (which could actually be the same object as the sequencer) instead of sending <code>MidiMessages</code> to a <code>Receiver</code> associated with a separate object. However, the rest of this discussion assumes the normal case of a sequencer that uses <code>Receivers</code> and <code>Transmitters</code>.</p><p><a name="when" id="when"></a></p><h2 id="When-to-Use-a-Sequencer"><a href="#When-to-Use-a-Sequencer" class="headerlink" title="When to Use a Sequencer"></a>When to Use a Sequencer</h2><p><a name="124536" id="124536"></a> It’s possible for an application program to send MIDI messages directly to a device, without using a sequencer, as was described in<br><a href="MIDI-messages.html">Transmitting and Receiving MIDI Messages</a>. The program simply invokes the <code>Receiver.send</code> method each time it wants to send a message. This is a straightforward approach that’s useful when the program itself creates the messages in real time. For example, consider a program that lets the user play notes by clicking on an onscreen piano keyboard. When the program gets a mouse-down event, it immediately sends the appropriate Note On message to the synthesizer.</p><p><a name="124540" id="124540"></a> As previously mentioned, the program can include a time stamp with each MIDI message it sends to the device’s receiver. However, such time stamps are used only for fine-tuning the timing, to correct for processing latency. The caller can’t generally set arbitrary time stamps; the time value passed to <code>Receiver.send</code> must be close to the present time, or the receiving device might not be able to schedule the message correctly. This means that if an application program wanted to create a queue of MIDI messages for an entire piece of music ahead of time (instead of creating each message in response to a real-time event), it would have to be very careful to schedule each invocation of <code>Receiver.send</code> for nearly the right time.</p><p><a name="124541" id="124541"></a> Fortunately, most application programs don’t have to be concerned with such scheduling. Instead of invoking <code>Receiver.send</code> itself, a program can use a <code>Sequencer</code> object to manage the queue of MIDI messages for it. The sequencer takes care of scheduling and sending the messages&#226;&#128;&#148;in other words, playing the music with the correct timing. Generally, it’s advantageous to use a sequencer whenever you need to convert a non-real-time series of MIDI messages to a real-time series (as in playback), or vice versa (as in recording). Sequencers are most commonly used for playing data from MIDI files and for recording data from a MIDI input port.</p><p><a name="124542" id="124542"></a></p><h2 id="Understanding-Sequence-Data"><a href="#Understanding-Sequence-Data" class="headerlink" title="Understanding Sequence Data"></a>Understanding Sequence Data</h2><p><a name="124543" id="124543"></a> Before examining the <code>Sequencer</code> API, it helps to understand the kind of data that’s stored in a sequence.</p><p><a name="124544" id="124544"></a></p><h3 id="Sequences-and-Tracks"><a href="#Sequences-and-Tracks" class="headerlink" title="Sequences and Tracks"></a>Sequences and Tracks</h3><p><a name="124545" id="124545"></a> In the Java Sound API, sequencers closely follow the Standard MIDI Files specification in the way that they organize recorded MIDI data. As mentioned above, a <code>Sequence</code> is an aggregation of <code>MidiEvents</code>, organized in time. But there is more structure to a <code>Sequence</code> than just a linear series of <code>MidiEvents</code>: a <code>Sequence</code> actually contains global timing information plus a collection of <code>Tracks</code>, and it is the <code>Tracks</code> themselves that hold the <code>MidiEvent</code> data. So the data played by a sequencer consists of a three-level hierarchy of objects: <code>Sequencer</code>, <code>Track</code>, and <code>MidiEvent</code>.</p><p><a name="124546" id="124546"></a> In the conventional use of these objects, the <code>Sequence</code> represents a complete musical composition or section of a composition, with each <code>Track</code> corresponding to a voice or player in the ensemble. In this model, all the data on a particular <code>Track</code> would also therefore be encoded into a particular MIDI channel reserved for that voice or player.</p><p><a name="124547" id="124547"></a> This way of organizing data is convenient for purposes of editing sequences, but note that this is just a conventional way to use <code>Tracks</code>. There is nothing in the definition of the <code>Track</code> class that keeps it from containing a mix of <code>MidiEvents</code> on different MIDI channels. For example, an entire multi-channel MIDI composition can be mixed and recorded onto one <code>Track</code>. Also, standard MIDI files of Type 0 (as opposed to Type 1 and Type 2) contain by definition only one track; so a <code>Sequence</code> that’s read from such a file will necessarily have a single <code>Track</code> object.</p><p><a name="124548" id="124548"></a></p><h3 id="MidiEvents-and-Ticks"><a href="#MidiEvents-and-Ticks" class="headerlink" title="MidiEvents and Ticks"></a>MidiEvents and Ticks</h3><p><a name="124552" id="124552"></a> As discussed in<br><a href="overview-MIDI.html">Overview of the MIDI Package</a>, the Java Sound API includes <code>MidiMessage</code> objects that correspond to the raw two- or three-byte sequences that make up most standard MIDI messages. A <code>MidiEvent</code> is simply a packaging of a <code>MidiMessage</code> along with an accompanying timing value that specifies when the event occurs. (We might then say that a sequence really consists of a four- or five-level hierarchy of data, rather than three-level, because the ostensible lowest level, <code>MidiEvent</code>, actually contains a lower-level <code>MidiMessage</code>, and likewise the <code>MidiMessage</code> object contains an array of bytes that comprises a standard MIDI message.)</p><p><a name="124553" id="124553"></a> In the Java Sound API, there are two different ways in which <code>MidiMessages</code> can be associated with timing values. One is the way mentioned above under “When to Use a Sequencer.” This technique was described in detail under<br><a href="MIDI-messages.html#sending">Sending a Message to a Receiver without Using a Transmitter</a> and<br><a href="MIDI-messages.html#understanding_time">Understanding Time Stamps</a>. There, we saw that the <code>send</code> method of <code>Receiver</code> takes a <code>MidiMessage</code> argument and a time-stamp argument. That kind of time stamp can only be expressed in microseconds.</p><p><a name="124566" id="124566"></a> The other way in which a <code>MidiMessage</code> can have its timing specified is by being encapsulated in a <code>MidiEvent</code>. In this case, the timing is expressed in slightly more abstract units called <strong>ticks</strong>.</p><p><a name="124567" id="124567"></a> What is the duration of a tick? It can vary between sequences (but not within a sequence), and its value is stored in the header of a standard MIDI file. The size of a tick is given in one of two types of units:</p><ul><li><a name="124568" id="124568"></a>Pulses (ticks) per quarter note, abbreviated as PPQ</li><li><a name="124569" id="124569"></a>Ticks per frame, also known as SMPTE time code (a standard adopted by the Society of Motion Picture and Television Engineers)</li></ul><p><a name="124570" id="124570"></a> If the unit is PPQ, the size of a tick is expressed as a fraction of a quarter note, which is a relative, not absolute, time value. A quarter note is a musical duration value that often corresponds to one beat of the music (a quarter of a measure in 4/4 time). The duration of a quarter note is dependent on the tempo, which can vary during the course of the music if the sequence contains tempo-change events. So if the sequence’s timing increments (ticks) occur, say 96 times per quarter note, each event’s timing value measures that event’s position in musical terms, not as an absolute time value.</p><p><a name="124571" id="124571"></a> On the other hand, in the case of SMPTE, the units measure absolute time, and the notion of tempo is inapplicable. There are actually four different SMPTE conventions available, which refer to the number of motion-picture frames per second. The number of frames per second can be 24, 25, 29.97, or 30. With SMPTE time code, the size of a tick is expressed as a fraction of a frame.</p><p><a name="124572" id="124572"></a> In the Java Sound API, you can invoke <code>Sequence.getDivisionType</code> to learn which type of unit&#226;&#128;&#148;namely, PPQ or one of the SMPTE units&#226;&#128;&#148;is used in a particular sequence. You can then calculate the size of a tick after invoking <code>Sequence.getResolution</code>. The latter method returns the number of ticks per quarter note if the division type is PPQ, or per SMPTE frame if the division type is one of the SMPTE conventions. You can get the size of a tick using this formula in the case of PPQ:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ticksPerSecond &#x3D;  </span><br><span class="line">    resolution * (currentTempoInBeatsPerMinute &#x2F; 60.0);</span><br><span class="line">tickSize &#x3D; 1.0 &#x2F; ticksPerSecond;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124576" id="124576"></a> and this formula in the case of SMPTE:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">framesPerSecond &#x3D; </span><br><span class="line">  (divisionType &#x3D;&#x3D; Sequence.SMPTE_24 ? 24</span><br><span class="line">    : (divisionType &#x3D;&#x3D; Sequence.SMPTE_25 ? 25</span><br><span class="line">      : (divisionType &#x3D;&#x3D; Sequence.SMPTE_30 ? 30</span><br><span class="line">        : (divisionType &#x3D;&#x3D; Sequence.SMPTE_30DROP ?&lt;br &#x2F;&gt;</span><br><span class="line">            29.97))));</span><br><span class="line">ticksPerSecond &#x3D; resolution * framesPerSecond;</span><br><span class="line">tickSize &#x3D; 1.0 &#x2F; ticksPerSecond;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="124584" id="124584"></a> The Java Sound API’s definition of timing in a sequence mirrors that of the Standard MIDI Files specification. However, there’s one important difference. The tick values contained in <code>MidiEvents</code> measure <strong>cumulative</strong> time, rather than <strong>delta</strong> time. In a standard MIDI file, each event’s timing information measures the amount of time elapsed since the onset of the previous event in the sequence. This is called delta time. But in the Java Sound API, the ticks aren’t delta values; they’re the previous event’s time value <strong>plus</strong> the delta value. In other words, in the Java Sound API the timing value for each event is always greater than that of the previous event in the sequence (or equal, if the events are supposed to be simultaneous). Each event’s timing value measures the time elapsed since the beginning of the sequence.</p><p><a name="124585" id="124585"></a> To summarize, the Java Sound API expresses timing information in either MIDI ticks or microseconds. <code>MidiEvents</code> store timing information in terms of MIDI ticks. The duration of a tick can be calculated from the <code>Sequence&#39;s</code> global timing information and, if the sequence uses tempo-based timing, the current musical tempo. The time stamp associated with a <code>MidiMessage</code> sent to a <code>Receiver</code>, on the other hand, is always expressed in microseconds.</p><p><a name="124586" id="124586"></a> One goal of this design is to avoid conflicting notions of time. It’s the job of a <code>Sequencer</code> to interpret the time units in its <code>MidiEvents</code>, which might have PPQ units, and translate these into absolute time in microseconds, taking the current tempo into account. The sequencer must also express the microseconds relative to the time when the device receiving the message was opened. Note that a sequencer can have multiple transmitters, each delivering messages to a different receiver that might be associated with a completely different device. You can see, then, that the sequencer has to be able to perform multiple translations at the same time, making sure that each device receives time stamps appropriate for its notion of time.</p><p><a name="124587" id="124587"></a> To make matters more complicated, different devices might update their notions of time based on different sources (such as the operating system’s clock, or a clock maintained by a sound card). This means that their timings can drift relative to the sequencer’s. To keep in synchronization with the sequencer, some devices permit themselves to be “slaves” to the sequencer’s notion of time. Setting masters and slaves is discussed later under<br><a href="https://docs.oracle.com/javase/8/docs/api/javax/sound/midi/MidiEvent.html"><code>MidiEvent</code></a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction-to-Sequencers&quot;&gt;&lt;a href=&quot;#Introduction-to-Sequencers&quot; class=&quot;headerlink&quot; title=&quot;Introduction to Sequencers&quot;&gt;&lt;/a&gt;Introduc</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
  <entry>
    <title>Capturing Audio</title>
    <link href="http://example.com/wiki/2021-03-04-Capturing%20Audio/"/>
    <id>http://example.com/wiki/2021-03-04-Capturing%20Audio/</id>
    <published>2021-03-04T14:32:16.031Z</published>
    <updated>2021-03-04T15:17:55.801Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Capturing-Audio"><a href="#Capturing-Audio" class="headerlink" title="Capturing Audio"></a>Capturing Audio</h1><p><a name="114180" id="114180"></a> <strong>Capturing</strong> refers to the process of obtaining a signal from outside the computer. A common application of audio capture is recording, such as recording the microphone input to a sound file. However, capturing isn’t synonymous with recording, because recording implies that the application always saves the sound data that’s coming in. An application that captures audio doesn’t necessarily store the audio. Instead it might do something with the data as it’s coming in &#8212; such as transcribe speech into text &#8212; but then discard each buffer of audio as soon as it’s finished with that buffer.</p><p><a name="114181" id="114181"></a> As discussed in<br><a href="sampled-overview.html">Overview of the Sampled Package</a>, a typical audio-input system in an implementation of the Java Sound API consists of:</p><ol><li><a name="113952" id="113952"></a>An input port, such as a microphone port or a line-in port, which feeds its incoming audio data into:</li><li><a name="115691" id="115691"></a>A mixer, which places the input data in:</li><li><a name="115692" id="115692"></a>One or more target data lines, from which an application can retrieve the data.</li></ol><p><a name="113956" id="113956"></a> Commonly, only one input port can be open at a time, but an audio-input mixer that mixes audio from multiple ports is also possible. Another scenario consists of a mixer that has no ports but instead gets its audio input over a network.</p><p><a name="113958" id="113958"></a> The <code>TargetDataLine</code> interface was introduced briefly under<br><a href="sampled-overview.html#lineHierarchy">The Line Interface Hierarchy</a>. <code>TargetDataLine</code> is directly analogous to the <code>SourceDataLine</code> interface, which was discussed extensively in<br><a href="playing.html">Playing Back Audio</a>. Recall that the <code>SourceDataLine</code> interface consists of:</p><ul><li><a name="113960" id="113960"></a>A <code>write</code> method to send audio to the mixer</li><li><a name="113961" id="113961"></a>An <code>available</code> method to determine how much data can be written to the buffer without blocking</li></ul><p><a name="113963" id="113963"></a> Similarly, <code>TargetDataLine</code> consists of:</p><ul><li><a name="113965" id="113965"></a>A <code>read</code> method to get audio from the mixer</li><li><a name="113966" id="113966"></a>An <code>available</code> method to determine how much data can be read from the buffer without blocking</li></ul><h2 id="Setting-Up-a-TargetDataLine"><a href="#Setting-Up-a-TargetDataLine" class="headerlink" title="Setting Up a TargetDataLine"></a><a name="113969" id="113969"></a>Setting Up a TargetDataLine</h2><p><a name="113971" id="113971"></a> The process of obtaining a target data line was described in<br><a href="accessing.html">Accessing Audio System Resources</a> but we repeat it here for convenience:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">TargetDataLine line;</span><br><span class="line">DataLine.Info info &#x3D; new DataLine.Info(TargetDataLine.class, </span><br><span class="line">    format); &#x2F;&#x2F; format is an AudioFormat object</span><br><span class="line">if (!AudioSystem.isLineSupported(info)) &#123;</span><br><span class="line">    &#x2F;&#x2F; Handle the error ... </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; Obtain and open the line.</span><br><span class="line">try &#123;</span><br><span class="line">    line &#x3D; (TargetDataLine) AudioSystem.getLine(info);</span><br><span class="line">    line.open(format);</span><br><span class="line">&#125; catch (LineUnavailableException ex) &#123;</span><br><span class="line">    &#x2F;&#x2F; Handle the error ... </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="113992" id="113992"></a> You could instead invoke <code>Mixer&#39;s</code> <code>getLine</code> method, rather than <code>AudioSystem&#39;s</code>.</p><p><a name="113994" id="113994"></a> As shown in this example, once you’ve obtained a target data line, you reserve it for your application’s use by invoking the <code>SourceDataLine</code> method <code>open</code>, exactly as was described in the case of a source data line in<br><a href="playing.html">Playing Back Audio</a>. The single-parameter version of the <code>open</code> method causes the line’s buffer to have the default size. You can instead set the buffer size according to your application’s needs by invoking the two-parameter version:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">void open(AudioFormat format, int bufferSize)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a name="115741" id="115741"></a></p><h2 id="Reading-the-Data-from-the-TargetDataLine"><a href="#Reading-the-Data-from-the-TargetDataLine" class="headerlink" title="Reading the Data from the TargetDataLine"></a>Reading the Data from the TargetDataLine</h2><p><a name="114004" id="114004"></a> Once the line is open, it is ready to start capturing data, but it isn’t active yet. To actually commence the audio capture, use the <code>DataLine</code> method <code>start</code>. This begins delivering input audio data to the line’s buffer for your application to read. Your application should invoke start only when it’s ready to begin reading from the line; otherwise a lot of processing is wasted on filling the capture buffer, only to have it overflow (that is, discard data).</p><p><a name="114006" id="114006"></a> To start retrieving data from the buffer, invoke <code>TargetDataLine&#39;s</code> read method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int read(byte[] b, int offset, int length)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method attempts to read <code>length</code> bytes of data into the array <code>b</code>, starting at the byte position <code>offset</code> in the array. The method returns the number of bytes actually read.</p><p><a name="114011" id="114011"></a> As with <code>SourceDataLine&#39;s write</code> method, you can request more data than actually fits in the buffer, because the method blocks until the requested amount of data has been delivered, even if you request many buffers’ worth of data.</p><p><a name="115205" id="115205"></a> To avoid having your application hang during recording, you can invoke the read method within a loop, until you’ve retrieved all the audio input, as in this example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#x2F;&#x2F; Assume that the TargetDataLine, line, has already</span><br><span class="line">&#x2F;&#x2F; been obtained and opened.</span><br><span class="line">ByteArrayOutputStream out  &#x3D; new ByteArrayOutputStream();</span><br><span class="line">int numBytesRead;</span><br><span class="line">byte[] data &#x3D; new byte[line.getBufferSize() &#x2F; 5];</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Begin audio capture.</span><br><span class="line">line.start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Here, stopped is a global boolean set by another thread.</span><br><span class="line">while (!stopped) &#123;</span><br><span class="line">   &#x2F;&#x2F; Read the next chunk of data from the TargetDataLine.</span><br><span class="line">   numBytesRead &#x3D;  line.read(data, 0, data.length);</span><br><span class="line">   &#x2F;&#x2F; Save this chunk of data.</span><br><span class="line">   out.write(data, 0, numBytesRead);</span><br><span class="line">&#125;     </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Notice that in this example, the size of the byte array into which the data is read is set to be one-fifth the size of the line’s buffer. If you instead make it as big as the line’s buffer and try to read the entire buffer, you need to be very exact in your timing, because data will be dumped if the mixer needs to deliver data to the line while you are reading from it. By using some fraction of the line’s buffer size, as shown here, your application will be more successful in sharing access to the line’s buffer with the mixer.</p><p><a name="114034" id="114034"></a> The <code>read</code> method of <code>TargetDataLine</code> takes three arguments: a byte array, an offset into the array, and the number of bytes of input data that you would like to read. In this example, the third argument is simply the length of your byte array. The <code>read</code> method returns the number of bytes that were actually read into your array.</p><p><a name="114036" id="114036"></a> Typically, you read data from the line in a loop, as in this example. Within the <code>while</code> loop, each chunk of retrieved data is processed in whatever way is appropriate for the application&#226;&#128;&#148;here, it’s written to a <code>ByteArrayOutputStream</code>. Not shown here is the use of a separate thread to set the boolean <code>stopped</code>, which terminates the loop. This boolean’s value might be set to <code>true</code> when the user clicks a Stop button, and also when a listener receives a <code>CLOSE</code> or <code>STOP</code> event from the line. The listener is necessary for <code>CLOSE</code> events and recommended for <code>STOP</code> events. Otherwise, if the line gets stopped somehow without stopped being set to <code>true</code>, the <code>while</code> loop will capture zero bytes on each iteration, running fast and wasting CPU cycles. A more thorough code example would show the loop being re-entered if capture becomes active again.</p><p><a name="114039" id="114039"></a> As with a source data line, it’s possible to drain or flush a target data line. For example, if you’re recording the input to a file, you’ll probably want to invoke the <code>drain</code> method when the user clicks a Stop button. The <code>drain</code> method will cause the mixer’s remaining data to get delivered to the target data line’s buffer. If you don’t drain the data, the captured sound might seem to be truncated prematurely at the end.</p><p><a name="114041" id="114041"></a> There might be some cases where you instead want to flush the data. In any case, if you neither flush nor drain the data, it will be left in the mixer. This means that when capture recommences, there will be some leftover sound at the beginning of the new recording, which might be undesirable. It can be useful, then, to flush the target data line before restarting the capture.</p><p><a name="114432" id="114432"></a></p><h2 id="Monitoring-the-Line’s-Status"><a href="#Monitoring-the-Line’s-Status" class="headerlink" title="Monitoring the Line’s Status"></a>Monitoring the Line’s Status</h2><p><a name="114046" id="114046"></a> Because the <code>TargetDataLine</code> interface extends <code>DataLine</code>, target data lines generate events in the same way source data lines do. You can register an object to receive events whenever the target data line opens, closes, starts, or stops. For more information, see the previous discussion of<br><a href="playing.html#113711">Monitoring a Line’s Status</a>.</p><p><a name="114049" id="114049"></a></p><h2 id="Processing-the-Incoming-Audio"><a href="#Processing-the-Incoming-Audio" class="headerlink" title="Processing the Incoming Audio"></a>Processing the Incoming Audio</h2><p><a name="114051" id="114051"></a> Like some source data lines, some mixers’ target data lines have signal-processing controls, such as gain, pan, reverb, or sample-rate controls. The input ports might have similar controls, especially gain controls. In the next section, you’ll learn how to determine whether a line has such controls, and how to use them if it does.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Capturing-Audio&quot;&gt;&lt;a href=&quot;#Capturing-Audio&quot; class=&quot;headerlink&quot; title=&quot;Capturing Audio&quot;&gt;&lt;/a&gt;Capturing Audio&lt;/h1&gt;&lt;p&gt;&lt;a name=&quot;114180&quot; i</summary>
      
    
    
    
    <category term="BigJava" scheme="http://example.com/categories/BigJava/"/>
    
    <category term="Sound" scheme="http://example.com/categories/BigJava/Sound/"/>
    
    
    <category term="Sound" scheme="http://example.com/tags/Sound/"/>
    
    <category term="Bigjava" scheme="http://example.com/tags/Bigjava/"/>
    
  </entry>
  
</feed>
